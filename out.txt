-- Testing: 215 tests, 16 workers --
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_middle_dim.mlir (1 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_middle_dim.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %out, %5 : tensor<32x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: note: see current operation: tt.store %arg2, %19 : tensor<32x16x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %out, %5 : tensor<32x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: note: see current operation: tt.store %arg2, %19 : tensor<32x16x!tt.ptr<bf16>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, tensor<32x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: tensor<32x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "tts.make_tptr"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %6 = "tt.reduce"(%5) <{axis = 1 : i32}> ({
    ^bb0(%arg3: bf16, %arg4: bf16):
      %7 = "arith.addf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%7) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<32x16xbf16>
    "tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
these are the uses:
"tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
actual processing
processing user
"tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
tensor<32x16x!tt.ptr<bf16>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, tensor<32x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: tensor<32x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "tts.make_tptr"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %6 = "tt.reduce"(%5) <{axis = 1 : i32}> ({
    ^bb0(%arg3: bf16, %arg4: bf16):
      %8 = "arith.addf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%8) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<32x16xbf16>
    %7 = "tts.create_ptr"(%arg2, %0) : (tensor<32x16x!tt.ptr<bf16>>, tensor<32x16xi32>) -> tensor<32x16x!tt.ptr<bf16>>
    "tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, tensor<32x16x!tt.ptr<bf16>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: tensor<32x16x!tt.ptr<bf16>>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<32x16xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg13: i32, %arg14: i32):
      "linalg.yield"(%arg13) : (i32) -> ()
    }) : (i32, tensor<32x16xi32>) -> tensor<32x16xi32>
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %5 = "tts.make_tptr"(%4, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (memref<*xbf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %6 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %7 = "arith.constant"() <{value = 0.000000e+00 : bf16}> : () -> bf16
    %8 = "tensor.empty"() : () -> tensor<32x16xbf16>
    %9 = "linalg.fill"(%7, %8) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg11: bf16, %arg12: bf16):
      "linalg.yield"(%arg11) : (bf16) -> ()
    }) : (bf16, tensor<32x16xbf16>) -> tensor<32x16xbf16>
    %10 = "linalg.reduce"(%6, %9) <{dimensions = array<i64: 1>}> ({
    ^bb0(%arg9: bf16, %arg10: bf16):
      %12 = "arith.addf"(%arg9, %arg10) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "linalg.yield"(%12) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>, tensor<32x16xbf16>) -> tensor<32x16xbf16>
    %11 = "tts.create_ptr"(%arg2, %2) : (tensor<32x16x!tt.ptr<bf16>>, tensor<32x16xi32>) -> tensor<32x16x!tt.ptr<bf16>>
    "tt.store"(%11, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16x!tt.ptr<bf16>>, tensor<32x16xbf16>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<32x16x!tt.ptr<bf16>>'
    tt.store %out, %5 : tensor<32x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: note: see current operation: %12 = "memref.cast"(%arg2) : (tensor<32x16x!tt.ptr<bf16>>) -> memref<?xbf16>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_complex_offset.mlir (2 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_complex_offset.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --structured-to-memref --split-input-file /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --structured-to-memref --split-input-file /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir
module {
  func.func @fused_attention_fwd_kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: i64, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32) -> tensor<128x128xbf16> {
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %0 = arith.remsi %arg8, %arg3 : i32
    %1 = arith.extsi %0 : i32 to i64
    %2 = arith.muli %1, %arg2 : i64
    %3 = tt.addptr %arg0, %2 : !tt.ptr<bf16>, i64
    %4 = tts.make_tptr %3 to sizes: [128, 128], strides: [%c128, %c1], offsets: [%c0, %c0], shape: [%c128, %c128], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x128xbf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x128xbf16>>) -> tensor<128x128xbf16>
    return %5 : tensor<128x128xbf16>
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir:1 offset :12:10: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<bf16>'
    %4 = tts.make_tptr %3 to sizes: [128, 128], strides: [%c128, %c1], offsets: [%c0, %c0], shape: [%c128, %c128], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x128xbf16>>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir:1 offset :12:10: note: see current operation: %8 = "memref.reinterpret_cast"(%6, %7, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 128, 128>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index, index) -> memref<128x128xbf16, strided<[?, ?], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_complex_offset.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_splat_float.mlir (3 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_splat_float.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %save0, %0 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: note: see current operation: tt.store %arg2, %0 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %save0, %0 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: note: see current operation: tt.store %arg2, %0 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :10:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %save1, %1 : tensor<128x256x!tt.ptr<bf16>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :10:9: note: see current operation: tt.store %arg3, %1 : tensor<128x256x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :10:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %save1, %1 : tensor<128x256x!tt.ptr<bf16>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :10:9: note: see current operation: tt.store %arg3, %1 : tensor<128x256x!tt.ptr<bf16>>
"builtin.module"() ({
  "tt.func"() <{function_type = (f32, bf16, tensor<1024x!tt.ptr<f32>>, tensor<128x256x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: f32, %arg1: bf16, %arg2: tensor<1024x!tt.ptr<f32>>, %arg3: tensor<128x256x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x256xi32>}> : () -> tensor<128x256xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %2 = "tt.splat"(%arg0) : (f32) -> tensor<1024xf32>
    %3 = "tt.splat"(%arg1) : (bf16) -> tensor<128x256xbf16>
    "tt.store"(%1, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.store"(%0, %3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x256xi32>, tensor<128x256xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%1, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x256xi32>}> : () -> tensor<128x256xi32>
these are the uses:
"tt.store"(%0, %3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x256xi32>, tensor<128x256xbf16>) -> ()
actual processing
processing user
"tt.store"(%0, %3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x256xi32>, tensor<128x256xbf16>) -> ()
tensor<128x256x!tt.ptr<bf16>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (f32, bf16, tensor<1024x!tt.ptr<f32>>, tensor<128x256x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: f32, %arg1: bf16, %arg2: tensor<1024x!tt.ptr<f32>>, %arg3: tensor<128x256x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x256xi32>}> : () -> tensor<128x256xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %2 = "tt.splat"(%arg0) : (f32) -> tensor<1024xf32>
    %3 = "tt.splat"(%arg1) : (bf16) -> tensor<128x256xbf16>
    %4 = "tts.create_ptr"(%arg2, %1) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%1, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    %5 = "tts.create_ptr"(%arg3, %0) : (tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>) -> tensor<128x256x!tt.ptr<bf16>>
    "tt.store"(%0, %3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x256xi32>, tensor<128x256xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @kernel(%arg0: f32, %arg1: bf16, %arg2: tensor<1024x!tt.ptr<f32>>, %arg3: tensor<128x256x!tt.ptr<bf16>>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32) {
    %c0_i32 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<128x256xi32>
    %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<128x256xi32>) -> tensor<128x256xi32>
    %c0_i32_0 = arith.constant 0 : i32
    %2 = tensor.empty() : tensor<1024xi32>
    %3 = linalg.fill ins(%c0_i32_0 : i32) outs(%2 : tensor<1024xi32>) -> tensor<1024xi32>
    %4 = tensor.empty() : tensor<1024xf32>
    %5 = linalg.fill ins(%arg0 : f32) outs(%4 : tensor<1024xf32>) -> tensor<1024xf32>
    %6 = tensor.empty() : tensor<128x256xbf16>
    %7 = linalg.fill ins(%arg1 : bf16) outs(%6 : tensor<128x256xbf16>) -> tensor<128x256xbf16>
    %8 = "tts.create_ptr"(%arg2, %3) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    tt.store %8, %5 : tensor<1024x!tt.ptr<f32>>
    %9 = "tts.create_ptr"(%arg3, %1) : (tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>) -> tensor<128x256x!tt.ptr<bf16>>
    tt.store %9, %7 : tensor<128x256x!tt.ptr<bf16>>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
        tt.store %save0, %0 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir:1 offset :9:9: note: see current operation: %10 = "memref.cast"(%arg2) : (tensor<1024x!tt.ptr<f32>>) -> memref<?xf32>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_splat_float.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_side_by_side.mlir (4 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir (5 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %c, %res0 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: note: see current operation: tt.store %arg2, %15 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %c, %res0 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: note: see current operation: tt.store %arg2, %15 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %d, %res1 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: note: see current operation: tt.store %arg3, %16 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %d, %res1 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: note: see current operation: tt.store %arg3, %16 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<128x128x!tt.ptr<f32>>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %7 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %8 = "arith.addf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %9 = "arith.subf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<128x128x!tt.ptr<f32>>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %7 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %8 = "arith.addf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %9 = "arith.subf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %10 = "tts.create_ptr"(%arg2, %1) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %11 = "tts.create_ptr"(%arg3, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<128x128x!tt.ptr<f32>>, %arg3: tensor<128x128x!tt.ptr<f32>>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<128x128xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg16: i32, %arg17: i32):
      "linalg.yield"(%arg16) : (i32) -> ()
    }) : (i32, tensor<128x128xi32>) -> tensor<128x128xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %8 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %9 = "linalg.generic"(%7, %8, %7) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg13: f32, %arg14: f32, %arg15: f32):
      %14 = "arith.addf"(%arg13, %arg14) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%14) : (f32) -> ()
    }) : (tensor<128x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %10 = "linalg.generic"(%7, %8, %7) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg10: f32, %arg11: f32, %arg12: f32):
      %13 = "arith.subf"(%arg10, %arg11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%13) : (f32) -> ()
    }) : (tensor<128x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %11 = "tts.create_ptr"(%arg2, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%11, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    %12 = "tts.create_ptr"(%arg3, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%12, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<f32>>'
        tt.store %c, %res0 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: note: see current operation: %13 = "memref.cast"(%arg2) : (tensor<128x128x!tt.ptr<f32>>) -> memref<?xf32>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir (6 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %res, %6 : tensor<256x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: note: see current operation: tt.store %arg1, %19 : tensor<256x16x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %res, %6 : tensor<256x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: note: see current operation: tt.store %arg1, %19 : tensor<256x16x!tt.ptr<bf16>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, tensor<256x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: tensor<256x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 256 : index}> : () -> index
    %3 = "tts.make_tptr"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %5 = "tt.reduce"(%4) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %6 = "arith.cmpf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (bf16, bf16) -> i1
      %7 = "arith.select"(%6, %arg2, %arg3) : (i1, bf16, bf16) -> bf16
      "tt.reduce.return"(%7) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<256x16xbf16>
    "tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
these are the uses:
"tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
actual processing
processing user
"tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
tensor<256x16x!tt.ptr<bf16>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, tensor<256x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: tensor<256x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 256 : index}> : () -> index
    %3 = "tts.make_tptr"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %5 = "tt.reduce"(%4) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %7 = "arith.cmpf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (bf16, bf16) -> i1
      %8 = "arith.select"(%7, %arg2, %arg3) : (i1, bf16, bf16) -> bf16
      "tt.reduce.return"(%8) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<256x16xbf16>
    %6 = "tts.create_ptr"(%arg1, %0) : (tensor<256x16x!tt.ptr<bf16>>, tensor<256x16xi32>) -> tensor<256x16x!tt.ptr<bf16>>
    "tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, tensor<256x16x!tt.ptr<bf16>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: tensor<256x16x!tt.ptr<bf16>>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<256x16xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg12: i32, %arg13: i32):
      "linalg.yield"(%arg12) : (i32) -> ()
    }) : (i32, tensor<256x16xi32>) -> tensor<256x16xi32>
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %5 = "tts.make_tptr"(%4, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (memref<*xbf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %6 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %7 = "arith.constant"() <{value = 0xFF80 : bf16}> : () -> bf16
    %8 = "tensor.empty"() : () -> tensor<256x16xbf16>
    %9 = "linalg.fill"(%7, %8) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg10: bf16, %arg11: bf16):
      "linalg.yield"(%arg10) : (bf16) -> ()
    }) : (bf16, tensor<256x16xbf16>) -> tensor<256x16xbf16>
    %10 = "linalg.reduce"(%6, %9) <{dimensions = array<i64: 0>}> ({
    ^bb0(%arg8: bf16, %arg9: bf16):
      %12 = "arith.maximumf"(%arg8, %arg9) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "linalg.yield"(%12) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>, tensor<256x16xbf16>) -> tensor<256x16xbf16>
    %11 = "tts.create_ptr"(%arg1, %2) : (tensor<256x16x!tt.ptr<bf16>>, tensor<256x16xi32>) -> tensor<256x16x!tt.ptr<bf16>>
    "tt.store"(%11, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16x!tt.ptr<bf16>>, tensor<256x16xbf16>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<256x16x!tt.ptr<bf16>>'
    tt.store %res, %6 : tensor<256x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: note: see current operation: %12 = "memref.cast"(%arg1) : (tensor<256x16x!tt.ptr<bf16>>) -> memref<?xbf16>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir (7 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save0, %5 : tensor<128x128x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save0, %5 : tensor<128x128x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save1, %6 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: note: see current operation: tt.store %arg4, %20 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save1, %6 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: note: see current operation: tt.store %arg4, %20 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save2, %7 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: note: see current operation: tt.store %arg5, %21 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save2, %7 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: note: see current operation: tt.store %arg5, %21 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save3, %10 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: note: see current operation: tt.store %arg6, %22 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save3, %10 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: note: see current operation: tt.store %arg6, %22 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save4, %11 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: note: see current operation: tt.store %arg7, %23 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save4, %11 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: note: see current operation: tt.store %arg7, %23 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<128x128x!tt.ptr<bf16>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<128x128x!tt.ptr<bf16>>, %arg4: tensor<128x128x!tt.ptr<f32>>, %arg5: tensor<128x128x!tt.ptr<f32>>, %arg6: tensor<128x128x!tt.ptr<f32>>, %arg7: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i32>>) -> tensor<128x128xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f16>>) -> tensor<128x128xf16>
    %14 = "arith.truncf"(%11) : (tensor<128x128xf32>) -> tensor<128x128xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %16 = "arith.sitofp"(%12) : (tensor<128x128xi32>) -> tensor<128x128xf32>
    %17 = "arith.extf"(%13) : (tensor<128x128xf16>) -> tensor<128x128xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
actual processing
processing user
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
tensor<128x128x!tt.ptr<bf16>>
~~~~
processing val
%3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<128x128x!tt.ptr<bf16>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<128x128x!tt.ptr<bf16>>, %arg4: tensor<128x128x!tt.ptr<f32>>, %arg5: tensor<128x128x!tt.ptr<f32>>, %arg6: tensor<128x128x!tt.ptr<f32>>, %arg7: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i32>>) -> tensor<128x128xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f16>>) -> tensor<128x128xf16>
    %14 = "arith.truncf"(%11) : (tensor<128x128xf32>) -> tensor<128x128xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %16 = "arith.sitofp"(%12) : (tensor<128x128xi32>) -> tensor<128x128xf32>
    %17 = "arith.extf"(%13) : (tensor<128x128xf16>) -> tensor<128x128xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %19 = "tts.create_ptr"(%arg3, %4) : (tensor<128x128x!tt.ptr<bf16>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<bf16>>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
    %20 = "tts.create_ptr"(%arg4, %3) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %21 = "tts.create_ptr"(%arg5, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %22 = "tts.create_ptr"(%arg6, %1) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %23 = "tts.create_ptr"(%arg7, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<128x128x!tt.ptr<bf16>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<128x128x!tt.ptr<bf16>>, %arg4: tensor<128x128x!tt.ptr<f32>>, %arg5: tensor<128x128x!tt.ptr<f32>>, %arg6: tensor<128x128x!tt.ptr<f32>>, %arg7: tensor<128x128x!tt.ptr<f32>>, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<128x128xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg24: i32, %arg25: i32):
      "linalg.yield"(%arg24) : (i32) -> ()
    }) : (i32, tensor<128x128xi32>) -> tensor<128x128xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<i32>) -> memref<*xi32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xi32>) -> tensor<128x128x!tt.ptr<i32>>
    %7 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<f16>) -> memref<*xf16>
    %8 = "tts.make_tptr"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf16>) -> tensor<128x128x!tt.ptr<f16>>
    %9 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %10 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i32>>) -> tensor<128x128xi32>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f16>>) -> tensor<128x128xf16>
    %12 = "tensor.empty"() : () -> tensor<128x128xbf16>
    %13 = "linalg.generic"(%9, %12) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg22: f32, %arg23: bf16):
      %29 = "arith.truncf"(%arg22) : (f32) -> bf16
      "linalg.yield"(%29) : (bf16) -> ()
    }) : (tensor<128x128xf32>, tensor<128x128xbf16>) -> tensor<128x128xbf16>
    %14 = "linalg.generic"(%9, %9) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg20: f32, %arg21: f32):
      %28 = "math.exp"(%arg20) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "linalg.yield"(%28) : (f32) -> ()
    }) : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %15 = "tensor.empty"() : () -> tensor<128x128xf32>
    %16 = "linalg.generic"(%10, %15) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg18: i32, %arg19: f32):
      %27 = "arith.sitofp"(%arg18) : (i32) -> f32
      "linalg.yield"(%27) : (f32) -> ()
    }) : (tensor<128x128xi32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %17 = "tensor.empty"() : () -> tensor<128x128xf32>
    %18 = "linalg.generic"(%11, %17) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg16: f16, %arg17: f32):
      %26 = "arith.extf"(%arg16) : (f16) -> f32
      "linalg.yield"(%26) : (f32) -> ()
    }) : (tensor<128x128xf16>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %19 = "linalg.generic"(%9, %9) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg14: f32, %arg15: f32):
      %25 = "math.sqrt"(%arg14) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "linalg.yield"(%25) : (f32) -> ()
    }) : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %20 = "tts.create_ptr"(%arg3, %2) : (tensor<128x128x!tt.ptr<bf16>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<bf16>>
    "tt.store"(%20, %13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<bf16>>, tensor<128x128xbf16>) -> ()
    %21 = "tts.create_ptr"(%arg4, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%21, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    %22 = "tts.create_ptr"(%arg5, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%22, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    %23 = "tts.create_ptr"(%arg6, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%23, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    %24 = "tts.create_ptr"(%arg7, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%24, %19) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<bf16>>'
    tt.store %save0, %5 : tensor<128x128x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: note: see current operation: %23 = "memref.cast"(%arg3) : (tensor<128x128x!tt.ptr<bf16>>) -> memref<?xbf16>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir (8 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %d, %100 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %d, %100 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i1>) -> tensor<128x128x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i1>>) -> tensor<128x128xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i1>) -> tensor<128x128x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i1>>) -> tensor<128x128xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %11 = "tts.create_ptr"(%arg3, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<128x128x!tt.ptr<f32>>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<128x128xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg14: i32, %arg15: i32):
      "linalg.yield"(%arg14) : (i32) -> ()
    }) : (i32, tensor<128x128xi32>) -> tensor<128x128xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<i1>) -> memref<*xi1>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xi1>) -> tensor<128x128x!tt.ptr<i1>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<f32>) -> memref<*xf32>
    %8 = "tts.make_tptr"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>) -> tensor<128x128x!tt.ptr<f32>>
    %9 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i1>>) -> tensor<128x128xi1>
    %10 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %12 = "linalg.generic"(%9, %10, %11, %10) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 3, 1>}> ({
    ^bb0(%arg10: i1, %arg11: f32, %arg12: f32, %arg13: f32):
      %14 = "arith.select"(%arg10, %arg11, %arg12) : (i1, f32, f32) -> f32
      "linalg.yield"(%14) : (f32) -> ()
    }) : (tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %13 = "tts.create_ptr"(%arg3, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%13, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<f32>>'
        tt.store %d, %100 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: note: see current operation: %16 = "memref.cast"(%arg3) : (tensor<128x128x!tt.ptr<f32>>) -> memref<?xf32>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_advance.mlir (9 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_advance.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir
module {
  tt.func public @matmul_kernel_with_block_pointers_01234567891011(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xbf16>
    %c64_i32 = arith.constant 64 : i32
    %c0_i32_2 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = arith.index_cast %arg6 : i32 to index
    %1 = arith.index_cast %arg12 : i32 to index
    %2 = arith.muli %1, %0 : index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = arith.index_cast %arg7 : i32 to index
    %5 = arith.index_cast %arg5 : i32 to index
    %6 = arith.muli %4, %c64 : index
    %7:3 = scf.for %arg14 = %c0_i32_2 to %arg5 step %c64_i32 iter_args(%arg15 = %cst, %arg16 = %6, %arg17 = %2) -> (tensor<128x64xbf16>, index, index)  : i32 {
      %16 = tts.make_tptr %arg0 to sizes: [128, 64], strides: [%0, %4], offsets: [%arg17, %c0], shape: [%3, %5], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
      %17 = tts.make_tptr %arg0 to sizes: [128, 64], strides: [%0, %4], offsets: [%2, %arg16], shape: [%3, %5], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
      %18 = "tts.load"(%17) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %19 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %20 = arith.addf %18, %19 : tensor<128x64xbf16>
      %21 = arith.addf %arg15, %20 : tensor<128x64xbf16>
      %22 = arith.muli %4, %c64 : index
      %23 = arith.addi %22, %arg16 : index
      %24 = arith.muli %0, %c64 : index
      %25 = arith.addi %24, %arg17 : index
      scf.yield %21, %23, %25 : tensor<128x64xbf16>, index, index
    }
    %8 = arith.muli %arg13, %c256_i32 : i32
    %9 = arith.index_cast %arg10 : i32 to index
    %10 = arith.muli %1, %9 : index
    %11 = arith.index_cast %arg11 : i32 to index
    %12 = arith.index_cast %8 : i32 to index
    %13 = arith.muli %12, %11 : index
    %14 = arith.index_cast %arg4 : i32 to index
    %15 = tts.make_tptr %arg2 to sizes: [128, 64], strides: [%9, %11], offsets: [%10, %13], shape: [%3, %14], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
    "tts.store"(%15, %7#0) <{static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>, tensor<128x64xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32_1 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @matmul_kernel_with_block_pointers_01234567891011(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xbf16>
    %c64_i32 = arith.constant 64 : i32
    %c0_i32_2 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = arith.index_cast %arg6 : i32 to index
    %1 = arith.index_cast %arg12 : i32 to index
    %2 = arith.muli %1, %0 : index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = arith.index_cast %arg7 : i32 to index
    %5 = arith.index_cast %arg5 : i32 to index
    %6 = arith.muli %4, %c64 : index
    %7:3 = scf.for %arg14 = %c0_i32_2 to %arg5 step %c64_i32 iter_args(%arg15 = %cst, %arg16 = %6, %arg17 = %2) -> (tensor<128x64xbf16>, index, index)  : i32 {
      %16 = tts.make_tptr %arg0 to sizes: [128, 64], strides: [%0, %4], offsets: [%arg17, %c0], shape: [%3, %5], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
      %17 = tts.make_tptr %arg0 to sizes: [128, 64], strides: [%0, %4], offsets: [%2, %arg16], shape: [%3, %5], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
      %18 = "tts.load"(%17) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %19 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %20 = arith.addf %18, %19 : tensor<128x64xbf16>
      %21 = arith.addf %arg15, %20 : tensor<128x64xbf16>
      %22 = arith.muli %4, %c64 : index
      %23 = arith.addi %22, %arg16 : index
      %24 = arith.muli %0, %c64 : index
      %25 = arith.addi %24, %arg17 : index
      scf.yield %21, %23, %25 : tensor<128x64xbf16>, index, index
    }
    %8 = arith.muli %arg13, %c256_i32 : i32
    %9 = arith.index_cast %arg10 : i32 to index
    %10 = arith.muli %1, %9 : index
    %11 = arith.index_cast %arg11 : i32 to index
    %12 = arith.index_cast %8 : i32 to index
    %13 = arith.muli %12, %11 : index
    %14 = arith.index_cast %arg4 : i32 to index
    %15 = tts.make_tptr %arg2 to sizes: [128, 64], strides: [%9, %11], offsets: [%10, %13], shape: [%3, %14], order: [1, 0] : <bf16> to !tt.ptr<tensor<128x64xbf16>>
    "tts.store"(%15, %7#0) <{static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>, tensor<128x64xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "matmul_kernel_with_block_pointers_01234567891011"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32, %arg16: i32, %arg17: i32, %arg18: i32, %arg19: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 64 : index}> : () -> index
    %2 = "arith.constant"() <{value = 0 : index}> : () -> index
    %3 = "arith.constant"() <{value = 0.000000e+00 : bf16}> : () -> bf16
    %4 = "tensor.empty"() : () -> tensor<128x64xbf16>
    %5 = "linalg.fill"(%3, %4) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg30: bf16, %arg31: bf16):
      "linalg.yield"(%arg30) : (bf16) -> ()
    }) : (bf16, tensor<128x64xbf16>) -> tensor<128x64xbf16>
    %6 = "arith.constant"() <{value = 64 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %8 = "arith.index_cast"(%arg6) : (i32) -> index
    %9 = "arith.index_cast"(%arg12) : (i32) -> index
    %10 = "arith.muli"(%9, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %11 = "arith.index_cast"(%arg3) : (i32) -> index
    %12 = "arith.index_cast"(%arg7) : (i32) -> index
    %13 = "arith.index_cast"(%arg5) : (i32) -> index
    %14 = "arith.muli"(%12, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %15:3 = "scf.for"(%0, %arg5, %6, %5, %14, %10) ({
    ^bb0(%arg20: i32, %arg21: tensor<128x64xbf16>, %arg22: index, %arg23: index):
      %25 = "tts.make_tptr"(%arg0, %8, %12, %arg23, %2, %11, %13) <{operandSegmentSizes = array<i32: 1, 2, 2, 2>, order = array<i32: 1, 0>, sizes = array<i64: 128, 64>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: -9223372036854775808, -9223372036854775808>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index, index, index, index, index) -> !tt.ptr<tensor<128x64xbf16>>
      %26 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
      %27 = "tts.make_tptr"(%26, %8, %12, %10, %arg22, %11, %13) <{operandSegmentSizes = array<i32: 1, 2, 2, 2>, order = array<i32: 1, 0>, sizes = array<i64: 128, 64>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: -9223372036854775808, -9223372036854775808>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xbf16>, index, index, index, index, index, index) -> !tt.ptr<tensor<128x64xbf16>>
      %28 = "tts.load"(%27) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %29 = "tts.load"(%25) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>) -> tensor<128x64xbf16>
      %30 = "linalg.generic"(%28, %29, %28) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg27: bf16, %arg28: bf16, %arg29: bf16):
        %37 = "arith.addf"(%arg27, %arg28) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
        "linalg.yield"(%37) : (bf16) -> ()
      }) : (tensor<128x64xbf16>, tensor<128x64xbf16>, tensor<128x64xbf16>) -> tensor<128x64xbf16>
      %31 = "linalg.generic"(%arg21, %30, %arg21) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg24: bf16, %arg25: bf16, %arg26: bf16):
        %36 = "arith.addf"(%arg24, %arg25) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
        "linalg.yield"(%36) : (bf16) -> ()
      }) : (tensor<128x64xbf16>, tensor<128x64xbf16>, tensor<128x64xbf16>) -> tensor<128x64xbf16>
      %32 = "arith.muli"(%12, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %33 = "arith.addi"(%32, %arg22) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %34 = "arith.muli"(%8, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %35 = "arith.addi"(%34, %arg23) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%31, %33, %35) : (tensor<128x64xbf16>, index, index) -> ()
    }) : (i32, i32, i32, tensor<128x64xbf16>, index, index) -> (tensor<128x64xbf16>, index, index)
    %16 = "arith.muli"(%arg13, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %17 = "arith.index_cast"(%arg10) : (i32) -> index
    %18 = "arith.muli"(%9, %17) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %19 = "arith.index_cast"(%arg11) : (i32) -> index
    %20 = "arith.index_cast"(%16) : (i32) -> index
    %21 = "arith.muli"(%20, %19) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %22 = "arith.index_cast"(%arg4) : (i32) -> index
    %23 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %24 = "tts.make_tptr"(%23, %17, %19, %18, %21, %11, %22) <{operandSegmentSizes = array<i32: 1, 2, 2, 2>, order = array<i32: 1, 0>, sizes = array<i64: 128, 64>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: -9223372036854775808, -9223372036854775808>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xbf16>, index, index, index, index, index, index) -> !tt.ptr<tensor<128x64xbf16>>
    "tts.store"(%24, %15#0) <{static_mask_dims = array<i64>}> : (!tt.ptr<tensor<128x64xbf16>>, tensor<128x64xbf16>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir:1 offset :16:12: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<bf16>'
    %7:3 = scf.for %arg14 = %c0_i32 to %arg5 step %c64_i32 iter_args(%arg15 = %6, %arg16 = %5, %arg17 = %4) -> (tensor<128x64xbf16>, !tt.ptr<tensor<128x64xbf16>>, !tt.ptr<tensor<128x64xbf16>>)  : i32 {
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir:1 offset :16:12: note: see current operation: %27 = "memref.reinterpret_cast"(%arg0, %26, %8, %12) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 128, 64>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index, index) -> memref<128x64xbf16, strided<[?, ?], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/block_ptr_advance.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/kernel-02-fused-softmax.mlir (10 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/nested_loops.mlir (11 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/nested_loops.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
module {
  tt.func public @nested2_complex_body(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c3 = arith.constant 3 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg2, %c2_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = arith.addi %arg5, %c1 : index
      %6 = arith.addi %arg6, %c1 : index
      %7:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %5, %arg9 = %6) -> (index, index)  : i32 {
        %12 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%12, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg8, %c3 : index
        %16 = arith.addi %arg9, %c3 : index
        scf.yield %15, %16 : index, index
      }
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %8, %c1 : index
      %10 = arith.addi %arg6, %3 : index
      %11 = arith.addi %10, %c1 : index
      scf.yield %9, %11 : index, index
    }
    tt.return
  }
  tt.func public @nested2_use_loop_results(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg3, %c4_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg6, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      "tts.store"(%5, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %arg6, %3 : index
      %10:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %8, %arg9 = %9) -> (index, index)  : i32 {
        %11 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %14 = arith.addi %arg8, %3 : index
        %15 = arith.addi %arg9, %3 : index
        scf.yield %14, %15 : index, index
      }
      scf.yield %10#0, %10#1 : index, index
    }
    tt.return
  }
  tt.func public @nested3(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %10 = arith.addi %arg9, %4 : index
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%10, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %10, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %14 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%14, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %18 = arith.addi %arg15, %4 : index
          %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %18, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          scf.yield %15, %23, %22 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        scf.yield %13#0, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %9 = arith.addi %8#0, %4 : index
      scf.yield %9, %8#1, %8#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
  tt.func public @nested_use_same_level_loop_result(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5) -> (index)  : i32 {
        %9 = arith.addi %arg9, %4 : index
        scf.yield %9 : index
      }
      %7:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %6, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %9 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %10 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %11 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %12 = arith.addi %arg9, %4 : index
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%12, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%9, %11) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg11, %4 : index
        %16 = arith.addi %15, %4 : index
        %17 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        "tts.store"(%17, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %18 = arith.addi %16, %4 : index
        %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %20 = arith.addi %12, %4 : index
        scf.yield %20, %19, %18 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %8 = arith.addi %7#0, %4 : index
      scf.yield %8, %7#1, %7#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @nested2_complex_body(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c3 = arith.constant 3 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg2, %c2_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = arith.addi %arg5, %c1 : index
      %6 = arith.addi %arg6, %c1 : index
      %7:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %5, %arg9 = %6) -> (index, index)  : i32 {
        %12 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%12, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg8, %c3 : index
        %16 = arith.addi %arg9, %c3 : index
        scf.yield %15, %16 : index, index
      }
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %8, %c1 : index
      %10 = arith.addi %arg6, %3 : index
      %11 = arith.addi %10, %c1 : index
      scf.yield %9, %11 : index, index
    }
    tt.return
  }
  tt.func public @nested2_use_loop_results(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg3, %c4_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg6, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      "tts.store"(%5, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %arg6, %3 : index
      %10:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %8, %arg9 = %9) -> (index, index)  : i32 {
        %11 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %14 = arith.addi %arg8, %3 : index
        %15 = arith.addi %arg9, %3 : index
        scf.yield %14, %15 : index, index
      }
      scf.yield %10#0, %10#1 : index, index
    }
    tt.return
  }
  tt.func public @nested3(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %10 = arith.addi %arg9, %4 : index
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%10, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %10, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %14 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%14, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %18 = arith.addi %arg15, %4 : index
          %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %18, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          scf.yield %15, %23, %22 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        scf.yield %13#0, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %9 = arith.addi %8#0, %4 : index
      scf.yield %9, %8#1, %8#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
  tt.func public @nested_use_same_level_loop_result(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5) -> (index)  : i32 {
        %9 = arith.addi %arg9, %4 : index
        scf.yield %9 : index
      }
      %7:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %6, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %9 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %10 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %11 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %12 = arith.addi %arg9, %4 : index
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%12, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%9, %11) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg11, %4 : index
        %16 = arith.addi %15, %4 : index
        %17 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        "tts.store"(%17, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %18 = arith.addi %16, %4 : index
        %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %20 = arith.addi %12, %4 : index
        scf.yield %20, %19, %18 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %8 = arith.addi %7#0, %4 : index
      scf.yield %8, %7#1, %7#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "nested2_complex_body"}> ({
  ^bb0(%arg58: !tt.ptr<f32>, %arg59: !tt.ptr<f32>, %arg60: i32, %arg61: i32, %arg62: i32, %arg63: i32, %arg64: i32, %arg65: i32, %arg66: i32, %arg67: i32):
    %81 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %82 = "arith.constant"() <{value = 3 : index}> : () -> index
    %83 = "arith.constant"() <{value = 1 : index}> : () -> index
    %84 = "arith.constant"() <{value = 0 : index}> : () -> index
    %85 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %86 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %87 = "arith.index_cast"(%arg60) : (i32) -> index
    %88 = "arith.index_cast"(%arg61) : (i32) -> index
    %89 = "arith.muli"(%arg60, %86) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %90 = "arith.index_cast"(%89) : (i32) -> index
    %91:2 = "scf.for"(%81, %86, %85, %84, %84) ({
    ^bb0(%arg68: i32, %arg69: index, %arg70: index):
      %92 = "arith.addi"(%arg69, %83) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %93 = "arith.addi"(%arg70, %83) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %94:2 = "scf.for"(%81, %86, %85, %92, %93) ({
      ^bb0(%arg71: i32, %arg72: index, %arg73: index):
        %99 = "builtin.unrealized_conversion_cast"(%arg59) : (!tt.ptr<f32>) -> memref<*xf32>
        %100 = "tts.make_tptr"(%99, %87, %88, %arg73, %84) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %101 = "builtin.unrealized_conversion_cast"(%arg58) : (!tt.ptr<f32>) -> memref<*xf32>
        %102 = "tts.make_tptr"(%101, %87, %88, %arg72, %84) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %103 = "tts.load"(%102) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%100, %103) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %104 = "arith.addi"(%arg72, %82) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %105 = "arith.addi"(%arg73, %82) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%104, %105) : (index, index) -> ()
      }) : (i32, i32, i32, index, index) -> (index, index)
      %95 = "arith.addi"(%arg69, %90) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %96 = "arith.addi"(%95, %83) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %97 = "arith.addi"(%arg70, %90) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %98 = "arith.addi"(%97, %83) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%96, %98) : (index, index) -> ()
    }) : (i32, i32, i32, index, index) -> (index, index)
    "func.return"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "nested2_use_loop_results"}> ({
  ^bb0(%arg42: !tt.ptr<f32>, %arg43: !tt.ptr<f32>, %arg44: i32, %arg45: i32, %arg46: i32, %arg47: i32, %arg48: i32, %arg49: i32, %arg50: i32, %arg51: i32):
    %58 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %59 = "arith.constant"() <{value = 0 : index}> : () -> index
    %60 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %61 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %62 = "arith.constant"() <{value = 4 : i32}> : () -> i32
    %63 = "arith.index_cast"(%arg44) : (i32) -> index
    %64 = "arith.index_cast"(%arg45) : (i32) -> index
    %65 = "arith.muli"(%arg45, %62) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %66 = "arith.index_cast"(%65) : (i32) -> index
    %67:2 = "scf.for"(%58, %61, %60, %59, %59) ({
    ^bb0(%arg52: i32, %arg53: index, %arg54: index):
      %68 = "tts.make_tptr"(%arg43, %63, %64, %arg54, %59) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
      %69 = "tts.make_tptr"(%arg42, %63, %64, %arg53, %59) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
      %70 = "tts.load"(%69) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      "tts.store"(%68, %70) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
      %71 = "arith.addi"(%arg53, %66) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %72 = "arith.addi"(%arg54, %66) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %73:2 = "scf.for"(%58, %61, %60, %71, %72) ({
      ^bb0(%arg55: i32, %arg56: index, %arg57: index):
        %74 = "builtin.unrealized_conversion_cast"(%arg43) : (!tt.ptr<f32>) -> memref<*xf32>
        %75 = "tts.make_tptr"(%74, %63, %64, %arg57, %59) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %76 = "builtin.unrealized_conversion_cast"(%arg42) : (!tt.ptr<f32>) -> memref<*xf32>
        %77 = "tts.make_tptr"(%76, %63, %64, %arg56, %59) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %78 = "tts.load"(%77) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%75, %78) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %79 = "arith.addi"(%arg56, %66) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %80 = "arith.addi"(%arg57, %66) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%79, %80) : (index, index) -> ()
      }) : (i32, i32, i32, index, index) -> (index, index)
      "scf.yield"(%73#0, %73#1) : (index, index) -> ()
    }) : (i32, i32, i32, index, index) -> (index, index)
    "func.return"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "nested3"}> ({
  ^bb0(%arg20: !tt.ptr<f32>, %arg21: !tt.ptr<f32>, %arg22: i32, %arg23: i32, %arg24: i32, %arg25: i32, %arg26: i32, %arg27: i32, %arg28: i32, %arg29: i32):
    %28 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %29 = "arith.constant"() <{value = 0 : index}> : () -> index
    %30 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %31 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %32 = "arith.index_cast"(%arg22) : (i32) -> index
    %33 = "arith.index_cast"(%arg23) : (i32) -> index
    %34 = "tts.make_tptr"(%arg21, %32, %33) <{operandSegmentSizes = array<i32: 1, 2, 0, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> tensor<2x2x!tt.ptr<f32>>
    %35 = "arith.muli"(%arg23, %31) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %36 = "arith.index_cast"(%35) : (i32) -> index
    %37:3 = "scf.for"(%28, %31, %30, %29, %34, %29) ({
    ^bb0(%arg30: i32, %arg31: index, %arg32: tensor<2x2x!tt.ptr<f32>>, %arg33: index):
      %38 = "tts.make_tptr"(%arg20, %32, %33, %arg31, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
      %39 = "tts.load"(%38) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %40:3 = "scf.for"(%28, %31, %30, %arg31, %arg32, %arg33) ({
      ^bb0(%arg34: i32, %arg35: index, %arg36: tensor<2x2x!tt.ptr<f32>>, %arg37: index):
        %42 = "arith.addi"(%arg35, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %43 = "tts.make_tptr"(%arg20, %32, %33, %42, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %44 = "tts.load"(%43) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %45:3 = "scf.for"(%28, %31, %30, %42, %arg36, %arg37) ({
        ^bb0(%arg38: i32, %arg39: index, %arg40: tensor<2x2x!tt.ptr<f32>>, %arg41: index):
          %46 = "tts.make_tptr"(%arg21, %32, %33, %arg41, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %47 = "arith.addi"(%arg39, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %48 = "builtin.unrealized_conversion_cast"(%arg20) : (!tt.ptr<f32>) -> memref<*xf32>
          %49 = "tts.make_tptr"(%48, %32, %33, %47, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %50 = "tts.load"(%49) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%46, %39) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %51 = "arith.addi"(%arg41, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %52 = "tts.make_tptr"(%arg21, %32, %33, %51, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%52, %44) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %53 = "arith.addi"(%51, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %54 = "tts.make_tptr"(%arg21, %32, %33, %53, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%54, %50) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %55 = "arith.addi"(%53, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %56 = "builtin.unrealized_conversion_cast"(%arg21) : (!tt.ptr<f32>) -> memref<*xf32>
          %57 = "tts.make_tptr"(%56, %32, %33, %55, %29) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          "scf.yield"(%47, %57, %55) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
        }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
        "scf.yield"(%45#0, %45#1, %45#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
      %41 = "arith.addi"(%40#0, %36) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%41, %40#1, %40#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    "func.return"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "nested_use_same_level_loop_result"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : index}> : () -> index
    %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %4 = "arith.index_cast"(%arg2) : (i32) -> index
    %5 = "arith.index_cast"(%arg3) : (i32) -> index
    %6 = "tts.make_tptr"(%arg1, %4, %5) <{operandSegmentSizes = array<i32: 1, 2, 0, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> tensor<2x2x!tt.ptr<f32>>
    %7 = "arith.muli"(%arg3, %3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9:3 = "scf.for"(%0, %3, %2, %1, %6, %1) ({
    ^bb0(%arg10: i32, %arg11: index, %arg12: tensor<2x2x!tt.ptr<f32>>, %arg13: index):
      %10 = "scf.for"(%0, %3, %2, %arg11) ({
      ^bb0(%arg18: i32, %arg19: index):
        %27 = "arith.addi"(%arg19, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%27) : (index) -> ()
      }) : (i32, i32, i32, index) -> index
      %11:3 = "scf.for"(%0, %3, %2, %10, %arg12, %arg13) ({
      ^bb0(%arg14: i32, %arg15: index, %arg16: tensor<2x2x!tt.ptr<f32>>, %arg17: index):
        %13 = "tts.make_tptr"(%arg1, %4, %5, %arg17, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.make_tptr"(%arg0, %4, %5, %arg15, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %15 = "tts.load"(%14) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %16 = "arith.addi"(%arg15, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %17 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
        %18 = "tts.make_tptr"(%17, %4, %5, %16, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%13, %15) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %20 = "arith.addi"(%arg17, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %21 = "arith.addi"(%20, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %22 = "tts.make_tptr"(%arg1, %4, %5, %21, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        "tts.store"(%22, %19) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %23 = "arith.addi"(%21, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %24 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
        %25 = "tts.make_tptr"(%24, %4, %5, %23, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %26 = "arith.addi"(%16, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%26, %25, %23) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
      %12 = "arith.addi"(%11#0, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%12, %11#1, %11#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir:1 offset :104:11: error: failed to legalize unresolved materialization from ('memref<2x2xf32, strided<[?, ?]>>') to 'tensor<2x2x!tt.ptr<f32>>' that remained live after conversion
    %15 = tt.addptr %14, %8 : tensor<2x2x!tt.ptr<f32>>, tensor<2x2xi32>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir:1 offset :104:11: note: see current operation: %7 = "builtin.unrealized_conversion_cast"(%6) : (memref<2x2xf32, strided<[?, ?]>>) -> tensor<2x2x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir:1 offset :109:13: note: see existing live user here: 
%10:3 = "scf.for"(%0, %3, %2, %1, %7, %1) ({
^bb0(%arg10: i32, %arg11: index, %arg12: tensor<2x2x!tt.ptr<f32>>, %arg13: index):
  %11 = "arith.addi"(%arg11, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %12 = "memref.reinterpret_cast"(%arg0, %11, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
  %13 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
  "memref.copy"(%12, %13) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
  %14 = "bufferization.to_tensor"(%13) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
  %15:3 = "scf.for"(%0, %3, %2, %arg11, %arg12, %arg13) ({
  ^bb0(%arg14: i32, %arg15: index, %arg16: tensor<2x2x!tt.ptr<f32>>, %arg17: index):
    %17 = "arith.addi"(%arg15, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %18 = "arith.addi"(%17, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %19 = "memref.reinterpret_cast"(%arg0, %18, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
    %20 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
    "memref.copy"(%19, %20) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
    %21 = "bufferization.to_tensor"(%20) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
    %22:3 = "scf.for"(%0, %3, %2, %17, %arg16, %arg17) ({
    ^bb0(%arg18: i32, %arg19: index, %arg20: tensor<2x2x!tt.ptr<f32>>, %arg21: index):
      %23 = "arith.addi"(%arg21, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %24 = "memref.reinterpret_cast"(%arg1, %23, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %25 = "arith.addi"(%arg19, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %26 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
      %27 = "arith.addi"(%25, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %28 = "memref.reinterpret_cast"(%26, %27, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %29 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
      "memref.copy"(%28, %29) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
      %30 = "bufferization.to_tensor"(%29) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
      "bufferization.materialize_in_destination"(%14, %24) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %31 = "arith.addi"(%arg21, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %32 = "arith.addi"(%31, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %33 = "memref.reinterpret_cast"(%arg1, %32, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      "bufferization.materialize_in_destination"(%21, %33) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %34 = "arith.addi"(%31, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %35 = "arith.addi"(%34, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %36 = "memref.reinterpret_cast"(%arg1, %35, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      "bufferization.materialize_in_destination"(%30, %36) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %37 = "arith.addi"(%34, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %38 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
      %39 = "arith.addi"(%37, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %40 = "memref.reinterpret_cast"(%38, %39, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %41 = "builtin.unrealized_conversion_cast"(%40) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
      "scf.yield"(%25, %41, %37) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    "scf.yield"(%22#0, %22#1, %22#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
  }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
  %16 = "arith.addi"(%15#0, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  "scf.yield"(%16, %15#1, %15#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
}) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    %20:2 = scf.for %arg4 = %c0_i32 to %c2_i32 step %c1_i32 iter_args(%arg5 = %11, %arg6 = %15) -> (tensor<2x2x!tt.ptr<f32>>, tensor<2x2x!tt.ptr<f32>>)  : i32 {
            ^
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir (12 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir (13 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :59:5: remark: PtrAnalysis: scalar storeOp will not be rewritten
    tt.store %22, %11 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :59:5: note: see current operation: tt.store %24, %13 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :59:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %22, %11 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :59:5: note: see current operation: tt.store %24, %13 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :61:5: remark: PtrAnalysis: scalar storeOp will not be rewritten
    tt.store %23, %21 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :61:5: note: see current operation: tt.store %26, %24 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :61:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %23, %21 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :61:5: note: see current operation: tt.store %26, %24 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, i32, i32, f32) -> (), sym_name = "_layer_norm_fwd_fused_0123456789", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>, %arg6: i32, %arg7: i32, %arg8: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %7 = "arith.constant"() <{value = 256 : index}> : () -> index
    %8 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<256xf32>}> : () -> tensor<256xf32>
    %9 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %10 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
    %12 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %13 = "arith.muli"(%12, %arg6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %14 = "arith.index_cast"(%13) : (i32) -> index
    %15 = "tt.make_range"() <{end = 256 : i32, start = 0 : i32}> : () -> tensor<256xi32>
    %16 = "tt.splat"(%arg7) : (i32) -> tensor<256xi32>
    %17 = "scf.for"(%10, %arg7, %9, %8) ({
    ^bb0(%arg16: i32, %arg17: tensor<256xf32>):
      %65 = "arith.index_cast"(%arg16) : (i32) -> index
      %66 = "arith.addi"(%14, %65) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %67 = "tts.make_tptr"(%arg0, %66) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %68 = "arith.addi"(%65, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %69 = "arith.index_cast"(%arg7) : (i32) -> index
      %70 = "arith.minsi"(%68, %69) : (index, index) -> index
      %71 = "arith.subi"(%70, %65) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %72 = "tts.load"(%67, %71, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %73 = "arith.addf"(%arg17, %72) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%73) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %18 = "tt.reduce"(%17) <{axis = 0 : i32}> ({
    ^bb0(%arg14: f32, %arg15: f32):
      %64 = "arith.addf"(%arg14, %arg15) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%64) : (f32) -> ()
    }) : (tensor<256xf32>) -> f32
    %19 = "arith.sitofp"(%arg7) : (i32) -> f32
    %20 = "arith.divf"(%18, %19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %21 = "tt.splat"(%20) : (f32) -> tensor<256xf32>
    %22 = "scf.for"(%10, %arg7, %9, %8) ({
    ^bb0(%arg12: i32, %arg13: tensor<256xf32>):
      %49 = "tt.splat"(%arg12) : (i32) -> tensor<256xi32>
      %50 = "arith.addi"(%49, %15) <{overflowFlags = #arith.overflow<none>}> : (tensor<256xi32>, tensor<256xi32>) -> tensor<256xi32>
      %51 = "arith.cmpi"(%50, %16) <{predicate = 2 : i64}> : (tensor<256xi32>, tensor<256xi32>) -> tensor<256xi1>
      %52 = "arith.index_cast"(%arg12) : (i32) -> index
      %53 = "arith.addi"(%14, %52) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %54 = "tts.make_tptr"(%arg0, %53) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %55 = "arith.addi"(%52, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %56 = "arith.index_cast"(%arg7) : (i32) -> index
      %57 = "arith.minsi"(%55, %56) : (index, index) -> index
      %58 = "arith.subi"(%57, %52) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %59 = "tts.load"(%54, %58, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %60 = "arith.subf"(%59, %21) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %61 = "arith.select"(%51, %60, %8) : (tensor<256xi1>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %62 = "arith.mulf"(%61, %61) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %63 = "arith.addf"(%arg13, %62) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%63) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %23 = "tt.reduce"(%22) <{axis = 0 : i32}> ({
    ^bb0(%arg10: f32, %arg11: f32):
      %48 = "arith.addf"(%arg10, %arg11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%48) : (f32) -> ()
    }) : (tensor<256xf32>) -> f32
    %24 = "arith.divf"(%23, %19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %25 = "arith.addf"(%24, %arg8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %26 = "math.sqrt"(%25) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %27 = "arith.divf"(%11, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %28 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
    "tt.store"(%28, %20) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %29 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
    "tt.store"(%29, %27) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %30 = "tt.splat"(%27) : (f32) -> tensor<256xf32>
    "scf.for"(%10, %arg7, %9) ({
    ^bb0(%arg9: i32):
      %31 = "arith.index_cast"(%arg9) : (i32) -> index
      %32 = "tts.make_tptr"(%arg2, %31) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %33 = "arith.addi"(%31, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %34 = "arith.index_cast"(%arg7) : (i32) -> index
      %35 = "arith.minsi"(%33, %34) : (index, index) -> index
      %36 = "arith.subi"(%35, %31) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %37 = "tts.load"(%32, %36) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %38 = "tts.make_tptr"(%arg3, %31) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %39 = "tts.load"(%38, %36) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %40 = "arith.addi"(%14, %31) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %41 = "tts.make_tptr"(%arg0, %40) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %42 = "tts.load"(%41, %36, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %43 = "arith.subf"(%42, %21) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %44 = "arith.mulf"(%43, %30) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %45 = "arith.mulf"(%44, %37) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %46 = "arith.addf"(%45, %39) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %47 = "tts.make_tptr"(%arg1, %40) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      "tts.store"(%47, %46, %36) <{static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, tensor<256xf32>, index) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%28 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%28 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%30 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%30 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%29 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%29, %20) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
actual processing
processing user
"tt.store"(%29, %20) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
~~~~
processing val
%32 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%32, %27) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
actual processing
processing user
"tt.store"(%32, %27) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, i32, i32, f32) -> (), sym_name = "_layer_norm_fwd_fused_0123456789", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>, %arg6: i32, %arg7: i32, %arg8: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %7 = "arith.constant"() <{value = 256 : index}> : () -> index
    %8 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<256xf32>}> : () -> tensor<256xf32>
    %9 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %10 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
    %12 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %13 = "arith.muli"(%12, %arg6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %14 = "arith.index_cast"(%13) : (i32) -> index
    %15 = "tt.make_range"() <{end = 256 : i32, start = 0 : i32}> : () -> tensor<256xi32>
    %16 = "tt.splat"(%arg7) : (i32) -> tensor<256xi32>
    %17 = "scf.for"(%10, %arg7, %9, %8) ({
    ^bb0(%arg16: i32, %arg17: tensor<256xf32>):
      %69 = "arith.index_cast"(%arg16) : (i32) -> index
      %70 = "arith.addi"(%14, %69) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %71 = "tts.make_tptr"(%arg0, %70) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %72 = "arith.addi"(%69, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %73 = "arith.index_cast"(%arg7) : (i32) -> index
      %74 = "arith.minsi"(%72, %73) : (index, index) -> index
      %75 = "arith.subi"(%74, %69) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %76 = "tts.load"(%71, %75, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %77 = "arith.addf"(%arg17, %76) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%77) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %18 = "tt.reduce"(%17) <{axis = 0 : i32}> ({
    ^bb0(%arg14: f32, %arg15: f32):
      %68 = "arith.addf"(%arg14, %arg15) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%68) : (f32) -> ()
    }) : (tensor<256xf32>) -> f32
    %19 = "arith.sitofp"(%arg7) : (i32) -> f32
    %20 = "arith.divf"(%18, %19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %21 = "tt.splat"(%20) : (f32) -> tensor<256xf32>
    %22 = "scf.for"(%10, %arg7, %9, %8) ({
    ^bb0(%arg12: i32, %arg13: tensor<256xf32>):
      %53 = "tt.splat"(%arg12) : (i32) -> tensor<256xi32>
      %54 = "arith.addi"(%53, %15) <{overflowFlags = #arith.overflow<none>}> : (tensor<256xi32>, tensor<256xi32>) -> tensor<256xi32>
      %55 = "arith.cmpi"(%54, %16) <{predicate = 2 : i64}> : (tensor<256xi32>, tensor<256xi32>) -> tensor<256xi1>
      %56 = "arith.index_cast"(%arg12) : (i32) -> index
      %57 = "arith.addi"(%14, %56) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %58 = "tts.make_tptr"(%arg0, %57) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %59 = "arith.addi"(%56, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %60 = "arith.index_cast"(%arg7) : (i32) -> index
      %61 = "arith.minsi"(%59, %60) : (index, index) -> index
      %62 = "arith.subi"(%61, %56) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %63 = "tts.load"(%58, %62, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %64 = "arith.subf"(%63, %21) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %65 = "arith.select"(%55, %64, %8) : (tensor<256xi1>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %66 = "arith.mulf"(%65, %65) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %67 = "arith.addf"(%arg13, %66) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%67) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %23 = "tt.reduce"(%22) <{axis = 0 : i32}> ({
    ^bb0(%arg10: f32, %arg11: f32):
      %52 = "arith.addf"(%arg10, %arg11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%52) : (f32) -> ()
    }) : (tensor<256xf32>) -> f32
    %24 = "arith.divf"(%23, %19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %25 = "arith.addf"(%24, %arg8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %26 = "math.sqrt"(%25) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %27 = "arith.divf"(%11, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %28 = "arith.addi"(%1, %12) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %29 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
    %30 = "tts.create_ptr"(%arg4, %28) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%29, %20) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %31 = "arith.addi"(%0, %12) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %32 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
    %33 = "tts.create_ptr"(%arg5, %31) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%32, %27) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %34 = "tt.splat"(%27) : (f32) -> tensor<256xf32>
    "scf.for"(%10, %arg7, %9) ({
    ^bb0(%arg9: i32):
      %35 = "arith.index_cast"(%arg9) : (i32) -> index
      %36 = "tts.make_tptr"(%arg2, %35) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %37 = "arith.addi"(%35, %7) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %38 = "arith.index_cast"(%arg7) : (i32) -> index
      %39 = "arith.minsi"(%37, %38) : (index, index) -> index
      %40 = "arith.subi"(%39, %35) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %41 = "tts.load"(%36, %40) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %42 = "tts.make_tptr"(%arg3, %35) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %43 = "tts.load"(%42, %40) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %44 = "arith.addi"(%14, %35) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %45 = "tts.make_tptr"(%arg0, %44) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %46 = "tts.load"(%45, %40, %6) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %47 = "arith.subf"(%46, %21) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %48 = "arith.mulf"(%47, %34) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %49 = "arith.mulf"(%48, %41) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %50 = "arith.addf"(%49, %43) <{fastmath = #arith.fastmath<none>}> : (tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %51 = "tts.make_tptr"(%arg1, %44) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      "tts.store"(%51, %50, %40) <{static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, tensor<256xf32>, index) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%32 = "tt.addptr"(%0, %12) : (i32, i32) -> !tt.ptr<f32>
deleting
%29 = "tt.addptr"(%1, %12) : (i32, i32) -> !tt.ptr<f32>
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, i32, i32, f32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "_layer_norm_fwd_fused_0123456789"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>, %arg6: i32, %arg7: i32, %arg8: f32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %2 = "arith.constant"() <{value = 256 : index}> : () -> index
    %3 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %4 = "tensor.empty"() : () -> tensor<256xf32>
    %5 = "linalg.fill"(%3, %4) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg67: f32, %arg68: f32):
      "linalg.yield"(%arg67) : (f32) -> ()
    }) : (f32, tensor<256xf32>) -> tensor<256xf32>
    %6 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
    %8 = "arith.muli"(%arg12, %arg6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "arith.index_cast"(%8) : (i32) -> index
    %10 = "tensor.empty"() : () -> tensor<256xi32>
    %11 = "linalg.generic"(%10) <{indexing_maps = [affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 0, 1>}> ({
    ^bb0(%arg66: i32):
      %98 = "linalg.index"() <{dim = 0 : i64}> : () -> index
      %99 = "arith.index_cast"(%98) : (index) -> i32
      "linalg.yield"(%99) : (i32) -> ()
    }) : (tensor<256xi32>) -> tensor<256xi32>
    %12 = "tensor.empty"() : () -> tensor<256xi32>
    %13 = "linalg.fill"(%arg7, %12) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg64: i32, %arg65: i32):
      "linalg.yield"(%arg64) : (i32) -> ()
    }) : (i32, tensor<256xi32>) -> tensor<256xi32>
    %14 = "scf.for"(%0, %arg7, %6, %5) ({
    ^bb0(%arg59: i32, %arg60: tensor<256xf32>):
      %88 = "arith.index_cast"(%arg59) : (i32) -> index
      %89 = "arith.addi"(%9, %88) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %90 = "tts.make_tptr"(%arg0, %89) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %91 = "arith.addi"(%88, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %92 = "arith.index_cast"(%arg7) : (i32) -> index
      %93 = "arith.minsi"(%91, %92) : (index, index) -> index
      %94 = "arith.subi"(%93, %88) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %95 = "tts.load"(%90, %94, %1) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %96 = "linalg.generic"(%arg60, %95, %arg60) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg61: f32, %arg62: f32, %arg63: f32):
        %97 = "arith.addf"(%arg61, %arg62) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%97) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%96) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %15 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %16 = "bufferization.alloc_tensor"() <{operandSegmentSizes = array<i32: 0, 0, 0>}> : () -> tensor<f32>
    %17 = "tensor.insert"(%15, %16) : (f32, tensor<f32>) -> tensor<f32>
    %18 = "linalg.reduce"(%14, %17) <{dimensions = array<i64: 0>}> ({
    ^bb0(%arg57: f32, %arg58: f32):
      %87 = "arith.addf"(%arg57, %arg58) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%87) : (f32) -> ()
    }) : (tensor<256xf32>, tensor<f32>) -> tensor<f32>
    %19 = "tensor.extract"(%18) : (tensor<f32>) -> f32
    %20 = "arith.sitofp"(%arg7) : (i32) -> f32
    %21 = "arith.divf"(%19, %20) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %22 = "tensor.empty"() : () -> tensor<256xf32>
    %23 = "linalg.fill"(%21, %22) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg55: f32, %arg56: f32):
      "linalg.yield"(%arg55) : (f32) -> ()
    }) : (f32, tensor<256xf32>) -> tensor<256xf32>
    %24 = "scf.for"(%0, %arg7, %6, %5) ({
    ^bb0(%arg32: i32, %arg33: tensor<256xf32>):
      %64 = "tensor.empty"() : () -> tensor<256xi32>
      %65 = "linalg.fill"(%arg32, %64) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg53: i32, %arg54: i32):
        "linalg.yield"(%arg53) : (i32) -> ()
      }) : (i32, tensor<256xi32>) -> tensor<256xi32>
      %66 = "linalg.generic"(%65, %11, %65) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg50: i32, %arg51: i32, %arg52: i32):
        %86 = "arith.addi"(%arg50, %arg51) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
        "linalg.yield"(%86) : (i32) -> ()
      }) : (tensor<256xi32>, tensor<256xi32>, tensor<256xi32>) -> tensor<256xi32>
      %67 = "tensor.empty"() : () -> tensor<256xi1>
      %68 = "linalg.generic"(%66, %13, %67) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg47: i32, %arg48: i32, %arg49: i1):
        %85 = "arith.cmpi"(%arg47, %arg48) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "linalg.yield"(%85) : (i1) -> ()
      }) : (tensor<256xi32>, tensor<256xi32>, tensor<256xi1>) -> tensor<256xi1>
      %69 = "arith.index_cast"(%arg32) : (i32) -> index
      %70 = "arith.addi"(%9, %69) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %71 = "tts.make_tptr"(%arg0, %70) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<256x!tt.ptr<f32>>
      %72 = "arith.addi"(%69, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %73 = "arith.index_cast"(%arg7) : (i32) -> index
      %74 = "arith.minsi"(%72, %73) : (index, index) -> index
      %75 = "arith.subi"(%74, %69) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %76 = "tts.load"(%71, %75, %1) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %77 = "linalg.generic"(%76, %23, %76) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg44: f32, %arg45: f32, %arg46: f32):
        %84 = "arith.subf"(%arg44, %arg45) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%84) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %78 = "linalg.generic"(%68, %77, %5, %77) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 3, 1>}> ({
      ^bb0(%arg40: i1, %arg41: f32, %arg42: f32, %arg43: f32):
        %83 = "arith.select"(%arg40, %arg41, %arg42) : (i1, f32, f32) -> f32
        "linalg.yield"(%83) : (f32) -> ()
      }) : (tensor<256xi1>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %79 = "linalg.generic"(%78, %78, %78) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg37: f32, %arg38: f32, %arg39: f32):
        %82 = "arith.mulf"(%arg37, %arg38) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%82) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %80 = "linalg.generic"(%arg33, %79, %arg33) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg34: f32, %arg35: f32, %arg36: f32):
        %81 = "arith.addf"(%arg34, %arg35) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%81) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      "scf.yield"(%80) : (tensor<256xf32>) -> ()
    }) : (i32, i32, i32, tensor<256xf32>) -> tensor<256xf32>
    %25 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %26 = "bufferization.alloc_tensor"() <{operandSegmentSizes = array<i32: 0, 0, 0>}> : () -> tensor<f32>
    %27 = "tensor.insert"(%25, %26) : (f32, tensor<f32>) -> tensor<f32>
    %28 = "linalg.reduce"(%24, %27) <{dimensions = array<i64: 0>}> ({
    ^bb0(%arg30: f32, %arg31: f32):
      %63 = "arith.addf"(%arg30, %arg31) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%63) : (f32) -> ()
    }) : (tensor<256xf32>, tensor<f32>) -> tensor<f32>
    %29 = "tensor.extract"(%28) : (tensor<f32>) -> f32
    %30 = "arith.divf"(%29, %20) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %31 = "arith.addf"(%30, %arg8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %32 = "math.sqrt"(%31) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %33 = "arith.divf"(%7, %32) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %34 = "tts.create_ptr"(%arg4, %arg12) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%34, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %35 = "tts.create_ptr"(%arg5, %arg12) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%35, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
    %36 = "tensor.empty"() : () -> tensor<256xf32>
    %37 = "linalg.fill"(%33, %36) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg28: f32, %arg29: f32):
      "linalg.yield"(%arg28) : (f32) -> ()
    }) : (f32, tensor<256xf32>) -> tensor<256xf32>
    "scf.for"(%0, %arg7, %6) ({
    ^bb0(%arg15: i32):
      %38 = "arith.index_cast"(%arg15) : (i32) -> index
      %39 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<f32>) -> memref<*xf32>
      %40 = "tts.make_tptr"(%39, %38) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<256x!tt.ptr<f32>>
      %41 = "arith.addi"(%38, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %42 = "arith.index_cast"(%arg7) : (i32) -> index
      %43 = "arith.minsi"(%41, %42) : (index, index) -> index
      %44 = "arith.subi"(%43, %38) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %45 = "tts.load"(%40, %44) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %46 = "builtin.unrealized_conversion_cast"(%arg3) : (!tt.ptr<f32>) -> memref<*xf32>
      %47 = "tts.make_tptr"(%46, %38) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<256x!tt.ptr<f32>>
      %48 = "tts.load"(%47, %44) <{operandSegmentSizes = array<i32: 1, 1, 0>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index) -> tensor<256xf32>
      %49 = "arith.addi"(%9, %38) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %50 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
      %51 = "tts.make_tptr"(%50, %49) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<256x!tt.ptr<f32>>
      %52 = "tts.load"(%51, %44, %1) <{operandSegmentSizes = array<i32: 1, 1, 1>, static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, index, f32) -> tensor<256xf32>
      %53 = "linalg.generic"(%52, %23, %52) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg25: f32, %arg26: f32, %arg27: f32):
        %62 = "arith.subf"(%arg25, %arg26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%62) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %54 = "linalg.generic"(%53, %37, %53) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg22: f32, %arg23: f32, %arg24: f32):
        %61 = "arith.mulf"(%arg22, %arg23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%61) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %55 = "linalg.generic"(%54, %45, %54) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg19: f32, %arg20: f32, %arg21: f32):
        %60 = "arith.mulf"(%arg19, %arg20) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%60) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %56 = "linalg.generic"(%55, %48, %55) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg16: f32, %arg17: f32, %arg18: f32):
        %59 = "arith.addf"(%arg16, %arg17) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%59) : (f32) -> ()
      }) : (tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<256xf32>
      %57 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
      %58 = "tts.make_tptr"(%57, %49) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 256>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<256x!tt.ptr<f32>>
      "tts.store"(%58, %56, %44) <{static_mask_dims = array<i64: -9223372036854775808>}> : (tensor<256x!tt.ptr<f32>>, tensor<256xf32>, index) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
%90 = "memref.reinterpret_cast"(%arg0, %89) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<256xf32, strided<[1], offset: ?>>
%96 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<256xf32>
%71 = "memref.reinterpret_cast"(%arg0, %70) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<256xf32, strided<[1], offset: ?>>
%77 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<256xf32>
%40 = "memref.reinterpret_cast"(%39, %38) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> memref<256xf32, strided<[1], offset: ?>>
%46 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<256xf32>
%52 = "memref.reinterpret_cast"(%51, %38) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> memref<256xf32, strided<[1], offset: ?>>
%54 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<256xf32>
%61 = "memref.reinterpret_cast"(%60, %59) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> memref<256xf32, strided<[1], offset: ?>>
%63 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<256xf32>
%77 = "memref.reinterpret_cast"(%76, %59) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> memref<256xf32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :21:13: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
      %35 = tt.addptr %7, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>
            ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir:1 offset :21:13: note: see current operation: %112 = "memref.reinterpret_cast"(%arg0, %111) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 256>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<256xf32, strided<[1], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/kernel-05-layer-norm-dwdb.mlir (14 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/kernel-03-matrix-multiplication.mlir (15 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax.mlir (16 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_accumulation.mlir (17 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/use_end_chain.mlir (18 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_stacked.mlir (19 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir (20 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 128 : index}> : () -> index
    %3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
    %4 = "arith.constant"() <{value = 3 : index}> : () -> index
    %5 = "arith.constant"() <{value = 12 : index}> : () -> index
    %6 = "arith.constant"() <{value = 0 : index}> : () -> index
    %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %8 = "arith.muli"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "arith.index_cast"(%8) : (i32) -> index
    %10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
    %11:3 = "scf.for"(%6, %5, %4, %3, %10, %9) ({
    ^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
      %16 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %17 = "tts.make_tptr"(%arg1, %16) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
      %18 = "tts.load"(%17) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
      %19 = "math.exp"(%18) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
      %20 = "arith.addf"(%arg6, %19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %21 = "arith.index_cast"(%arg5) : (index) -> i32
      %22 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %23 = "tt.addptr"(%arg7, %21) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "scf.yield"(%20, %23, %22) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
    }) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
    %12 = "arith.muli"(%7, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %13 = "arith.index_cast"(%12) : (i32) -> index
    %14 = "arith.addi"(%13, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %15 = "tts.make_tptr"(%arg0, %14) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
    "tts.store"(%15, %11#0) <{static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "tt.addptr"(%arg7, %22) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %24, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
actual processing
processing user
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "tt.addptr"(%arg7, %22) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %24, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
arg number: 4
init arg size
3
num region iter-args
3
dump from that index
iter arg
init arg
%3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
%9 = "arith.index_cast"(%8) : (i32) -> index
~~~~
processing val
<block argument> of type 'i32' at index: 2
these are the uses:
%24 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%24 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: i32, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "arith.addi"(%arg7, %22) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, i32, index)
these are the uses:
actual processing
~~~~
processing val
%25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
actual processing
processing user
"scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
~~~~
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 128 : index}> : () -> index
    %3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
    %4 = "arith.constant"() <{value = 3 : index}> : () -> index
    %5 = "arith.constant"() <{value = 12 : index}> : () -> index
    %6 = "arith.constant"() <{value = 0 : index}> : () -> index
    %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %8 = "arith.muli"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "arith.index_cast"(%8) : (i32) -> index
    %10 = "arith.addi"(%0, %8) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
    %12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
    ^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: i32, %arg8: index):
      %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
      %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
      %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
      %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %22 = "arith.index_cast"(%arg5) : (index) -> i32
      %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %24 = "arith.addi"(%arg7, %22) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
      "scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
    }) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, i32, index)
    %13 = "arith.muli"(%7, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %14 = "arith.index_cast"(%13) : (i32) -> index
    %15 = "arith.addi"(%14, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %16 = "tts.make_tptr"(%arg0, %15) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
    "tts.store"(%16, %12#0) <{static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
deleting
%25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
"builtin.module"() ({
  "func.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}, {}, {}, {}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32):
    %0 = "arith.constant"() <{value = 128 : index}> : () -> index
    %1 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %2 = "tensor.empty"() : () -> tensor<128x128xf32>
    %3 = "linalg.fill"(%1, %2) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg20: f32, %arg21: f32):
      "linalg.yield"(%arg20) : (f32) -> ()
    }) : (f32, tensor<128x128xf32>) -> tensor<128x128xf32>
    %4 = "arith.constant"() <{value = 3 : index}> : () -> index
    %5 = "arith.constant"() <{value = 12 : index}> : () -> index
    %6 = "arith.constant"() <{value = 0 : index}> : () -> index
    %7 = "arith.muli"(%arg8, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9:3 = "scf.for"(%6, %5, %4, %3, %7, %8) ({
    ^bb0(%arg11: index, %arg12: tensor<128x128xf32>, %arg13: i32, %arg14: index):
      %15 = "arith.addi"(%arg14, %0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %16 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
      %17 = "tts.make_tptr"(%16, %15) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>, index) -> tensor<128x128x!tt.ptr<f32>>
      %18 = "tts.load"(%17) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
      %19 = "linalg.generic"(%18, %18) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg18: f32, %arg19: f32):
        %25 = "math.exp"(%arg18) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        "linalg.yield"(%25) : (f32) -> ()
      }) : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %20 = "linalg.generic"(%arg12, %19, %arg12) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg15: f32, %arg16: f32, %arg17: f32):
        %24 = "arith.addf"(%arg15, %arg16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%24) : (f32) -> ()
      }) : (tensor<128x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %21 = "arith.index_cast"(%arg11) : (index) -> i32
      %22 = "arith.addi"(%arg14, %arg11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %23 = "arith.addi"(%arg13, %21) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      "scf.yield"(%20, %23, %22) : (tensor<128x128xf32>, i32, index) -> ()
    }) : (index, index, index, tensor<128x128xf32>, i32, index) -> (tensor<128x128xf32>, i32, index)
    %10 = "arith.muli"(%arg8, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %11 = "arith.index_cast"(%10) : (i32) -> index
    %12 = "arith.addi"(%11, %0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %13 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %14 = "tts.make_tptr"(%13, %12) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (memref<*xf32>, index) -> tensor<128x128x!tt.ptr<f32>>
    "tts.store"(%14, %9#0) <{static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir:70:11: error: CHECK: expected string not found in input
// CHECK: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_3_]]{{.}}, sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
          ^
<stdin>:12:41: note: scanning from here
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:12:41: note: with "PARAM_1_" equal to "%arg1"
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:12:41: note: with "VAR_3_" equal to "%3"
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:15:18: note: possible intended match here
 %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%8], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1], offset: ?>>
                 ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
            .
            .
            .
            7:  %c128 = arith.constant 128 : index 
            8:  %cst = arith.constant 0.000000e+00 : f32 
            9:  %0 = tensor.empty() : tensor<128x128xf32> 
           10:  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<128x128xf32>) -> tensor<128x128xf32> 
           11:  %2 = arith.muli %arg8, %arg2 : i32 
           12:  %3 = arith.index_cast %2 : i32 to index 
check:70'0                                             X error: no match found
check:70'1                                               with "PARAM_1_" equal to "%arg1"
check:70'2                                               with "VAR_3_" equal to "%3"
           13:  %4:3 = scf.for %arg11 = %c0 to %c12 step %c3 iter_args(%arg12 = %1, %arg13 = %2, %arg14 = %3) -> (tensor<128x128xf32>, i32, index) { 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           14:  %8 = arith.addi %arg14, %c128 : index 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           15:  %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%8], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1], offset: ?>> 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
check:70'3                      ?                                                                                                                                                           possible intended match
           16:  %alloc = memref.alloc() : memref<128x128xf32> 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           17:  memref.copy %reinterpret_cast_0, %alloc : memref<128x128xf32, strided<[1, 1], offset: ?>> to memref<128x128xf32> 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           18:  %9 = bufferization.to_tensor %alloc restrict writable : memref<128x128xf32> 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           19:  %10 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%9 : tensor<128x128xf32>) outs(%9 : tensor<128x128xf32>) { 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           20:  ^bb0(%in: f32, %out: f32): 
check:70'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            .
            .
            .
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir (21 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
module+ FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
 {
  tt.func public @tensor_indices_nested(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c3_i32 = arith.constant 3 : i32
    %cst = arith.constant dense<4> : tensor<4xi32>
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1:4 = scf.for %arg2 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg3 = %0, %arg4 = %c0, %arg5 = %0, %arg6 = %c0) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
      %2 = arith.muli %arg2, %c2_i32 : i32
      %3 = arith.index_cast %2 : i32 to index
      %4 = tt.splat %2 : i32 -> tensor<4xi32>
      %5 = arith.addi %arg3, %4 : tensor<4xi32>
      %6 = arith.addi %arg4, %3 : index
      %7 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      %8 = "tts.load"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
      %9 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      "tts.store"(%9, %8) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
      %10 = arith.addi %5, %cst : tensor<4xi32>
      %11 = arith.addi %arg5, %cst : tensor<4xi32>
      %12 = arith.addi %6, %c4 : index
      %13 = arith.addi %arg6, %c4 : index
      %14:4 = scf.for %arg7 = %c0_i32_1 to %c3_i32 step %c1_i32 iter_args(%arg8 = %10, %arg9 = %12, %arg10 = %11, %arg11 = %13) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
        %15 = arith.muli %arg7, %c3_i32 : i32
        %16 = arith.index_cast %15 : i32 to index
        %17 = tt.splat %15 : i32 -> tensor<4xi32>
        %18 = arith.addi %arg8, %17 : tensor<4xi32>
        %19 = arith.addi %arg9, %16 : index
        %20 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%19], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        %21 = "tts.load"(%20) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
        %22 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg11], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        "tts.store"(%22, %21) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
        %23 = arith.addi %18, %cst : tensor<4xi32>
        %24 = arith.addi %arg10, %cst : tensor<4xi32>
        %25 = arith.addi %19, %c4 : index
        %26 = arith.addi %arg11, %c4 : index
        scf.yield %23, %25, %24, %26 : tensor<4xi32>, index, tensor<4xi32>, index
      }
      scf.yield %14#0, %14#1, %14#2, %14#3 : tensor<4xi32>, index, tensor<4xi32>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @tensor_indices_nested(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c3_i32 = arith.constant 3 : i32
    %cst = arith.constant dense<4> : tensor<4xi32>
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1:4 = scf.for %arg2 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg3 = %0, %arg4 = %c0, %arg5 = %0, %arg6 = %c0) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
      %2 = arith.muli %arg2, %c2_i32 : i32
      %3 = arith.index_cast %2 : i32 to index
      %4 = tt.splat %2 : i32 -> tensor<4xi32>
      %5 = arith.addi %arg3, %4 : tensor<4xi32>
      %6 = arith.addi %arg4, %3 : index
      %7 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      %8 = "tts.load"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
      %9 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      "tts.store"(%9, %8) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
      %10 = arith.addi %5, %cst : tensor<4xi32>
      %11 = arith.addi %arg5, %cst : tensor<4xi32>
      %12 = arith.addi %6, %c4 : index
      %13 = arith.addi %arg6, %c4 : index
      %14:4 = scf.for %arg7 = %c0_i32_1 to %c3_i32 step %c1_i32 iter_args(%arg8 = %10, %arg9 = %12, %arg10 = %11, %arg11 = %13) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
        %15 = arith.muli %arg7, %c3_i32 : i32
        %16 = arith.index_cast %15 : i32 to index
        %17 = tt.splat %15 : i32 -> tensor<4xi32>
        %18 = arith.addi %arg8, %17 : tensor<4xi32>
        %19 = arith.addi %arg9, %16 : index
        %20 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%19], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        %21 = "tts.load"(%20) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
        %22 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg11], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        "tts.store"(%22, %21) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
        %23 = arith.addi %18, %cst : tensor<4xi32>
        %24 = arith.addi %arg10, %cst : tensor<4xi32>
        %25 = arith.addi %19, %c4 : index
        %26 = arith.addi %arg11, %c4 : index
        scf.yield %23, %25, %24, %26 : tensor<4xi32>, index, tensor<4xi32>, index
      }
      scf.yield %14#0, %14#1, %14#2, %14#3 : tensor<4xi32>, index, tensor<4xi32>, index
    }
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "tensor_indices_nested"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 4 : index}> : () -> index
    %2 = "arith.constant"() <{value = 1 : index}> : () -> index
    %3 = "arith.constant"() <{value = 0 : index}> : () -> index
    %4 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 3 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 4 : i32}> : () -> i32
    %7 = "tensor.empty"() : () -> tensor<4xi32>
    %8 = "linalg.fill"(%6, %7) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg41: i32, %arg42: i32):
      "linalg.yield"(%arg41) : (i32) -> ()
    }) : (i32, tensor<4xi32>) -> tensor<4xi32>
    %9 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %10 = "tensor.empty"() : () -> tensor<4xi32>
    %11 = "linalg.generic"(%10) <{indexing_maps = [affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 0, 1>}> ({
    ^bb0(%arg40: i32):
      %48 = "linalg.index"() <{dim = 0 : i64}> : () -> index
      %49 = "arith.index_cast"(%48) : (index) -> i32
      "linalg.yield"(%49) : (i32) -> ()
    }) : (tensor<4xi32>) -> tensor<4xi32>
    %12:4 = "scf.for"(%0, %9, %4, %11, %3, %11, %3) ({
    ^bb0(%arg8: i32, %arg9: tensor<4xi32>, %arg10: index, %arg11: tensor<4xi32>, %arg12: index):
      %13 = "arith.muli"(%arg8, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %14 = "arith.index_cast"(%13) : (i32) -> index
      %15 = "tensor.empty"() : () -> tensor<4xi32>
      %16 = "linalg.fill"(%13, %15) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg38: i32, %arg39: i32):
        "linalg.yield"(%arg38) : (i32) -> ()
      }) : (i32, tensor<4xi32>) -> tensor<4xi32>
      %17 = "linalg.generic"(%arg9, %16, %arg9) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg35: i32, %arg36: i32, %arg37: i32):
        %47 = "arith.addi"(%arg35, %arg36) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
        "linalg.yield"(%47) : (i32) -> ()
      }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
      %18 = "arith.addi"(%arg10, %14) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %19 = "tts.make_tptr"(%arg0, %2, %18) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> tensor<4x!tt.ptr<f32>>
      %20 = "tts.load"(%19) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
      %21 = "tts.make_tptr"(%arg1, %2, %arg12) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> tensor<4x!tt.ptr<f32>>
      "tts.store"(%21, %20) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
      %22 = "linalg.generic"(%17, %8, %17) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg32: i32, %arg33: i32, %arg34: i32):
        %46 = "arith.addi"(%arg32, %arg33) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
        "linalg.yield"(%46) : (i32) -> ()
      }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
      %23 = "linalg.generic"(%arg11, %8, %arg11) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg29: i32, %arg30: i32, %arg31: i32):
        %45 = "arith.addi"(%arg29, %arg30) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
        "linalg.yield"(%45) : (i32) -> ()
      }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
      %24 = "arith.addi"(%18, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %25 = "arith.addi"(%arg12, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %26:4 = "scf.for"(%0, %5, %4, %22, %24, %23, %25) ({
      ^bb0(%arg13: i32, %arg14: tensor<4xi32>, %arg15: index, %arg16: tensor<4xi32>, %arg17: index):
        %27 = "arith.muli"(%arg13, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
        %28 = "arith.index_cast"(%27) : (i32) -> index
        %29 = "tensor.empty"() : () -> tensor<4xi32>
        %30 = "linalg.fill"(%27, %29) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg27: i32, %arg28: i32):
          "linalg.yield"(%arg27) : (i32) -> ()
        }) : (i32, tensor<4xi32>) -> tensor<4xi32>
        %31 = "linalg.generic"(%arg14, %30, %arg14) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg24: i32, %arg25: i32, %arg26: i32):
          %44 = "arith.addi"(%arg24, %arg25) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
          "linalg.yield"(%44) : (i32) -> ()
        }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
        %32 = "arith.addi"(%arg15, %28) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %33 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
        %34 = "tts.make_tptr"(%33, %2, %32) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: -9223372036854775808>}> : (memref<*xf32>, index, index) -> tensor<4x!tt.ptr<f32>>
        %35 = "tts.load"(%34) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
        %36 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
        %37 = "tts.make_tptr"(%36, %2, %arg17) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: -9223372036854775808>}> : (memref<*xf32>, index, index) -> tensor<4x!tt.ptr<f32>>
        "tts.store"(%37, %35) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
        %38 = "linalg.generic"(%31, %8, %31) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg21: i32, %arg22: i32, %arg23: i32):
          %43 = "arith.addi"(%arg21, %arg22) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
          "linalg.yield"(%43) : (i32) -> ()
        }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
        %39 = "linalg.generic"(%arg16, %8, %arg16) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg18: i32, %arg19: i32, %arg20: i32):
          %42 = "arith.addi"(%arg18, %arg19) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
          "linalg.yield"(%42) : (i32) -> ()
        }) : (tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
        %40 = "arith.addi"(%32, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %41 = "arith.addi"(%arg17, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%38, %40, %39, %41) : (tensor<4xi32>, index, tensor<4xi32>, index) -> ()
      }) : (i32, i32, i32, tensor<4xi32>, index, tensor<4xi32>, index) -> (tensor<4xi32>, index, tensor<4xi32>, index)
      "scf.yield"(%26#0, %26#1, %26#2, %26#3) : (tensor<4xi32>, index, tensor<4xi32>, index) -> ()
    }) : (i32, i32, i32, tensor<4xi32>, index, tensor<4xi32>, index) -> (tensor<4xi32>, index, tensor<4xi32>, index)
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
/home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir:18:12: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
      %7 = tt.addptr %1, %6 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
           ^
/home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir:18:12: note: see current operation: %19 = "memref.reinterpret_cast"(%arg0, %18, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 1>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 4>, static_strides = array<i64: -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> memref<4xf32, strided<[?], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/use_mid_chain.mlir (22 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterarg_with_masks.mlir (23 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/ridiculously_nested_loops.mlir (24 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/ridiculously_nested_loops.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
module {
  tt.func public @nested_who_knows_how_many_levels(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:4 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7, %arg12 = %7) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
        %11 = arith.addi %arg9, %4 : index
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %14:5 = scf.for %arg13 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg14 = %11, %arg15 = %arg10, %arg16 = %arg11, %arg17 = %arg12, %arg18 = %13) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
          %16 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = arith.addi %arg14, %4 : index
          %18 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%17, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%16, %arg17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %arg16, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %arg18) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%23, %19) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %24 = arith.addi %22, %4 : index
          %25 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%24, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %26:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %arg17, %arg21 = %18, %arg22 = %17, %arg23 = %arg18, %arg24 = %19, %arg25 = %25, %arg26 = %24) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:7 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26, %arg34 = %29) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:7 = scf.for %arg35 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg36 = %32, %arg37 = %31, %arg38 = %arg31, %arg39 = %arg32, %arg40 = %arg33, %arg41 = %arg34, %arg42 = %33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg40, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg37, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %arg41) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg40, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %arg42) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %45:7 = scf.for %arg43 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg44 = %arg41, %arg45 = %37, %arg46 = %36, %arg47 = %arg42, %arg48 = %38, %arg49 = %44, %arg50 = %43) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                  %46 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg46, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                  %47 = "tts.load"(%46) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                  %48:7 = scf.for %arg51 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg52 = %arg45, %arg53 = %arg46, %arg54 = %arg47, %arg55 = %arg48, %arg56 = %arg49, %arg57 = %arg50, %arg58 = %47) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
                    %49 = arith.addi %arg53, %4 : index
                    %50 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%49, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                    %51 = "tts.load"(%50) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                    %52:7 = scf.for %arg59 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg60 = %50, %arg61 = %49, %arg62 = %arg55, %arg63 = %arg56, %arg64 = %arg57, %arg65 = %arg58, %arg66 = %51) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                      %53 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg64, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %54 = arith.addi %arg61, %4 : index
                      %55 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%54, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %56 = "tts.load"(%55) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                      "tts.store"(%53, %arg65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %57 = arith.addi %arg64, %4 : index
                      %58 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%57, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%58, %arg66) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %59 = arith.addi %57, %4 : index
                      %60 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%59, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%60, %56) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %61 = arith.addi %59, %4 : index
                      %62 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%61, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %63:7 = scf.for %arg67 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg68 = %arg65, %arg69 = %55, %arg70 = %54, %arg71 = %arg66, %arg72 = %56, %arg73 = %62, %arg74 = %61) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                        %64 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg70, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                        %65 = "tts.load"(%64) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                        %66:6 = scf.for %arg75 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg76 = %arg69, %arg77 = %arg70, %arg78 = %arg71, %arg79 = %arg72, %arg80 = %arg73, %arg81 = %arg74) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                          %67 = arith.addi %arg77, %4 : index
                          %68 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%67, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                          %69 = "tts.load"(%68) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                          %70:5 = scf.for %arg82 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg83 = %68, %arg84 = %67, %arg85 = %arg79, %arg86 = %arg80, %arg87 = %arg81) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                            %71 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg87, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %72 = arith.addi %arg84, %4 : index
                            %73 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%72, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %74 = "tts.load"(%73) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                            "tts.store"(%71, %65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %75 = arith.addi %arg87, %4 : index
                            %76 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%75, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%76, %69) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %77 = arith.addi %75, %4 : index
                            %78 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%77, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%78, %74) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %79 = arith.addi %77, %4 : index
                            %80 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%79, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            scf.yield %73, %72, %74, %80, %79 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                          }
                          scf.yield %70#0, %70#1, %69, %70#2, %70#3, %70#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                        }
                        scf.yield %65, %66#0, %66#1, %66#2, %66#3, %66#4, %66#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                      }
                      scf.yield %63#1, %63#2, %63#4, %63#5, %63#6, %63#0, %63#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
                    }
                    scf.yield %52#0, %52#1, %52#6, %52#2, %52#3, %52#4, %52#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
                  }
                  scf.yield %48#6, %48#0, %48#1, %48#2, %48#3, %48#4, %48#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                }
                scf.yield %45#1, %45#2, %45#4, %45#5, %45#6, %45#0, %45#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
              }
              scf.yield %34#0, %34#1, %34#6, %34#2, %34#3, %34#4, %34#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
            }
            scf.yield %30#6, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          %27:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %26#0, %arg21 = %26#1, %arg22 = %26#2, %arg23 = %26#3, %arg24 = %26#4, %arg25 = %26#5, %arg26 = %26#6) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:6 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:5 = scf.for %arg34 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg35 = %32, %arg36 = %31, %arg37 = %arg31, %arg38 = %arg32, %arg39 = %arg33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg36, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %29) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg39, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %33) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                scf.yield %37, %36, %38, %44, %43 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
              }
              scf.yield %34#0, %34#1, %33, %34#2, %34#3, %34#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
            }
            scf.yield %29, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %27#2, %27#5, %27#6, %27#0, %27#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
        }
        %15 = arith.addi %14#0, %4 : index
        scf.yield %15, %14#1, %14#2, %14#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
      }
      %9:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %8#0, %arg10 = %8#1, %arg11 = %8#2) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %arg9, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          %18:3 = scf.for %arg16 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg17 = %15, %arg18 = %arg14, %arg19 = %arg15) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg19, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %20 = arith.addi %arg17, %4 : index
            %21 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %22 = "tts.load"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %23 = arith.addi %arg19, %4 : index
            %24 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%23, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%24, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %25 = arith.addi %23, %4 : index
            %26 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%25, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%26, %22) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %27 = arith.addi %25, %4 : index
            %28 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%27, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            scf.yield %20, %28, %27 : index, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %18#0, %18#1, %18#2 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        %14 = arith.addi %13#0, %4 : index
        scf.yield %14, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %10 = arith.addi %9#0, %4 : index
      scf.yield %10, %9#1, %9#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @nested_who_knows_how_many_levels(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:4 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7, %arg12 = %7) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
        %11 = arith.addi %arg9, %4 : index
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %14:5 = scf.for %arg13 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg14 = %11, %arg15 = %arg10, %arg16 = %arg11, %arg17 = %arg12, %arg18 = %13) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
          %16 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = arith.addi %arg14, %4 : index
          %18 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%17, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%16, %arg17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %arg16, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %arg18) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%23, %19) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %24 = arith.addi %22, %4 : index
          %25 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%24, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %26:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %arg17, %arg21 = %18, %arg22 = %17, %arg23 = %arg18, %arg24 = %19, %arg25 = %25, %arg26 = %24) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:7 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26, %arg34 = %29) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:7 = scf.for %arg35 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg36 = %32, %arg37 = %31, %arg38 = %arg31, %arg39 = %arg32, %arg40 = %arg33, %arg41 = %arg34, %arg42 = %33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg40, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg37, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %arg41) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg40, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %arg42) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %45:7 = scf.for %arg43 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg44 = %arg41, %arg45 = %37, %arg46 = %36, %arg47 = %arg42, %arg48 = %38, %arg49 = %44, %arg50 = %43) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                  %46 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg46, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                  %47 = "tts.load"(%46) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                  %48:7 = scf.for %arg51 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg52 = %arg45, %arg53 = %arg46, %arg54 = %arg47, %arg55 = %arg48, %arg56 = %arg49, %arg57 = %arg50, %arg58 = %47) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
                    %49 = arith.addi %arg53, %4 : index
                    %50 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%49, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                    %51 = "tts.load"(%50) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                    %52:7 = scf.for %arg59 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg60 = %50, %arg61 = %49, %arg62 = %arg55, %arg63 = %arg56, %arg64 = %arg57, %arg65 = %arg58, %arg66 = %51) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                      %53 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg64, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %54 = arith.addi %arg61, %4 : index
                      %55 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%54, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %56 = "tts.load"(%55) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                      "tts.store"(%53, %arg65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %57 = arith.addi %arg64, %4 : index
                      %58 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%57, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%58, %arg66) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %59 = arith.addi %57, %4 : index
                      %60 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%59, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%60, %56) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %61 = arith.addi %59, %4 : index
                      %62 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%61, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %63:7 = scf.for %arg67 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg68 = %arg65, %arg69 = %55, %arg70 = %54, %arg71 = %arg66, %arg72 = %56, %arg73 = %62, %arg74 = %61) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                        %64 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg70, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                        %65 = "tts.load"(%64) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                        %66:6 = scf.for %arg75 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg76 = %arg69, %arg77 = %arg70, %arg78 = %arg71, %arg79 = %arg72, %arg80 = %arg73, %arg81 = %arg74) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                          %67 = arith.addi %arg77, %4 : index
                          %68 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%67, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                          %69 = "tts.load"(%68) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                          %70:5 = scf.for %arg82 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg83 = %68, %arg84 = %67, %arg85 = %arg79, %arg86 = %arg80, %arg87 = %arg81) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                            %71 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg87, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %72 = arith.addi %arg84, %4 : index
                            %73 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%72, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %74 = "tts.load"(%73) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                            "tts.store"(%71, %65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %75 = arith.addi %arg87, %4 : index
                            %76 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%75, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%76, %69) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %77 = arith.addi %75, %4 : index
                            %78 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%77, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%78, %74) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %79 = arith.addi %77, %4 : index
                            %80 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%79, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            scf.yield %73, %72, %74, %80, %79 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                          }
                          scf.yield %70#0, %70#1, %69, %70#2, %70#3, %70#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                        }
                        scf.yield %65, %66#0, %66#1, %66#2, %66#3, %66#4, %66#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                      }
                      scf.yield %63#1, %63#2, %63#4, %63#5, %63#6, %63#0, %63#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
                    }
                    scf.yield %52#0, %52#1, %52#6, %52#2, %52#3, %52#4, %52#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
                  }
                  scf.yield %48#6, %48#0, %48#1, %48#2, %48#3, %48#4, %48#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                }
                scf.yield %45#1, %45#2, %45#4, %45#5, %45#6, %45#0, %45#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
              }
              scf.yield %34#0, %34#1, %34#6, %34#2, %34#3, %34#4, %34#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
            }
            scf.yield %30#6, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          %27:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %26#0, %arg21 = %26#1, %arg22 = %26#2, %arg23 = %26#3, %arg24 = %26#4, %arg25 = %26#5, %arg26 = %26#6) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:6 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:5 = scf.for %arg34 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg35 = %32, %arg36 = %31, %arg37 = %arg31, %arg38 = %arg32, %arg39 = %arg33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg36, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %29) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg39, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %33) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                scf.yield %37, %36, %38, %44, %43 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
              }
              scf.yield %34#0, %34#1, %33, %34#2, %34#3, %34#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
            }
            scf.yield %29, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %27#2, %27#5, %27#6, %27#0, %27#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
        }
        %15 = arith.addi %14#0, %4 : index
        scf.yield %15, %14#1, %14#2, %14#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
      }
      %9:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %8#0, %arg10 = %8#1, %arg11 = %8#2) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %arg9, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          %18:3 = scf.for %arg16 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg17 = %15, %arg18 = %arg14, %arg19 = %arg15) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg19, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %20 = arith.addi %arg17, %4 : index
            %21 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %22 = "tts.load"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %23 = arith.addi %arg19, %4 : index
            %24 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%23, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%24, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %25 = arith.addi %23, %4 : index
            %26 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%25, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%26, %22) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %27 = arith.addi %25, %4 : index
            %28 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%27, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            scf.yield %20, %28, %27 : index, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %18#0, %18#1, %18#2 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        %14 = arith.addi %13#0, %4 : index
        scf.yield %14, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %10 = arith.addi %9#0, %4 : index
      scf.yield %10, %9#1, %9#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "nested_who_knows_how_many_levels"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : index}> : () -> index
    %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %4 = "arith.index_cast"(%arg2) : (i32) -> index
    %5 = "arith.index_cast"(%arg3) : (i32) -> index
    %6 = "tts.make_tptr"(%arg1, %4, %5) <{operandSegmentSizes = array<i32: 1, 2, 0, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index) -> tensor<2x2x!tt.ptr<f32>>
    %7 = "arith.muli"(%arg3, %3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9:3 = "scf.for"(%0, %3, %2, %1, %6, %1) ({
    ^bb0(%arg10: i32, %arg11: index, %arg12: tensor<2x2x!tt.ptr<f32>>, %arg13: index):
      %10 = "tts.make_tptr"(%arg0, %4, %5, %arg11, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
      %11 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %12:4 = "scf.for"(%0, %3, %2, %arg11, %arg12, %arg13, %11) ({
      ^bb0(%arg26: i32, %arg27: index, %arg28: tensor<2x2x!tt.ptr<f32>>, %arg29: index, %arg30: tensor<2x2xf32>):
        %35 = "arith.addi"(%arg27, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %36 = "tts.make_tptr"(%arg0, %4, %5, %35, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %37 = "tts.load"(%36) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %38:5 = "scf.for"(%0, %3, %2, %35, %arg28, %arg29, %arg30, %37) ({
        ^bb0(%arg31: i32, %arg32: index, %arg33: tensor<2x2x!tt.ptr<f32>>, %arg34: index, %arg35: tensor<2x2xf32>, %arg36: tensor<2x2xf32>):
          %40 = "tts.make_tptr"(%arg1, %4, %5, %arg34, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %41 = "arith.addi"(%arg32, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %42 = "tts.make_tptr"(%arg0, %4, %5, %41, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %43 = "tts.load"(%42) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%40, %arg35) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %44 = "arith.addi"(%arg34, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %45 = "tts.make_tptr"(%arg1, %4, %5, %44, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%45, %arg36) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %46 = "arith.addi"(%44, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %47 = "tts.make_tptr"(%arg1, %4, %5, %46, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%47, %43) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %48 = "arith.addi"(%46, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %49 = "tts.make_tptr"(%arg1, %4, %5, %48, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %50:7 = "scf.for"(%0, %3, %2, %arg35, %42, %41, %arg36, %43, %49, %48) ({
          ^bb0(%arg58: i32, %arg59: tensor<2x2xf32>, %arg60: tensor<2x2x!tt.ptr<f32>>, %arg61: index, %arg62: tensor<2x2xf32>, %arg63: tensor<2x2xf32>, %arg64: tensor<2x2x!tt.ptr<f32>>, %arg65: index):
            %69 = "tts.make_tptr"(%arg0, %4, %5, %arg61, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            %70 = "tts.load"(%69) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %71:7 = "scf.for"(%0, %3, %2, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %70) ({
            ^bb0(%arg66: i32, %arg67: tensor<2x2x!tt.ptr<f32>>, %arg68: index, %arg69: tensor<2x2xf32>, %arg70: tensor<2x2xf32>, %arg71: tensor<2x2x!tt.ptr<f32>>, %arg72: index, %arg73: tensor<2x2xf32>):
              %72 = "arith.addi"(%arg68, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
              %73 = "tts.make_tptr"(%arg0, %4, %5, %72, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
              %74 = "tts.load"(%73) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %75:7 = "scf.for"(%0, %3, %2, %73, %72, %arg70, %arg71, %arg72, %arg73, %74) ({
              ^bb0(%arg74: i32, %arg75: tensor<2x2x!tt.ptr<f32>>, %arg76: index, %arg77: tensor<2x2xf32>, %arg78: tensor<2x2x!tt.ptr<f32>>, %arg79: index, %arg80: tensor<2x2xf32>, %arg81: tensor<2x2xf32>):
                %76 = "tts.make_tptr"(%arg1, %4, %5, %arg79, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                %77 = "arith.addi"(%arg76, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %78 = "tts.make_tptr"(%arg0, %4, %5, %77, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                %79 = "tts.load"(%78) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%76, %arg80) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %80 = "arith.addi"(%arg79, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %81 = "tts.make_tptr"(%arg1, %4, %5, %80, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%81, %arg81) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %82 = "arith.addi"(%80, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %83 = "tts.make_tptr"(%arg1, %4, %5, %82, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%83, %79) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %84 = "arith.addi"(%82, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %85 = "tts.make_tptr"(%arg1, %4, %5, %84, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                %86:7 = "scf.for"(%0, %3, %2, %arg80, %78, %77, %arg81, %79, %85, %84) ({
                ^bb0(%arg82: i32, %arg83: tensor<2x2xf32>, %arg84: tensor<2x2x!tt.ptr<f32>>, %arg85: index, %arg86: tensor<2x2xf32>, %arg87: tensor<2x2xf32>, %arg88: tensor<2x2x!tt.ptr<f32>>, %arg89: index):
                  %87 = "tts.make_tptr"(%arg0, %4, %5, %arg85, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                  %88 = "tts.load"(%87) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                  %89:7 = "scf.for"(%0, %3, %2, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %88) ({
                  ^bb0(%arg90: i32, %arg91: tensor<2x2x!tt.ptr<f32>>, %arg92: index, %arg93: tensor<2x2xf32>, %arg94: tensor<2x2xf32>, %arg95: tensor<2x2x!tt.ptr<f32>>, %arg96: index, %arg97: tensor<2x2xf32>):
                    %90 = "arith.addi"(%arg92, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                    %91 = "tts.make_tptr"(%arg0, %4, %5, %90, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                    %92 = "tts.load"(%91) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                    %93:7 = "scf.for"(%0, %3, %2, %91, %90, %arg94, %arg95, %arg96, %arg97, %92) ({
                    ^bb0(%arg98: i32, %arg99: tensor<2x2x!tt.ptr<f32>>, %arg100: index, %arg101: tensor<2x2xf32>, %arg102: tensor<2x2x!tt.ptr<f32>>, %arg103: index, %arg104: tensor<2x2xf32>, %arg105: tensor<2x2xf32>):
                      %94 = "tts.make_tptr"(%arg1, %4, %5, %arg103, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                      %95 = "arith.addi"(%arg100, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %96 = "tts.make_tptr"(%arg0, %4, %5, %95, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                      %97 = "tts.load"(%96) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                      "tts.store"(%94, %arg104) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %98 = "arith.addi"(%arg103, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %99 = "tts.make_tptr"(%arg1, %4, %5, %98, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%99, %arg105) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %100 = "arith.addi"(%98, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %101 = "tts.make_tptr"(%arg1, %4, %5, %100, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%101, %97) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %102 = "arith.addi"(%100, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %103 = "tts.make_tptr"(%arg1, %4, %5, %102, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                      %104:7 = "scf.for"(%0, %3, %2, %arg104, %96, %95, %arg105, %97, %103, %102) ({
                      ^bb0(%arg106: i32, %arg107: tensor<2x2xf32>, %arg108: tensor<2x2x!tt.ptr<f32>>, %arg109: index, %arg110: tensor<2x2xf32>, %arg111: tensor<2x2xf32>, %arg112: tensor<2x2x!tt.ptr<f32>>, %arg113: index):
                        %105 = "tts.make_tptr"(%arg0, %4, %5, %arg109, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                        %106 = "tts.load"(%105) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                        %107:6 = "scf.for"(%0, %3, %2, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113) ({
                        ^bb0(%arg114: i32, %arg115: tensor<2x2x!tt.ptr<f32>>, %arg116: index, %arg117: tensor<2x2xf32>, %arg118: tensor<2x2xf32>, %arg119: tensor<2x2x!tt.ptr<f32>>, %arg120: index):
                          %108 = "arith.addi"(%arg116, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                          %109 = "tts.make_tptr"(%arg0, %4, %5, %108, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                          %110 = "tts.load"(%109) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                          %111:5 = "scf.for"(%0, %3, %2, %109, %108, %arg118, %arg119, %arg120) ({
                          ^bb0(%arg121: i32, %arg122: tensor<2x2x!tt.ptr<f32>>, %arg123: index, %arg124: tensor<2x2xf32>, %arg125: tensor<2x2x!tt.ptr<f32>>, %arg126: index):
                            %112 = "tts.make_tptr"(%arg1, %4, %5, %arg126, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                            %113 = "arith.addi"(%arg123, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                            %114 = "tts.make_tptr"(%arg0, %4, %5, %113, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                            %115 = "tts.load"(%114) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                            "tts.store"(%112, %106) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %116 = "arith.addi"(%arg126, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                            %117 = "tts.make_tptr"(%arg1, %4, %5, %116, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%117, %110) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %118 = "arith.addi"(%116, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                            %119 = "tts.make_tptr"(%arg1, %4, %5, %118, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%119, %115) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %120 = "arith.addi"(%118, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                            %121 = "tts.make_tptr"(%arg1, %4, %5, %120, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                            "scf.yield"(%114, %113, %115, %121, %120) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                          }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                          "scf.yield"(%111#0, %111#1, %110, %111#2, %111#3, %111#4) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                        }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                        "scf.yield"(%106, %107#0, %107#1, %107#2, %107#3, %107#4, %107#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                      }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                      "scf.yield"(%104#1, %104#2, %104#4, %104#5, %104#6, %104#0, %104#3) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
                    }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
                    "scf.yield"(%93#0, %93#1, %93#6, %93#2, %93#3, %93#4, %93#5) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
                  }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
                  "scf.yield"(%89#6, %89#0, %89#1, %89#2, %89#3, %89#4, %89#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                "scf.yield"(%86#1, %86#2, %86#4, %86#5, %86#6, %86#0, %86#3) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
              }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
              "scf.yield"(%75#0, %75#1, %75#6, %75#2, %75#3, %75#4, %75#5) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
            }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
            "scf.yield"(%71#6, %71#0, %71#1, %71#2, %71#3, %71#4, %71#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
          }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
          %51:7 = "scf.for"(%0, %3, %2, %50#0, %50#1, %50#2, %50#3, %50#4, %50#5, %50#6) ({
          ^bb0(%arg37: i32, %arg38: tensor<2x2xf32>, %arg39: tensor<2x2x!tt.ptr<f32>>, %arg40: index, %arg41: tensor<2x2xf32>, %arg42: tensor<2x2xf32>, %arg43: tensor<2x2x!tt.ptr<f32>>, %arg44: index):
            %52 = "tts.make_tptr"(%arg0, %4, %5, %arg40, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            %53 = "tts.load"(%52) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %54:6 = "scf.for"(%0, %3, %2, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44) ({
            ^bb0(%arg45: i32, %arg46: tensor<2x2x!tt.ptr<f32>>, %arg47: index, %arg48: tensor<2x2xf32>, %arg49: tensor<2x2xf32>, %arg50: tensor<2x2x!tt.ptr<f32>>, %arg51: index):
              %55 = "arith.addi"(%arg47, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
              %56 = "tts.make_tptr"(%arg0, %4, %5, %55, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
              %57 = "tts.load"(%56) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %58:5 = "scf.for"(%0, %3, %2, %56, %55, %arg49, %arg50, %arg51) ({
              ^bb0(%arg52: i32, %arg53: tensor<2x2x!tt.ptr<f32>>, %arg54: index, %arg55: tensor<2x2xf32>, %arg56: tensor<2x2x!tt.ptr<f32>>, %arg57: index):
                %59 = "tts.make_tptr"(%arg1, %4, %5, %arg57, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                %60 = "arith.addi"(%arg54, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %61 = "tts.make_tptr"(%arg0, %4, %5, %60, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                %62 = "tts.load"(%61) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%59, %53) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %63 = "arith.addi"(%arg57, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %64 = "tts.make_tptr"(%arg1, %4, %5, %63, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%64, %57) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %65 = "arith.addi"(%63, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %66 = "tts.make_tptr"(%arg1, %4, %5, %65, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%66, %62) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %67 = "arith.addi"(%65, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %68 = "tts.make_tptr"(%arg1, %4, %5, %67, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
                "scf.yield"(%61, %60, %62, %68, %67) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
              }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
              "scf.yield"(%58#0, %58#1, %57, %58#2, %58#3, %58#4) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
            }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
            "scf.yield"(%53, %54#0, %54#1, %54#2, %54#3, %54#4, %54#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
          }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
          "scf.yield"(%51#2, %51#5, %51#6, %51#0, %51#3) : (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
        }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
        %39 = "arith.addi"(%38#0, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%39, %38#1, %38#2, %38#3) : (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
      }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
      %13:3 = "scf.for"(%0, %3, %2, %12#0, %12#1, %12#2) ({
      ^bb0(%arg14: i32, %arg15: index, %arg16: tensor<2x2x!tt.ptr<f32>>, %arg17: index):
        %15 = "tts.make_tptr"(%arg0, %4, %5, %arg15, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
        %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %17:3 = "scf.for"(%0, %3, %2, %arg15, %arg16, %arg17) ({
        ^bb0(%arg18: i32, %arg19: index, %arg20: tensor<2x2x!tt.ptr<f32>>, %arg21: index):
          %19 = "arith.addi"(%arg19, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %20 = "tts.make_tptr"(%arg0, %4, %5, %19, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
          %21 = "tts.load"(%20) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          %22:3 = "scf.for"(%0, %3, %2, %19, %arg20, %arg21) ({
          ^bb0(%arg22: i32, %arg23: index, %arg24: tensor<2x2x!tt.ptr<f32>>, %arg25: index):
            %23 = "tts.make_tptr"(%arg1, %4, %5, %arg25, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            %24 = "arith.addi"(%arg23, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %25 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
            %26 = "tts.make_tptr"(%25, %4, %5, %24, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            %27 = "tts.load"(%26) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            "tts.store"(%23, %16) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %28 = "arith.addi"(%arg25, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %29 = "tts.make_tptr"(%arg1, %4, %5, %28, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%29, %21) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %30 = "arith.addi"(%28, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %31 = "tts.make_tptr"(%arg1, %4, %5, %30, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%31, %27) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %32 = "arith.addi"(%30, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %33 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
            %34 = "tts.make_tptr"(%33, %4, %5, %32, %1) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 2, 2>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index, index) -> tensor<2x2x!tt.ptr<f32>>
            "scf.yield"(%24, %34, %32) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
          }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
          "scf.yield"(%22#0, %22#1, %22#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
        }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
        %18 = "arith.addi"(%17#0, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        "scf.yield"(%18, %17#1, %17#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
      %14 = "arith.addi"(%13#0, %8) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%14, %13#1, %13#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir:23:11: error: failed to legalize unresolved materialization from ('memref<2x2xf32, strided<[?, ?]>>') to 'tensor<2x2x!tt.ptr<f32>>' that remained live after conversion
    %15 = tt.addptr %14, %8 : tensor<2x2x!tt.ptr<f32>>, tensor<2x2xi32>
          ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir:23:11: note: see current operation: %7 = "builtin.unrealized_conversion_cast"(%6) : (memref<2x2xf32, strided<[?, ?]>>) -> tensor<2x2x!tt.ptr<f32>>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir:32:13: note: see existing live user here: 
%10:3 = "scf.for"(%0, %3, %2, %1, %7, %1) ({
^bb0(%arg10: i32, %arg11: index, %arg12: tensor<2x2x!tt.ptr<f32>>, %arg13: index):
  %11 = "arith.addi"(%arg11, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %12 = "memref.reinterpret_cast"(%arg0, %11, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
  %13 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
  "memref.copy"(%12, %13) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
  %14 = "bufferization.to_tensor"(%13) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
  %15:4 = "scf.for"(%0, %3, %2, %arg11, %arg12, %arg13, %14) ({
  ^bb0(%arg26: i32, %arg27: index, %arg28: tensor<2x2x!tt.ptr<f32>>, %arg29: index, %arg30: tensor<2x2xf32>):
    %49 = "arith.addi"(%arg27, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %50 = "arith.addi"(%49, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %51 = "memref.reinterpret_cast"(%arg0, %50, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
    %52 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
    "memref.copy"(%51, %52) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
    %53 = "bufferization.to_tensor"(%52) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
    %54:5 = "scf.for"(%0, %3, %2, %49, %arg28, %arg29, %arg30, %53) ({
    ^bb0(%arg31: i32, %arg32: index, %arg33: tensor<2x2x!tt.ptr<f32>>, %arg34: index, %arg35: tensor<2x2xf32>, %arg36: tensor<2x2xf32>):
      %56 = "arith.addi"(%arg34, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %57 = "memref.reinterpret_cast"(%arg1, %56, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %58 = "arith.addi"(%arg32, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %59 = "arith.addi"(%58, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %60 = "memref.reinterpret_cast"(%arg0, %59, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %61 = "builtin.unrealized_conversion_cast"(%60) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
      %62 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
      "memref.copy"(%60, %62) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
      %63 = "bufferization.to_tensor"(%62) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
      "bufferization.materialize_in_destination"(%arg35, %57) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %64 = "arith.addi"(%arg34, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %65 = "arith.addi"(%64, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %66 = "memref.reinterpret_cast"(%arg1, %65, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      "bufferization.materialize_in_destination"(%arg36, %66) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %67 = "arith.addi"(%64, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %68 = "arith.addi"(%67, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %69 = "memref.reinterpret_cast"(%arg1, %68, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      "bufferization.materialize_in_destination"(%63, %69) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
      %70 = "arith.addi"(%67, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %71 = "arith.addi"(%70, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %72 = "memref.reinterpret_cast"(%arg1, %71, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %73 = "builtin.unrealized_conversion_cast"(%72) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
      %74:7 = "scf.for"(%0, %3, %2, %arg35, %61, %58, %arg36, %63, %73, %70) ({
      ^bb0(%arg58: i32, %arg59: tensor<2x2xf32>, %arg60: tensor<2x2x!tt.ptr<f32>>, %arg61: index, %arg62: tensor<2x2xf32>, %arg63: tensor<2x2xf32>, %arg64: tensor<2x2x!tt.ptr<f32>>, %arg65: index):
        %106 = "arith.addi"(%arg61, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %107 = "memref.reinterpret_cast"(%arg0, %106, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        %108 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
        "memref.copy"(%107, %108) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
        %109 = "bufferization.to_tensor"(%108) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
        %110:7 = "scf.for"(%0, %3, %2, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %109) ({
        ^bb0(%arg66: i32, %arg67: tensor<2x2x!tt.ptr<f32>>, %arg68: index, %arg69: tensor<2x2xf32>, %arg70: tensor<2x2xf32>, %arg71: tensor<2x2x!tt.ptr<f32>>, %arg72: index, %arg73: tensor<2x2xf32>):
          %111 = "arith.addi"(%arg68, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %112 = "arith.addi"(%111, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %113 = "memref.reinterpret_cast"(%arg0, %112, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
          %114 = "builtin.unrealized_conversion_cast"(%113) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
          %115 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
          "memref.copy"(%113, %115) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
          %116 = "bufferization.to_tensor"(%115) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
          %117:7 = "scf.for"(%0, %3, %2, %114, %111, %arg70, %arg71, %arg72, %arg73, %116) ({
          ^bb0(%arg74: i32, %arg75: tensor<2x2x!tt.ptr<f32>>, %arg76: index, %arg77: tensor<2x2xf32>, %arg78: tensor<2x2x!tt.ptr<f32>>, %arg79: index, %arg80: tensor<2x2xf32>, %arg81: tensor<2x2xf32>):
            %118 = "arith.addi"(%arg79, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %119 = "memref.reinterpret_cast"(%arg1, %118, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %120 = "arith.addi"(%arg76, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %121 = "arith.addi"(%120, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %122 = "memref.reinterpret_cast"(%arg0, %121, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %123 = "builtin.unrealized_conversion_cast"(%122) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
            %124 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
            "memref.copy"(%122, %124) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
            %125 = "bufferization.to_tensor"(%124) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
            "bufferization.materialize_in_destination"(%arg80, %119) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %126 = "arith.addi"(%arg79, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %127 = "arith.addi"(%126, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %128 = "memref.reinterpret_cast"(%arg1, %127, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            "bufferization.materialize_in_destination"(%arg81, %128) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %129 = "arith.addi"(%126, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %130 = "arith.addi"(%129, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %131 = "memref.reinterpret_cast"(%arg1, %130, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            "bufferization.materialize_in_destination"(%125, %131) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %132 = "arith.addi"(%129, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %133 = "arith.addi"(%132, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %134 = "memref.reinterpret_cast"(%arg1, %133, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %135 = "builtin.unrealized_conversion_cast"(%134) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
            %136:7 = "scf.for"(%0, %3, %2, %arg80, %123, %120, %arg81, %125, %135, %132) ({
            ^bb0(%arg82: i32, %arg83: tensor<2x2xf32>, %arg84: tensor<2x2x!tt.ptr<f32>>, %arg85: index, %arg86: tensor<2x2xf32>, %arg87: tensor<2x2xf32>, %arg88: tensor<2x2x!tt.ptr<f32>>, %arg89: index):
              %137 = "arith.addi"(%arg85, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
              %138 = "memref.reinterpret_cast"(%arg0, %137, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
              %139 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
              "memref.copy"(%138, %139) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
              %140 = "bufferization.to_tensor"(%139) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
              %141:7 = "scf.for"(%0, %3, %2, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %140) ({
              ^bb0(%arg90: i32, %arg91: tensor<2x2x!tt.ptr<f32>>, %arg92: index, %arg93: tensor<2x2xf32>, %arg94: tensor<2x2xf32>, %arg95: tensor<2x2x!tt.ptr<f32>>, %arg96: index, %arg97: tensor<2x2xf32>):
                %142 = "arith.addi"(%arg92, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %143 = "arith.addi"(%142, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                %144 = "memref.reinterpret_cast"(%arg0, %143, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                %145 = "builtin.unrealized_conversion_cast"(%144) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                %146 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
                "memref.copy"(%144, %146) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
                %147 = "bufferization.to_tensor"(%146) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
                %148:7 = "scf.for"(%0, %3, %2, %145, %142, %arg94, %arg95, %arg96, %arg97, %147) ({
                ^bb0(%arg98: i32, %arg99: tensor<2x2x!tt.ptr<f32>>, %arg100: index, %arg101: tensor<2x2xf32>, %arg102: tensor<2x2x!tt.ptr<f32>>, %arg103: index, %arg104: tensor<2x2xf32>, %arg105: tensor<2x2xf32>):
                  %149 = "arith.addi"(%arg103, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %150 = "memref.reinterpret_cast"(%arg1, %149, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                  %151 = "arith.addi"(%arg100, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %152 = "arith.addi"(%151, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %153 = "memref.reinterpret_cast"(%arg0, %152, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                  %154 = "builtin.unrealized_conversion_cast"(%153) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                  %155 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
                  "memref.copy"(%153, %155) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
                  %156 = "bufferization.to_tensor"(%155) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
                  "bufferization.materialize_in_destination"(%arg104, %150) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                  %157 = "arith.addi"(%arg103, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %158 = "arith.addi"(%157, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %159 = "memref.reinterpret_cast"(%arg1, %158, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                  "bufferization.materialize_in_destination"(%arg105, %159) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                  %160 = "arith.addi"(%157, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %161 = "arith.addi"(%160, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %162 = "memref.reinterpret_cast"(%arg1, %161, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                  "bufferization.materialize_in_destination"(%156, %162) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                  %163 = "arith.addi"(%160, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %164 = "arith.addi"(%163, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                  %165 = "memref.reinterpret_cast"(%arg1, %164, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                  %166 = "builtin.unrealized_conversion_cast"(%165) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                  %167:7 = "scf.for"(%0, %3, %2, %arg104, %154, %151, %arg105, %156, %166, %163) ({
                  ^bb0(%arg106: i32, %arg107: tensor<2x2xf32>, %arg108: tensor<2x2x!tt.ptr<f32>>, %arg109: index, %arg110: tensor<2x2xf32>, %arg111: tensor<2x2xf32>, %arg112: tensor<2x2x!tt.ptr<f32>>, %arg113: index):
                    %168 = "arith.addi"(%arg109, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                    %169 = "memref.reinterpret_cast"(%arg0, %168, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                    %170 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
                    "memref.copy"(%169, %170) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
                    %171 = "bufferization.to_tensor"(%170) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
                    %172:6 = "scf.for"(%0, %3, %2, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113) ({
                    ^bb0(%arg114: i32, %arg115: tensor<2x2x!tt.ptr<f32>>, %arg116: index, %arg117: tensor<2x2xf32>, %arg118: tensor<2x2xf32>, %arg119: tensor<2x2x!tt.ptr<f32>>, %arg120: index):
                      %173 = "arith.addi"(%arg116, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %174 = "arith.addi"(%173, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                      %175 = "memref.reinterpret_cast"(%arg0, %174, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                      %176 = "builtin.unrealized_conversion_cast"(%175) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                      %177 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
                      "memref.copy"(%175, %177) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
                      %178 = "bufferization.to_tensor"(%177) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
                      %179:5 = "scf.for"(%0, %3, %2, %176, %173, %arg118, %arg119, %arg120) ({
                      ^bb0(%arg121: i32, %arg122: tensor<2x2x!tt.ptr<f32>>, %arg123: index, %arg124: tensor<2x2xf32>, %arg125: tensor<2x2x!tt.ptr<f32>>, %arg126: index):
                        %180 = "arith.addi"(%arg126, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %181 = "memref.reinterpret_cast"(%arg1, %180, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                        %182 = "arith.addi"(%arg123, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %183 = "arith.addi"(%182, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %184 = "memref.reinterpret_cast"(%arg0, %183, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                        %185 = "builtin.unrealized_conversion_cast"(%184) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                        %186 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
                        "memref.copy"(%184, %186) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
                        %187 = "bufferization.to_tensor"(%186) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
                        "bufferization.materialize_in_destination"(%171, %181) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                        %188 = "arith.addi"(%arg126, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %189 = "arith.addi"(%188, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %190 = "memref.reinterpret_cast"(%arg1, %189, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                        "bufferization.materialize_in_destination"(%178, %190) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                        %191 = "arith.addi"(%188, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %192 = "arith.addi"(%191, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %193 = "memref.reinterpret_cast"(%arg1, %192, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                        "bufferization.materialize_in_destination"(%187, %193) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
                        %194 = "arith.addi"(%191, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %195 = "arith.addi"(%194, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
                        %196 = "memref.reinterpret_cast"(%arg1, %195, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
                        %197 = "builtin.unrealized_conversion_cast"(%196) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
                        "scf.yield"(%185, %182, %187, %197, %194) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                      }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                      "scf.yield"(%179#0, %179#1, %178, %179#2, %179#3, %179#4) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                    }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                    "scf.yield"(%171, %172#0, %172#1, %172#2, %172#3, %172#4, %172#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
                  }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
                  "scf.yield"(%167#1, %167#2, %167#4, %167#5, %167#6, %167#0, %167#3) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
                }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
                "scf.yield"(%148#0, %148#1, %148#6, %148#2, %148#3, %148#4, %148#5) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
              }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
              "scf.yield"(%141#6, %141#0, %141#1, %141#2, %141#3, %141#4, %141#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
            }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
            "scf.yield"(%136#1, %136#2, %136#4, %136#5, %136#6, %136#0, %136#3) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
          }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
          "scf.yield"(%117#0, %117#1, %117#6, %117#2, %117#3, %117#4, %117#5) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
        }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
        "scf.yield"(%110#6, %110#0, %110#1, %110#2, %110#3, %110#4, %110#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
      %75:7 = "scf.for"(%0, %3, %2, %74#0, %74#1, %74#2, %74#3, %74#4, %74#5, %74#6) ({
      ^bb0(%arg37: i32, %arg38: tensor<2x2xf32>, %arg39: tensor<2x2x!tt.ptr<f32>>, %arg40: index, %arg41: tensor<2x2xf32>, %arg42: tensor<2x2xf32>, %arg43: tensor<2x2x!tt.ptr<f32>>, %arg44: index):
        %76 = "arith.addi"(%arg40, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %77 = "memref.reinterpret_cast"(%arg0, %76, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        %78 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
        "memref.copy"(%77, %78) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
        %79 = "bufferization.to_tensor"(%78) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
        %80:6 = "scf.for"(%0, %3, %2, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44) ({
        ^bb0(%arg45: i32, %arg46: tensor<2x2x!tt.ptr<f32>>, %arg47: index, %arg48: tensor<2x2xf32>, %arg49: tensor<2x2xf32>, %arg50: tensor<2x2x!tt.ptr<f32>>, %arg51: index):
          %81 = "arith.addi"(%arg47, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %82 = "arith.addi"(%81, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %83 = "memref.reinterpret_cast"(%arg0, %82, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
          %84 = "builtin.unrealized_conversion_cast"(%83) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
          %85 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
          "memref.copy"(%83, %85) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
          %86 = "bufferization.to_tensor"(%85) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
          %87:5 = "scf.for"(%0, %3, %2, %84, %81, %arg49, %arg50, %arg51) ({
          ^bb0(%arg52: i32, %arg53: tensor<2x2x!tt.ptr<f32>>, %arg54: index, %arg55: tensor<2x2xf32>, %arg56: tensor<2x2x!tt.ptr<f32>>, %arg57: index):
            %88 = "arith.addi"(%arg57, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %89 = "memref.reinterpret_cast"(%arg1, %88, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %90 = "arith.addi"(%arg54, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %91 = "arith.addi"(%90, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %92 = "memref.reinterpret_cast"(%arg0, %91, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %93 = "builtin.unrealized_conversion_cast"(%92) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
            %94 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
            "memref.copy"(%92, %94) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
            %95 = "bufferization.to_tensor"(%94) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
            "bufferization.materialize_in_destination"(%79, %89) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %96 = "arith.addi"(%arg57, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %97 = "arith.addi"(%96, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %98 = "memref.reinterpret_cast"(%arg1, %97, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            "bufferization.materialize_in_destination"(%86, %98) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %99 = "arith.addi"(%96, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %100 = "arith.addi"(%99, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %101 = "memref.reinterpret_cast"(%arg1, %100, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            "bufferization.materialize_in_destination"(%95, %101) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
            %102 = "arith.addi"(%99, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %103 = "arith.addi"(%102, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
            %104 = "memref.reinterpret_cast"(%arg1, %103, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
            %105 = "builtin.unrealized_conversion_cast"(%104) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
            "scf.yield"(%93, %90, %95, %105, %102) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
          }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
          "scf.yield"(%87#0, %87#1, %86, %87#2, %87#3, %87#4) : (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
        }) : (i32, i32, i32, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
        "scf.yield"(%79, %80#0, %80#1, %80#2, %80#3, %80#4, %80#5) : (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)
      "scf.yield"(%75#2, %75#5, %75#6, %75#0, %75#3) : (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)
    %55 = "arith.addi"(%54#0, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    "scf.yield"(%55, %54#1, %54#2, %54#3) : (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> ()
  }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)
  %16:3 = "scf.for"(%0, %3, %2, %15#0, %15#1, %15#2) ({
  ^bb0(%arg14: i32, %arg15: index, %arg16: tensor<2x2x!tt.ptr<f32>>, %arg17: index):
    %18 = "arith.addi"(%arg15, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %19 = "memref.reinterpret_cast"(%arg0, %18, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
    %20 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
    "memref.copy"(%19, %20) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
    %21 = "bufferization.to_tensor"(%20) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
    %22:3 = "scf.for"(%0, %3, %2, %arg15, %arg16, %arg17) ({
    ^bb0(%arg18: i32, %arg19: index, %arg20: tensor<2x2x!tt.ptr<f32>>, %arg21: index):
      %24 = "arith.addi"(%arg19, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %25 = "arith.addi"(%24, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %26 = "memref.reinterpret_cast"(%arg0, %25, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
      %27 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
      "memref.copy"(%26, %27) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
      %28 = "bufferization.to_tensor"(%27) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
      %29:3 = "scf.for"(%0, %3, %2, %24, %arg20, %arg21) ({
      ^bb0(%arg22: i32, %arg23: index, %arg24: tensor<2x2x!tt.ptr<f32>>, %arg25: index):
        %30 = "arith.addi"(%arg25, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %31 = "memref.reinterpret_cast"(%arg1, %30, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        %32 = "arith.addi"(%arg23, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %33 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
        %34 = "arith.addi"(%32, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %35 = "memref.reinterpret_cast"(%33, %34, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        %36 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<2x2xf32>
        "memref.copy"(%35, %36) : (memref<2x2xf32, strided<[?, ?], offset: ?>>, memref<2x2xf32>) -> ()
        %37 = "bufferization.to_tensor"(%36) <{restrict, writable}> : (memref<2x2xf32>) -> tensor<2x2xf32>
        "bufferization.materialize_in_destination"(%21, %31) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
        %38 = "arith.addi"(%arg25, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %39 = "arith.addi"(%38, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %40 = "memref.reinterpret_cast"(%arg1, %39, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        "bufferization.materialize_in_destination"(%28, %40) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
        %41 = "arith.addi"(%38, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %42 = "arith.addi"(%41, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %43 = "memref.reinterpret_cast"(%arg1, %42, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<f32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        "bufferization.materialize_in_destination"(%37, %43) <{writable}> : (tensor<2x2xf32>, memref<2x2xf32, strided<[?, ?], offset: ?>>) -> ()
        %44 = "arith.addi"(%41, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %45 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
        %46 = "arith.addi"(%44, %1) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %47 = "memref.reinterpret_cast"(%45, %46, %4, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 2>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 2, 2>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (memref<*xf32>, index, index, index) -> memref<2x2xf32, strided<[?, ?], offset: ?>>
        %48 = "builtin.unrealized_conversion_cast"(%47) : (memref<2x2xf32, strided<[?, ?], offset: ?>>) -> tensor<2x2x!tt.ptr<f32>>
        "scf.yield"(%32, %48, %44) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
      }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
      "scf.yield"(%29#0, %29#1, %29#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
    }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    %23 = "arith.addi"(%22#0, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    "scf.yield"(%23, %22#1, %22#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
  }) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
  %17 = "arith.addi"(%16#0, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  "scf.yield"(%17, %16#1, %16#2) : (index, tensor<2x2x!tt.ptr<f32>>, index) -> ()
}) : (i32, i32, i32, index, tensor<2x2x!tt.ptr<f32>>, index) -> (index, tensor<2x2x!tt.ptr<f32>>, index)
    %24:2 = scf.for %arg4 = %c0_i32 to %c2_i32 step %c1_i32 iter_args(%arg5 = %11, %arg6 = %15) -> (tensor<2x2x!tt.ptr<f32>>, tensor<2x2x!tt.ptr<f32>>)  : i32 {
            ^
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_chain.mlir (25 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_chain.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: remark: PtrAnalysis: scalar loadOp will not be rewritten
      %6 = tt.load %2 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: note: see current operation: %16 = tt.load %8 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: remark: PtrAnalysis: Failed to rewrite LoadOp
      %6 = tt.load %2 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: note: see current operation: %16 = tt.load %8 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: remark: PtrAnalysis: scalar loadOp will not be rewritten
      %7 = tt.load %3 : !tt.ptr<f32>
        + FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
   ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: note: see current operation: %17 = tt.load %10 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: remark: PtrAnalysis: Failed to rewrite LoadOp
      %7 = tt.load %3 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: note: see current operation: %17 = tt.load %10 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: remark: PtrAnalysis: scalar storeOp will not be rewritten
      tt.store %4, %6 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: note: see current operation: tt.store %13, %16 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: remark: PtrAnalysis: Failed to rewrite StoreOp
      tt.store %4, %6 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: note: see current operation: tt.store %13, %16 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: remark: PtrAnalysis: scalar storeOp will not be rewritten
      tt.store %5, %7 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: note: see current operation: tt.store %15, %17 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: remark: PtrAnalysis: Failed to rewrite StoreOp
      tt.store %5, %7 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: note: see current operation: tt.store %15, %17 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "addptr", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 10 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
    %7 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
    "scf.for"(%4, %3, %2) ({
    ^bb0(%arg2: i32):
      %8 = "tt.addptr"(%6, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %9 = "tt.addptr"(%8, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %11 = "tt.addptr"(%10, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %12 = "tt.load"(%8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %13 = "tt.load"(%9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      "tt.store"(%10, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "tt.store"(%11, %13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%8 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%8 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%13 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%13 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
%16 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
%12 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%16 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
!tt.ptr<f32>
processing user
%12 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%15, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
%16 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
"tt.store"(%15, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
processing user
%16 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
%20 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
actual processing
processing user
%20 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
!tt.ptr<f32>
~~~~
processing val
%17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
actual processing
processing user
"tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "addptr", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 10 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %6 = "arith.addi"(%1, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
    %8 = "arith.addi"(%0, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
    "scf.for"(%4, %3, %2) ({
    ^bb0(%arg2: i32):
      %10 = "arith.addi"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %12 = "arith.addi"(%11, %5) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %14 = "arith.addi"(%9, %arg2) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %16 = "arith.addi"(%15, %5) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %18 = "tts.create_ptr"(%arg0, %10) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %19 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %20 = "tts.create_ptr"(%arg0, %12) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %21 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %22 = "tts.create_ptr"(%arg1, %14) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "tt.store"(%15, %19) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      %23 = "tts.create_ptr"(%arg1, %16) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
total addptr count: 6
deleting
%15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
deleting
%13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
module {
  func.func @addptr(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %c10_i32 = arith.constant 10 : i32
    %c1_i32 = arith.constant 1 : i32
    scf.for %arg8 = %c0_i32 to %c10_i32 step %c2_i32  : i32 {
      %0 = arith.addi %arg8, %c1_i32 : i32
      %1 = arith.addi %0, %c1_i32 : i32
      %2 = arith.addi %arg8, %c1_i32 : i32
      %3 = arith.addi %2, %c1_i32 : i32
      %4 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %5 = tt.load %4 : !tt.ptr<f32>
      %6 = "tts.create_ptr"(%arg0, %1) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %7 = tt.load %6 : !tt.ptr<f32>
      %8 = "tts.create_ptr"(%arg1, %2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      tt.store %8, %5 : !tt.ptr<f32>
      %9 = "tts.create_ptr"(%arg1, %3) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      tt.store %9, %7 : !tt.ptr<f32>
    }
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:27:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: %c2 = arith.constant 2 : index
              ^
<stdin>:2:137: note: scanning from here
 func.func @addptr(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
                                                                                                                                        ^
<stdin>:4:6: note: possible intended match here
 %c2_i32 = arith.constant 2 : i32
     ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @addptr(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) { 
dag:27'0                                                                                                                                             X error: no match found
          3:  %c0_i32 = arith.constant 0 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %c2_i32 = arith.constant 2 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:27'1          ?                             possible intended match
          5:  %c10_i32 = arith.constant 10 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          6:  %c1_i32 = arith.constant 1 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          7:  scf.for %arg8 = %c0_i32 to %c10_i32 step %c2_i32 : i32 { 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          8:  %0 = arith.addi %arg8, %c1_i32 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          9:  %1 = arith.addi %arg8, %c2_i32 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir (26 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_2d_example.mlir (27 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/get_num_programs.mlir (28 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/get_num_programs.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
module {
  tt.func public @num_programs(%arg0: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = tt.get_num_programs x : i32
    %1 = tt.get_num_programs y : i32
    %2 = tt.get_num_programs z : i32
    %3 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c0], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %4 = tt.splat %0 : i32 -> tensor<1xi32>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %5 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c1], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %6 = tt.splat %1 : i32 -> tensor<1xi32>
    "tts.store"(%5, %6) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %7 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c2], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %8 = tt.splat %2 : i32 -> tensor<1xi32>
    "tts.store"(%7, %8) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    tt.return
  }
}
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @num_programs(%arg0: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = tt.get_num_programs x : i32
    %1 = tt.get_num_programs y : i32
    %2 = tt.get_num_programs z : i32
    %3 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c0], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %4 = tt.splat %0 : i32 -> tensor<1xi32>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %5 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c1], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %6 = tt.splat %1 : i32 -> tensor<1xi32>
    "tts.store"(%5, %6) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %7 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c2], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %8 = tt.splat %2 : i32 -> tensor<1xi32>
    "tts.store"(%7, %8) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<i32>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "num_programs"}> ({
  ^bb0(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32):
    %0 = "arith.constant"() <{value = 2 : index}> : () -> index
    %1 = "arith.constant"() <{value = 1 : index}> : () -> index
    %2 = "arith.constant"() <{value = 0 : index}> : () -> index
    %3 = "tts.make_tptr"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 0>}> : (!tt.ptr<i32>, index) -> tensor<1x!tt.ptr<i32>>
    %4 = "tensor.empty"() : () -> tensor<1xi32>
    %5 = "linalg.fill"(%arg1, %4) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg11: i32, %arg12: i32):
      "linalg.yield"(%arg11) : (i32) -> ()
    }) : (i32, tensor<1xi32>) -> tensor<1xi32>
    "tts.store"(%3, %5) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %6 = "tts.make_tptr"(%arg0, %1) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 0>}> : (!tt.ptr<i32>, index) -> tensor<1x!tt.ptr<i32>>
    %7 = "tensor.empty"() : () -> tensor<1xi32>
    %8 = "linalg.fill"(%arg2, %7) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg9: i32, %arg10: i32):
      "linalg.yield"(%arg9) : (i32) -> ()
    }) : (i32, tensor<1xi32>) -> tensor<1xi32>
    "tts.store"(%6, %8) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %9 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<i32>) -> memref<*xi32>
    %10 = "tts.make_tptr"(%9, %0) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 0>}> : (memref<*xi32>, index) -> tensor<1x!tt.ptr<i32>>
    %11 = "tensor.empty"() : () -> tensor<1xi32>
    %12 = "linalg.fill"(%arg3, %11) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg7: i32, %arg8: i32):
      "linalg.yield"(%arg7) : (i32) -> ()
    }) : (i32, tensor<1xi32>) -> tensor<1xi32>
    "tts.store"(%10, %12) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir:1 offset :12:10: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<i32>'
    %7 = tt.addptr %6, %3 : tensor<1x!tt.ptr<i32>>, tensor<1xi32>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir:1 offset :12:10: note: see current operation: %3 = "memref.reinterpret_cast"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>, index) -> memref<1xi32, strided<[1], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for.mlir (29 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
    %3 = "arith.constant"() <{value = 3 : index}> : () -> index
    %4 = "arith.constant"() <{value = 12 : index}> : () -> index
    %5 = "arith.constant"() <{value = 0 : index}> : () -> index
    %6 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %7 = "arith.muli"(%6, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
    %10:3 = "scf.for"(%5, %4, %3, %9, %8, %2) ({
    ^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
      %14 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
      %15 = "tts.load"(%14) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
      %16 = "math.exp"(%15) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
      %17 = "arith.addf"(%arg8, %16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %18 = "arith.index_cast"(%arg5) : (index) -> i32
      %19 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %20 = "tt.addptr"(%arg6, %18) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "scf.yield"(%20, %19, %17) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
    }) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
    %11 = "arith.muli"(%6, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %12 = "arith.index_cast"(%11) : (i32) -> index
    %13 = "tts.make_tptr"(%arg0, %12) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
    "tts.store"(%13, %10#2) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "tt.addptr"(%arg6, %19) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
actual processing
processing user
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "tt.addptr"(%arg6, %19) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
arg number: 3
init arg size
3
num region iter-args
3
dump from that index
iter arg
init arg
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
%8 = "arith.index_cast"(%7) : (i32) -> index
%2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
~~~~
processing val
<block argument> of type 'i32' at index: 1
these are the uses:
%21 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%21 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: i32, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "arith.addi"(%arg6, %19) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
  "scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (i32, index, tensor<1024xf32>)
these are the uses:
actual processing
~~~~
processing val
%22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
actual processing
processing user
"scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
~~~~
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
    %3 = "arith.constant"() <{value = 3 : index}> : () -> index
    %4 = "arith.constant"() <{value = 12 : index}> : () -> index
    %5 = "arith.constant"() <{value = 0 : index}> : () -> index
    %6 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %7 = "arith.muli"(%6, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9 = "arith.addi"(%0, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
    %11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
    ^bb0(%arg5: index, %arg6: i32, %arg7: index, %arg8: tensor<1024xf32>):
      %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
      %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
      %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
      %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %19 = "arith.index_cast"(%arg5) : (index) -> i32
      %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %21 = "arith.addi"(%arg6, %19) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
      "scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
    }) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (i32, index, tensor<1024xf32>)
    %12 = "arith.muli"(%6, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %13 = "arith.index_cast"(%12) : (i32) -> index
    %14 = "tts.make_tptr"(%arg0, %13) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
    "tts.store"(%14, %11#2) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
deleting
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
"builtin.module"() ({
  "func.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}, {}, {}, {}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32):
    %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %1 = "tensor.empty"() : () -> tensor<1024xf32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg20: f32, %arg21: f32):
      "linalg.yield"(%arg20) : (f32) -> ()
    }) : (f32, tensor<1024xf32>) -> tensor<1024xf32>
    %3 = "arith.constant"() <{value = 3 : index}> : () -> index
    %4 = "arith.constant"() <{value = 12 : index}> : () -> index
    %5 = "arith.constant"() <{value = 0 : index}> : () -> index
    %6 = "arith.muli"(%arg8, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %7 = "arith.index_cast"(%6) : (i32) -> index
    %8:3 = "scf.for"(%5, %4, %3, %6, %7, %2) ({
    ^bb0(%arg11: index, %arg12: i32, %arg13: index, %arg14: tensor<1024xf32>):
      %13 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
      %14 = "tts.make_tptr"(%13, %arg13) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<1024x!tt.ptr<f32>>
      %15 = "tts.load"(%14) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
      %16 = "linalg.generic"(%15, %15) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg18: f32, %arg19: f32):
        %22 = "math.exp"(%arg18) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        "linalg.yield"(%22) : (f32) -> ()
      }) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %17 = "linalg.generic"(%arg14, %16, %arg14) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg15: f32, %arg16: f32, %arg17: f32):
        %21 = "arith.addf"(%arg15, %arg16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%21) : (f32) -> ()
      }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %18 = "arith.index_cast"(%arg11) : (index) -> i32
      %19 = "arith.addi"(%arg13, %arg11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %20 = "arith.addi"(%arg12, %18) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      "scf.yield"(%20, %19, %17) : (i32, index, tensor<1024xf32>) -> ()
    }) : (index, index, index, i32, index, tensor<1024xf32>) -> (i32, index, tensor<1024xf32>)
    %9 = "arith.muli"(%arg8, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %10 = "arith.index_cast"(%9) : (i32) -> index
    %11 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %12 = "tts.make_tptr"(%11, %10) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>, index) -> tensor<1024x!tt.ptr<f32>>
    "tts.store"(%12, %8#2) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir:50:11: error: CHECK: expected string not found in input
// CHECK: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_3_]]{{.}}, sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
          ^
<stdin>:11:41: note: scanning from here
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:11:41: note: with "PARAM_1_" equal to "%arg1"
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:11:41: note: with "VAR_3_" equal to "%3"
 %3 = arith.index_cast %2 : i32 to index
                                        ^
<stdin>:34:18: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%6], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>
                 ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
            .
            .
            .
            6:  %c3 = arith.constant 3 : index 
            7:  %cst = arith.constant 0.000000e+00 : f32 
            8:  %0 = tensor.empty() : tensor<1024xf32> 
            9:  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32> 
           10:  %2 = arith.muli %arg8, %arg2 : i32 
           11:  %3 = arith.index_cast %2 : i32 to index 
check:50'0                                             X error: no match found
check:50'1                                               with "PARAM_1_" equal to "%arg1"
check:50'2                                               with "VAR_3_" equal to "%3"
           12:  %4:3 = scf.for %arg11 = %c0 to %c12 step %c3 iter_args(%arg12 = %2, %arg13 = %3, %arg14 = %1) -> (i32, index, tensor<1024xf32>) { 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           13:  %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%arg13], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           14:  %alloc = memref.alloc() : memref<1024xf32> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           15:  memref.copy %reinterpret_cast_0, %alloc : memref<1024xf32, strided<[1], offset: ?>> to memref<1024xf32> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           16:  %7 = bufferization.to_tensor %alloc restrict writable : memref<1024xf32> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            .
            .
            .
           29:  %12 = arith.addi %arg12, %10 : i32 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           30:  scf.yield %12, %11, %9 : i32, index, tensor<1024xf32> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           31:  } 
check:50'0     ~~~
           32:  %5 = arith.muli %arg8, %arg3 : i32 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           33:  %6 = arith.index_cast %5 : i32 to index 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           34:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%6], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>> 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
check:50'3                      ?                                                                                                                                            possible intended match
           35:  bufferization.materialize_in_destination %4#2 in writable %reinterpret_cast : (tensor<1024xf32>, memref<1024xf32, strided<[1], offset: ?>>) -> () 
check:50'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           36:  return 
check:50'0     ~~~~~~~~
           37:  } 
check:50'0     ~~~
           38: } 
check:50'0     ~~
           39:  
check:50'0     ~
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_splat.mlir (30 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_nested.mlir (31 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/early_return.mlir (32 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/masked_ldst_1d.mlir (33 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/convert_tensor_reshape.mlir (34 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/masked_ldst_2d.mlir (35 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir (36 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir:12:7: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
      tt.store %arg11, %2 : !tt.ptr<f32>
      ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir:12:7: note: see current operation: tt.store %arg11, %3 : !tt.ptr<f32>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir:12:7: remark: PtrAnalysis: Failed to rewrite StoreOp
      tt.store %arg11, %2 : !tt.ptr<f32>
      ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir:12:7: note: see current operation: tt.store %arg11, %3 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, {}, {}, {}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "reduce_kernel_2d_0d1d2de3de"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 5 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.index_cast"(%arg7) : (i32) -> index
    %6 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
    %7 = "arith.sitofp"(%arg7) : (i32) -> f32
    %8:3 = "scf.for"(%4, %3, %2, %6, %5, %5) ({
    ^bb0(%arg10: i32, %arg11: !tt.ptr<f32>, %arg12: index, %arg13: index):
      "tt.store"(%arg11, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      %9 = "arith.index_cast"(%arg10) : (i32) -> index
      %10 = "arith.addi"(%arg12, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %11 = "tt.addptr"(%arg11, %arg10) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %12 = "arith.addi"(%arg13, %9) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%11, %10, %12) : (!tt.ptr<f32>, index, index) -> ()
    }) : (i32, i32, i32, !tt.ptr<f32>, index, index) -> (!tt.ptr<f32>, index, index)
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%6 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%6 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%7 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%9:3 = "scf.for"(%4, %3, %2, %7, %5, %5) ({
^bb0(%arg10: i32, %arg11: !tt.ptr<f32>, %arg12: index, %arg13: index):
  "tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
  %10 = "arith.index_cast"(%arg10) : (i32) -> index
  %11 = "arith.addi"(%arg12, %10) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %12 = "tt.addptr"(%arg11, %arg10) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  %13 = "arith.addi"(%arg13, %10) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  "scf.yield"(%12, %11, %13) : (!tt.ptr<f32>, index, index) -> ()
}) : (i32, i32, i32, !tt.ptr<f32>, index, index) -> (!tt.ptr<f32>, index, index)
actual processing
processing user
%9:3 = "scf.for"(%4, %3, %2, %7, %5, %5) ({
^bb0(%arg10: i32, %arg11: !tt.ptr<f32>, %arg12: index, %arg13: index):
  "tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
  %10 = "arith.index_cast"(%arg10) : (i32) -> index
  %11 = "arith.addi"(%arg12, %10) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %12 = "tt.addptr"(%arg11, %arg10) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  %13 = "arith.addi"(%arg13, %10) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  "scf.yield"(%12, %11, %13) : (!tt.ptr<f32>, index, index) -> ()
}) : (i32, i32, i32, !tt.ptr<f32>, index, index) -> (!tt.ptr<f32>, index, index)
arg number: 3
init arg size
3
num region iter-args
3
dump from that index
iter arg
init arg
%7 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
%5 = "arith.index_cast"(%arg7) : (i32) -> index
%5 = "arith.index_cast"(%arg7) : (i32) -> index
~~~~
processing val
<block argument> of type 'i32' at index: 1
these are the uses:
"tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
%12 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
"tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
processing user
%13 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%9:3 = "scf.for"(%4, %3, %2, %7, %5, %5) ({
^bb0(%arg10: i32, %arg11: i32, %arg12: index, %arg13: index):
  %10 = "tts.create_ptr"(%arg1, %arg11) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
  %11 = "arith.index_cast"(%arg10) : (i32) -> index
  %12 = "arith.addi"(%arg12, %11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %13 = "arith.addi"(%arg11, %arg10) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %14 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
  %15 = "arith.addi"(%arg13, %11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  "scf.yield"(%14, %12, %15) : (!tt.ptr<f32>, index, index) -> ()
}) : (i32, i32, i32, !tt.ptr<f32>, index, index) -> (i32, index, index)
these are the uses:
actual processing
~~~~
processing val
%14 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"scf.yield"(%14, %12, %15) : (!tt.ptr<f32>, index, index) -> ()
actual processing
processing user
"scf.yield"(%14, %12, %15) : (!tt.ptr<f32>, index, index) -> ()
~~~~
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, {}, {}, {}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "reduce_kernel_2d_0d1d2de3de"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 5 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.index_cast"(%arg7) : (i32) -> index
    %6 = "arith.addi"(%0, %arg7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %7 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
    %8 = "arith.sitofp"(%arg7) : (i32) -> f32
    %9:3 = "scf.for"(%4, %3, %2, %7, %5, %5) ({
    ^bb0(%arg10: i32, %arg11: i32, %arg12: index, %arg13: index):
      %10 = "tts.create_ptr"(%arg1, %arg11) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "tt.store"(%arg11, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
      %11 = "arith.index_cast"(%arg10) : (i32) -> index
      %12 = "arith.addi"(%arg12, %11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %13 = "arith.addi"(%arg11, %arg10) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %14 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
      %15 = "arith.addi"(%arg13, %11) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%14, %12, %15) : (!tt.ptr<f32>, index, index) -> ()
    }) : (i32, i32, i32, !tt.ptr<f32>, index, index) -> (i32, index, index)
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%14 = "tt.addptr"(%arg11, %arg10) : (i32, i32) -> !tt.ptr<f32>
deleting
%7 = "tt.addptr"(%0, %arg7) : (i32, i32) -> !tt.ptr<f32>
module {
  func.func @reduce_kernel_2d_0d1d2de3de(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %c5_i32 = arith.constant 5 : i32
    %0 = arith.index_cast %arg7 : i32 to index
    %1 = arith.sitofp %arg7 : i32 to f32
    %2:3 = scf.for %arg16 = %c0_i32 to %c5_i32 step %c1_i32 iter_args(%arg17 = %arg7, %arg18 = %0, %arg19 = %0) -> (i32, index, index)  : i32 {
      %3 = "tts.create_ptr"(%arg1, %arg17) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      tt.store %3, %1 : !tt.ptr<f32>
      %4 = arith.index_cast %arg16 : i32 to index
      %5 = arith.addi %arg18, %4 : index
      %6 = arith.addi %arg17, %arg16 : i32
      %7 = arith.addi %arg19, %4 : index
      scf.yield %6, %5, %7 : i32, index, index
    }
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir:27:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_0_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index
              ^
<stdin>:2:440: note: scanning from here
 func.func @reduce_kernel_2d_0d1d2de3de(%arg0: memref<*xf32> {tt.divisibility = 16 : i32}, %arg1: memref<*xf32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) {
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
<stdin>:2:440: note: with "PARAM_7_" equal to "%arg13"
 func.func @reduce_kernel_2d_0d1d2de3de(%arg0: memref<*xf32> {tt.divisibility = 16 : i32}, %arg1: memref<*xf32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) {
                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
<stdin>:6:2: note: possible intended match here
 %0 = arith.index_cast %arg7 : i32 to index
 ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @reduce_kernel_2d_0d1d2de3de(%arg0: memref<*xf32> {tt.divisibility = 16 : i32}, %arg1: memref<*xf32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) { 
dag:27'0                                                                                                                                                                                                                                                                                                                                                                                                                                                            X error: no match found
dag:27'1                                                                                                                                                                                                                                                                                                                                                                                                                                                              with "PARAM_7_" equal to "%arg13"
          3:  %c0_i32 = arith.constant 0 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %c1_i32 = arith.constant 1 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          5:  %c5_i32 = arith.constant 5 : i32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          6:  %0 = arith.index_cast %arg7 : i32 to index 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:27'2      ?                                           possible intended match
          7:  %1 = arith.sitofp %arg7 : i32 to f32 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          8:  %2:3 = scf.for %arg16 = %c0_i32 to %c5_i32 step %c1_i32 iter_args(%arg17 = %arg7, %arg18 = %0, %arg19 = %0) -> (i32, index, index) : i32 { 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          9:  %3 = arith.index_cast %arg17 : i32 to index 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         10:  %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%3], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         11:  affine.store %1, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:27'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/use_dot_opc.mlir (37 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax.mlir (38 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_olt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_olt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_olt(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
    %c0_i32 = arith.constant 0 : i32
    %0 = arith.minimumf %arg1, %arg2 : f32
    %1 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %1, %0 : !tt.ptr<f32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ole", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 5 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ole", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 5 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_ole(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
    %c0_i32 = arith.constant 0 : i32
    %0 = arith.minimumf %arg1, %arg2 : f32
    %1 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %1, %0 : !tt.ptr<f32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ogt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ogt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_ogt(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
    %c0_i32 = arith.constant 0 : i32
    %0 = arith.maximumf %arg1, %arg2 : f32
    %1 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %1, %0 : !tt.ptr<f32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_oge", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 3 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_oge", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 3 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_oge(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
    %c0_i32 = arith.constant 0 : i32
    %0 = arith.maximumf %arg1, %arg2 : f32
    %1 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %1, %0 : !tt.ptr<f32>
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:46:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:2:143: note: scanning from here
 func.func @minmax_olt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:2:143: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_olt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:5:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:53:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:13:143: note: scanning from here
 func.func @minmax_ole(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:13:143: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_ole(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:16:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:60:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:24:143: note: scanning from here
 func.func @minmax_ogt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:24:143: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_ogt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:27:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:67:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:35:143: note: scanning from here
 func.func @minmax_oge(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:35:143: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_oge(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
                                                                                                                                              ^
<stdin>:38:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @minmax_olt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) { 
dag:46'0                                                                                                                                                   X error: no match found
dag:46'1                                                                                                                                                     with "PARAM_0_" equal to "%arg0"
          3:  %c0 = arith.constant 0 : index 
dag:46'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %0 = arith.minimumf %arg1, %arg2 : f32 
dag:46'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          5:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:46'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:46'2                  ?                                                                                                                                           possible intended match
          6:  affine.store %0, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:46'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          7:  return 
dag:46'0     ~~~~~~~~
          8:  } 
dag:46'0     ~~~
          9: } 
dag:46'0     ~~
         10:  
dag:46'0     ~
         11: // ----- 
dag:46'0     ~~~~~~~~~
         12: module { 
dag:46'0     ~~~~~~~~~
         13:  func.func @minmax_ole(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) { 
dag:46'0     ~~~~~~~~~~~~~~~~~~~~~~
dag:53'0                                                                                                                                                   X error: no match found
dag:53'1                                                                                                                                                     with "PARAM_0_" equal to "%arg0"
         14:  %c0 = arith.constant 0 : index 
dag:53'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         15:  %0 = arith.minimumf %arg1, %arg2 : f32 
dag:53'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         16:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:53'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:53'2                  ?                                                                                                                                           possible intended match
         17:  affine.store %0, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:53'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         18:  return 
dag:53'0     ~~~~~~~~
         19:  } 
dag:53'0     ~~~
         20: } 
dag:53'0     ~~
         21:  
dag:53'0     ~
         22: // ----- 
dag:53'0     ~~~~~~~~~
         23: module { 
dag:53'0     ~~~~~~~~~
         24:  func.func @minmax_ogt(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) { 
dag:53'0     ~~~~~~~~~~~~~~~~~~~~~~
dag:60'0                                                                                                                                                   X error: no match found
dag:60'1                                                                                                                                                     with "PARAM_0_" equal to "%arg0"
         25:  %c0 = arith.constant 0 : index 
dag:60'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         26:  %0 = arith.maximumf %arg1, %arg2 : f32 
dag:60'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         27:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:60'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:60'2                  ?                                                                                                                                           possible intended match
         28:  affine.store %0, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:60'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         29:  return 
dag:60'0     ~~~~~~~~
         30:  } 
dag:60'0     ~~~
         31: } 
dag:60'0     ~~
         32:  
dag:60'0     ~
         33: // ----- 
dag:60'0     ~~~~~~~~~
         34: module { 
dag:60'0     ~~~~~~~~~
         35:  func.func @minmax_oge(%arg0: memref<*xf32>, %arg1: f32, %arg2: f32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) { 
dag:60'0     ~~~~~~~~~~~~~~~~~~~~~~
dag:67'0                                                                                                                                                   X error: no match found
dag:67'1                                                                                                                                                     with "PARAM_0_" equal to "%arg0"
         36:  %c0 = arith.constant 0 : index 
dag:67'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         37:  %0 = arith.maximumf %arg1, %arg2 : f32 
dag:67'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         38:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:67'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:67'2                  ?                                                                                                                                           possible intended match
         39:  affine.store %0, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:67'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         40:  return 
dag:67'0     ~~~~~~~~
         41:  } 
dag:67'0     ~~~
         42: } 
dag:67'0     ~~
         43:  
dag:67'0     ~
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/kernel-01-vector-add.mlir (39 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_512_256_f32_axis1.mlir (40 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_dim1.mlir (41 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_dim1.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant dense<0.000000e+00> : tensor<4x256xbf16>
    %c256_i32 = arith.constant 256 : i32
    %c3 = arith.constant 3 : index
    %c12 = arith.constant 12 : index
    %c0 = arith.constant 0 : index
    %0 = tts.make_tptr %arg0 to sizes: [1, 256], strides: [0, 1], offsets: [0, 0], shape: [0, 0], order: [] : <bf16> to tensor<1x256x!tt.ptr<bf16>>
    %1 = "tts.load"(%0) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>) -> tensor<1x256xbf16>
    %2 = arith.index_cast %arg1 : i32 to index
    %3 = tts.make_tptr %arg0 to sizes: [1, 256], strides: [0, 1], offsets: [%2, 0], shape: [0, 0], order: [] : <bf16> to tensor<1x256x!tt.ptr<bf16>>
    "tts.store"(%3, %1) <{static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>, tensor<1x256xbf16>) -> ()
    %4:2 = scf.for %arg2 = %c0 to %c12 step %c3 iter_args(%arg3 = %cst, %arg4 = %c0) -> (tensor<4x256xbf16>, index) {
      %6 = arith.index_cast %arg2 : index to i32
      %7 = arith.muli %6, %c256_i32 : i32
      %8 = arith.index_cast %7 : i32 to index
      %9 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [%8, %c1], offsets: [%arg4, %c0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
      %10 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
      %11 = arith.addf %arg3, %10 : tensor<4x256xbf16>
      %12 = arith.addi %arg4, %c256 : index
      scf.yield %11, %12 : tensor<4x256xbf16>, index
    }
    %5 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [%c256, 1], offsets: [0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%5, %4#0) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant dense<0.000000e+00> : tensor<4x256xbf16>
    %c256_i32 = arith.constant 256 : i32
    %c3 = arith.constant 3 : index
    %c12 = arith.constant 12 : index
    %c0 = arith.constant 0 : index
    %0 = tts.make_tptr %arg0 to sizes: [1, 256], strides: [0, 1], offsets: [0, 0], shape: [0, 0], order: [] : <bf16> to tensor<1x256x!tt.ptr<bf16>>
    %1 = "tts.load"(%0) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>) -> tensor<1x256xbf16>
    %2 = arith.index_cast %arg1 : i32 to index
    %3 = tts.make_tptr %arg0 to sizes: [1, 256], strides: [0, 1], offsets: [%2, 0], shape: [0, 0], order: [] : <bf16> to tensor<1x256x!tt.ptr<bf16>>
    "tts.store"(%3, %1) <{static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>, tensor<1x256xbf16>) -> ()
    %4:2 = scf.for %arg2 = %c0 to %c12 step %c3 iter_args(%arg3 = %cst, %arg4 = %c0) -> (tensor<4x256xbf16>, index) {
      %6 = arith.index_cast %arg2 : index to i32
      %7 = arith.muli %6, %c256_i32 : i32
      %8 = arith.index_cast %7 : i32 to index
      %9 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [%8, %c1], offsets: [%arg4, %c0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
      %10 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
      %11 = arith.addf %arg3, %10 : tensor<4x256xbf16>
      %12 = arith.addi %arg4, %c256 : index
      scf.yield %11, %12 : tensor<4x256xbf16>, index
    }
    %5 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [%c256, 1], offsets: [0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%5, %4#0) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32):
    %0 = "arith.constant"() <{value = 256 : index}> : () -> index
    %1 = "arith.constant"() <{value = 1 : index}> : () -> index
    %2 = "arith.constant"() <{value = 0.000000e+00 : bf16}> : () -> bf16
    %3 = "tensor.empty"() : () -> tensor<4x256xbf16>
    %4 = "linalg.fill"(%2, %3) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg14: bf16, %arg15: bf16):
      "linalg.yield"(%arg14) : (bf16) -> ()
    }) : (bf16, tensor<4x256xbf16>) -> tensor<4x256xbf16>
    %5 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 3 : index}> : () -> index
    %7 = "arith.constant"() <{value = 12 : index}> : () -> index
    %8 = "arith.constant"() <{value = 0 : index}> : () -> index
    %9 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1, 256>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 0, 1>}> : (!tt.ptr<bf16>) -> tensor<1x256x!tt.ptr<bf16>>
    %10 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>) -> tensor<1x256xbf16>
    %11 = "arith.index_cast"(%arg1) : (i32) -> index
    %12 = "tts.make_tptr"(%arg0, %11) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1, 256>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 0, 1>}> : (!tt.ptr<bf16>, index) -> tensor<1x256x!tt.ptr<bf16>>
    "tts.store"(%12, %10) <{static_mask_dims = array<i64>}> : (tensor<1x256x!tt.ptr<bf16>>, tensor<1x256xbf16>) -> ()
    %13:2 = "scf.for"(%8, %7, %6, %4, %8) ({
    ^bb0(%arg8: index, %arg9: tensor<4x256xbf16>, %arg10: index):
      %16 = "arith.index_cast"(%arg8) : (index) -> i32
      %17 = "arith.muli"(%16, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %18 = "arith.index_cast"(%17) : (i32) -> index
      %19 = "tts.make_tptr"(%arg0, %18, %1, %arg10, %8) <{operandSegmentSizes = array<i32: 1, 2, 2, 0>, order = array<i32>, sizes = array<i64: 4, 256>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index, index, index) -> tensor<4x256x!tt.ptr<bf16>>
      %20 = "tts.load"(%19) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
      %21 = "linalg.generic"(%arg9, %20, %arg9) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg11: bf16, %arg12: bf16, %arg13: bf16):
        %23 = "arith.addf"(%arg11, %arg12) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
        "linalg.yield"(%23) : (bf16) -> ()
      }) : (tensor<4x256xbf16>, tensor<4x256xbf16>, tensor<4x256xbf16>) -> tensor<4x256xbf16>
      %22 = "arith.addi"(%arg10, %0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      "scf.yield"(%21, %22) : (tensor<4x256xbf16>, index) -> ()
    }) : (index, index, index, tensor<4x256xbf16>, index) -> (tensor<4x256xbf16>, index)
    %14 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %15 = "tts.make_tptr"(%14, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 4, 256>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: -9223372036854775808, 1>}> : (memref<*xbf16>, index) -> tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%15, %13#0) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir:1 offset :13:10: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<bf16>'
    %2 = tt.addptr %splat_arg0, %1 : tensor<1x256x!tt.ptr<bf16>>, tensor<1x256xi32>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir:1 offset :13:10: note: see current operation: %9 = "memref.reinterpret_cast"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0>, static_sizes = array<i64: 1, 256>, static_strides = array<i64: 256, 1>}> : (!tt.ptr<bf16>) -> memref<1x256xbf16, strided<[256, 1]>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_dim1.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_reduce.mlir (42 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_reduce.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_sgt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 4 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_sgt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 4 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_sgt(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4096xi32>
    %1 = linalg.fill ins(%c0_i32_0 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32>
    %c-2147483648_i32 = arith.constant -2147483648 : i32
    %2 = bufferization.alloc_tensor() : tensor<i32>
    %inserted = tensor.insert %c-2147483648_i32 into %2[] : tensor<i32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xi32>) outs(%inserted : tensor<i32>) dimensions = [0] 
      (%in: i32, %init: i32) {
        %4 = arith.maxsi %in, %init : i32
        linalg.yield %4 : i32
      }
    %extracted = tensor.extract %reduced[] : tensor<i32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    tt.store %3, %extracted : !tt.ptr<i32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ugt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 8 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ugt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 8 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_ugt(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4096xi32>
    %1 = linalg.fill ins(%c0_i32_0 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32>
    %c0_i32_1 = arith.constant 0 : i32
    %2 = bufferization.alloc_tensor() : tensor<i32>
    %inserted = tensor.insert %c0_i32_1 into %2[] : tensor<i32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xi32>) outs(%inserted : tensor<i32>) dimensions = [0] 
      (%in: i32, %init: i32) {
        %4 = arith.maxui %in, %init : i32
        linalg.yield %4 : i32
      }
    %extracted = tensor.extract %reduced[] : tensor<i32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    tt.store %3, %extracted : !tt.ptr<i32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_slt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_slt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_slt(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4096xi32>
    %1 = linalg.fill ins(%c0_i32_0 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32>
    %c2147483647_i32 = arith.constant 2147483647 : i32
    %2 = bufferization.alloc_tensor() : tensor<i32>
    %inserted = tensor.insert %c2147483647_i32 into %2[] : tensor<i32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xi32>) outs(%inserted : tensor<i32>) dimensions = [0] 
      (%in: i32, %init: i32) {
        %4 = arith.minsi %in, %init : i32
        linalg.yield %4 : i32
      }
    %extracted = tensor.extract %reduced[] : tensor<i32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    tt.store %3, %extracted : !tt.ptr<i32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ult", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 6 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ult", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 6 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minmax_ult(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4096xi32>
    %1 = linalg.fill ins(%c0_i32_0 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32>
    %c-1_i32 = arith.constant -1 : i32
    %2 = bufferization.alloc_tensor() : tensor<i32>
    %inserted = tensor.insert %c-1_i32 into %2[] : tensor<i32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xi32>) outs(%inserted : tensor<i32>) dimensions = [0] 
      (%in: i32, %init: i32) {
        %4 = arith.minui %in, %init : i32
        linalg.yield %4 : i32
      }
    %extracted = tensor.extract %reduced[] : tensor<i32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    tt.store %3, %extracted : !tt.ptr<i32>
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:20:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
              ^
<stdin>:2:119: note: scanning from here
 func.func @minmax_sgt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:2:119: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_sgt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:16:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:57:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
              ^
<stdin>:24:119: note: scanning from here
 func.func @minmax_ugt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:24:119: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_ugt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:37:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:94:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
              ^
<stdin>:45:119: note: scanning from here
 func.func @minmax_slt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:45:119: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_slt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:59:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:132:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
              ^
<stdin>:67:119: note: scanning from here
 func.func @minmax_ult(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:67:119: note: with "PARAM_0_" equal to "%arg0"
 func.func @minmax_ult(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                      ^
<stdin>:81:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
             ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
           1: module { 
           2:  func.func @minmax_sgt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:20'0                                                                                                                            X error: no match found
dag:20'1                                                                                                                              with "PARAM_0_" equal to "%arg0"
           3:  %c0 = arith.constant 0 : index 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           4:  %c-2147483648_i32 = arith.constant -2147483648 : i32 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           5:  %c0_i32 = arith.constant 0 : i32 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           6:  %0 = tensor.empty() : tensor<4096xi32> 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           7:  %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32> 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           .
           .
           .
          11:  (%in: i32, %init: i32) { 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~
          12:  %3 = arith.maxsi %in, %init : i32 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          13:  linalg.yield %3 : i32 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~
          14:  } 
dag:20'0      ~~~
          15:  %extracted = tensor.extract %reduced[] : tensor<i32> 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          16:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>> 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:20'2                   ?                                                                                                                                           possible intended match
          17:  affine.store %extracted, %reinterpret_cast[0] : memref<1xi32, strided<[1], offset: ?>> 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          18:  return 
dag:20'0      ~~~~~~~~
          19:  } 
dag:20'0      ~~~
          20: } 
dag:20'0      ~~
          21:  
dag:20'0      ~
          22: // ----- 
dag:20'0      ~~~~~~~~~
          23: module { 
dag:20'0      ~~~~~~~~~
          24:  func.func @minmax_ugt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:20'0      ~~~~~~~~~~~~~~~~~~~~~~
dag:57'0                                                                                                                            X error: no match found
dag:57'1                                                                                                                              with "PARAM_0_" equal to "%arg0"
          25:  %c0 = arith.constant 0 : index 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          26:  %c0_i32 = arith.constant 0 : i32 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          27:  %0 = tensor.empty() : tensor<4096xi32> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          28:  %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          29:  %2 = bufferization.alloc_tensor() : tensor<i32> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           .
           .
           .
          32:  (%in: i32, %init: i32) { 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~
          33:  %3 = arith.maxui %in, %init : i32 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          34:  linalg.yield %3 : i32 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~
          35:  } 
dag:57'0      ~~~
          36:  %extracted = tensor.extract %reduced[] : tensor<i32> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          37:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:57'2                   ?                                                                                                                                           possible intended match
          38:  affine.store %extracted, %reinterpret_cast[0] : memref<1xi32, strided<[1], offset: ?>> 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          39:  return 
dag:57'0      ~~~~~~~~
          40:  } 
dag:57'0      ~~~
          41: } 
dag:57'0      ~~
          42:  
dag:57'0      ~
          43: // ----- 
dag:57'0      ~~~~~~~~~
          44: module { 
dag:57'0      ~~~~~~~~~
          45:  func.func @minmax_slt(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:57'0      ~~~~~~~~~~~~~~~~~~~~~~
dag:94'0                                                                                                                            X error: no match found
dag:94'1                                                                                                                              with "PARAM_0_" equal to "%arg0"
          46:  %c0 = arith.constant 0 : index 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          47:  %c2147483647_i32 = arith.constant 2147483647 : i32 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          48:  %c0_i32 = arith.constant 0 : i32 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          49:  %0 = tensor.empty() : tensor<4096xi32> 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          50:  %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32> 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           .
           .
           .
          54:  (%in: i32, %init: i32) { 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~
          55:  %3 = arith.minsi %in, %init : i32 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          56:  linalg.yield %3 : i32 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~
          57:  } 
dag:94'0      ~~~
          58:  %extracted = tensor.extract %reduced[] : tensor<i32> 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          59:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>> 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:94'2                   ?                                                                                                                                           possible intended match
          60:  affine.store %extracted, %reinterpret_cast[0] : memref<1xi32, strided<[1], offset: ?>> 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          61:  return 
dag:94'0      ~~~~~~~~
          62:  } 
dag:94'0      ~~~
          63: } 
dag:94'0      ~~
          64:  
dag:94'0      ~
          65: // ----- 
dag:94'0      ~~~~~~~~~
          66: module { 
dag:94'0      ~~~~~~~~~
          67:  func.func @minmax_ult(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:94'0      ~~~~~~~~~~~~~~~~~~~~~~
dag:132'0                                                                                                                           X error: no match found
dag:132'1                                                                                                                             with "PARAM_0_" equal to "%arg0"
          68:  %c0 = arith.constant 0 : index 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          69:  %c-1_i32 = arith.constant -1 : i32 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          70:  %c0_i32 = arith.constant 0 : i32 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          71:  %0 = tensor.empty() : tensor<4096xi32> 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          72:  %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32> 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           .
           .
           .
          76:  (%in: i32, %init: i32) { 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
          77:  %3 = arith.minui %in, %init : i32 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          78:  linalg.yield %3 : i32 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~
          79:  } 
dag:132'0     ~~~
          80:  %extracted = tensor.extract %reduced[] : tensor<i32> 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          81:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>> 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:132'2                  ?                                                                                                                                           possible intended match
          82:  affine.store %extracted, %reinterpret_cast[0] : memref<1xi32, strided<[1], offset: ?>> 
dag:132'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          83:  return 
dag:132'0     ~~~~~~~~
          84:  } 
dag:132'0     ~~~
          85: } 
dag:132'0     ~~
          86:  
dag:132'0     ~
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_nested.mlir (43 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_nested.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c15 = arith.constant 15 : index
    %c10 = arith.constant 10 : index
    %c5 = arith.constant 5 : index
    %0 = arith.index_cast %arg1 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3 = arith.addi %0, %0 : index
    %4 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [2, %c10], offsets: [%3, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir
    %6 = arith.addf %2, %5 : tensor<4x256xbf16>
    %7 = arith.addi %3, %0 : index
    %8 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [3, %c15], offsets: [%7, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%8, %6) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c15 = arith.constant 15 : index
    %c10 = arith.constant 10 : index
    %c5 = arith.constant 5 : index
    %0 = arith.index_cast %arg1 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3 = arith.addi %0, %0 : index
    %4 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [2, %c10], offsets: [%3, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %6 = arith.addf %2, %5 : tensor<4x256xbf16>
    %7 = arith.addi %3, %0 : index
    %8 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [3, %c15], offsets: [%7, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%8, %6) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, i32, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32):
    %0 = "arith.constant"() <{value = 15 : index}> : () -> index
    %1 = "arith.constant"() <{value = 10 : index}> : () -> index
    %2 = "arith.constant"() <{value = 5 : index}> : () -> index
    %3 = "arith.index_cast"(%arg1) : (i32) -> index
    %4 = "tts.make_tptr"(%arg0, %2, %3) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4, 256>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index) -> tensor<4x256x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %6 = "arith.addi"(%3, %3) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %7 = "tts.make_tptr"(%arg0, %1, %6) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4, 256>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 2, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index) -> tensor<4x256x!tt.ptr<bf16>>
    %8 = "tts.load"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %9 = "linalg.generic"(%5, %8, %5) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg8: bf16, %arg9: bf16, %arg10: bf16):
      %13 = "arith.addf"(%arg8, %arg9) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "linalg.yield"(%13) : (bf16) -> ()
    }) : (tensor<4x256xbf16>, tensor<4x256xbf16>, tensor<4x256xbf16>) -> tensor<4x256xbf16>
    %10 = "arith.addi"(%6, %3) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %11 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %12 = "tts.make_tptr"(%11, %0, %10) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, order = array<i32>, sizes = array<i64: 4, 256>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 3, -9223372036854775808>}> : (memref<*xbf16>, index, index) -> tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%12, %9) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir:1 offset :30:10: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<bf16>'
    %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir:1 offset :30:10: note: see current operation: %4 = "memref.reinterpret_cast"(%arg0, %3, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 1>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 4, 256>, static_strides = array<i64: 1, -9223372036854775808>}> : (!tt.ptr<bf16>, index, index) -> memref<4x256xbf16, strided<[1, ?], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_nested.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_broadcast.mlir (44 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/dot.mlir (45 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_splat_2d.mlir (46 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/bitcast.mlir (47 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir (48 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %c, %6 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: note: see current operation: tt.store %arg2, %14 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %c, %6 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: note: see current operation: tt.store %arg2, %14 : tensor<1024x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %4 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %5 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %7 = "arith.addf"(%5, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %8 = "arith.subf"(%7, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %9 = "arith.mulf"(%8, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %10 = "arith.divf"(%9, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %11 = "arith.cmpf"(%10, %6) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>
    %12 = "arith.select"(%11, %5, %6) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    "tt.store"(%0, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%0, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %4 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %5 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %7 = "arith.addf"(%5, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %8 = "arith.subf"(%7, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %9 = "arith.mulf"(%8, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %10 = "arith.divf"(%9, %6) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %11 = "arith.cmpf"(%10, %6) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>
    %12 = "arith.select"(%11, %5, %6) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %13 = "tts.create_ptr"(%arg2, %0) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%0, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<1024x!tt.ptr<f32>>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<1024xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg28: i32, %arg29: i32):
      "linalg.yield"(%arg28) : (i32) -> ()
    }) : (i32, tensor<1024xi32>) -> tensor<1024xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>) -> tensor<1024x!tt.ptr<f32>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>) -> tensor<1024x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %8 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %9 = "linalg.generic"(%7, %8, %7) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg25: f32, %arg26: f32, %arg27: f32):
      %22 = "arith.addf"(%arg25, %arg26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%22) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %10 = "linalg.generic"(%9, %8, %9) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg22: f32, %arg23: f32, %arg24: f32):
      %21 = "arith.subf"(%arg22, %arg23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%21) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %11 = "linalg.generic"(%10, %8, %10) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg19: f32, %arg20: f32, %arg21: f32):
      %20 = "arith.mulf"(%arg19, %arg20) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%20) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %12 = "linalg.generic"(%11, %8, %11) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg16: f32, %arg17: f32, %arg18: f32):
      %19 = "arith.divf"(%arg16, %arg17) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%19) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %13 = "tensor.empty"() : () -> tensor<1024xi1>
    %14 = "linalg.generic"(%12, %8, %13) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg13: f32, %arg14: f32, %arg15: i1):
      %18 = "arith.cmpf"(%arg13, %arg14) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
      "linalg.yield"(%18) : (i1) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>, tensor<1024xi1>) -> tensor<1024xi1>
    %15 = "linalg.generic"(%14, %7, %8, %7) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 3, 1>}> ({
    ^bb0(%arg9: i1, %arg10: f32, %arg11: f32, %arg12: f32):
      %17 = "arith.select"(%arg9, %arg10, %arg11) : (i1, f32, f32) -> f32
      "linalg.yield"(%17) : (f32) -> ()
    }) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %16 = "tts.create_ptr"(%arg2, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%16, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
        tt.store %c, %6 : tensor<1024x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir:1 offset :25:9: note: see current operation: %18 = "memref.cast"(%arg2) : (tensor<1024x!tt.ptr<f32>>) -> memref<?xf32>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir (49 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save0, %5 : tensor<1024x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: note: see current operation: tt.store %arg3, %13 : tensor<1024x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save0, %5 : tensor<1024x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: note: see current operation: tt.store %arg3, %13 : tensor<1024x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :33:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save1, %6 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :33:5: note: see current operation: tt.store %arg4, %14 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :33:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save1, %6 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :33:5: note: see current operation: tt.store %arg4, %14 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :34:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save2, %7 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :34:5: note: see current operation: tt.store %arg5, %15 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :34:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save2, %7 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :34:5: note: see current operation: tt.store %arg5, %15 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :35:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save3, %10 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :35:5: note: see current operation: tt.store %arg6, %16 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :35:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save3, %10 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :35:5: note: see current operation: tt.store %arg6, %16 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :36:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save4, %11 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :36:5: note: see current operation: tt.store %arg7, %17 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :36:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save4, %11 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :36:5: note: see current operation: tt.store %arg7, %17 : tensor<1024x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<1024x!tt.ptr<bf16>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<1024x!tt.ptr<bf16>>, %arg4: tensor<1024x!tt.ptr<f32>>, %arg5: tensor<1024x!tt.ptr<f32>>, %arg6: tensor<1024x!tt.ptr<f32>>, %arg7: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f16>) -> tensor<1024x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i32>>) -> tensor<1024xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f16>>) -> tensor<1024xf16>
    %14 = "arith.truncf"(%11) : (tensor<1024xf32>) -> tensor<1024xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
    %16 = "arith.sitofp"(%12) : (tensor<1024xi32>) -> tensor<1024xf32>
    %17 = "arith.extf"(%13) : (tensor<1024xf16>) -> tensor<1024xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xbf16>) -> ()
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xbf16>) -> ()
actual processing
processing user
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xbf16>) -> ()
tensor<1024x!tt.ptr<bf16>>
~~~~
processing val
%3 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
processing val
%2 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<1024x!tt.ptr<bf16>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<1024x!tt.ptr<bf16>>, %arg4: tensor<1024x!tt.ptr<f32>>, %arg5: tensor<1024x!tt.ptr<f32>>, %arg6: tensor<1024x!tt.ptr<f32>>, %arg7: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f16>) -> tensor<1024x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i32>>) -> tensor<1024xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f16>>) -> tensor<1024xf16>
    %14 = "arith.truncf"(%11) : (tensor<1024xf32>) -> tensor<1024xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
    %16 = "arith.sitofp"(%12) : (tensor<1024xi32>) -> tensor<1024xf32>
    %17 = "arith.extf"(%13) : (tensor<1024xf16>) -> tensor<1024xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
    %19 = "tts.create_ptr"(%arg3, %4) : (tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<bf16>>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xbf16>) -> ()
    %20 = "tts.create_ptr"(%arg4, %3) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    %21 = "tts.create_ptr"(%arg5, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    %22 = "tts.create_ptr"(%arg6, %1) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    %23 = "tts.create_ptr"(%arg7, %0) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<1024x!tt.ptr<bf16>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, tensor<1024x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<1024x!tt.ptr<bf16>>, %arg4: tensor<1024x!tt.ptr<f32>>, %arg5: tensor<1024x!tt.ptr<f32>>, %arg6: tensor<1024x!tt.ptr<f32>>, %arg7: tensor<1024x!tt.ptr<f32>>, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<1024xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg24: i32, %arg25: i32):
      "linalg.yield"(%arg24) : (i32) -> ()
    }) : (i32, tensor<1024xi32>) -> tensor<1024xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<f32>) -> memref<*xf32>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>) -> tensor<1024x!tt.ptr<f32>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<i32>) -> memref<*xi32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xi32>) -> tensor<1024x!tt.ptr<i32>>
    %7 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<f16>) -> memref<*xf16>
    %8 = "tts.make_tptr"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf16>) -> tensor<1024x!tt.ptr<f16>>
    %9 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %10 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i32>>) -> tensor<1024xi32>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f16>>) -> tensor<1024xf16>
    %12 = "tensor.empty"() : () -> tensor<1024xbf16>
    %13 = "linalg.generic"(%9, %12) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg22: f32, %arg23: bf16):
      %29 = "arith.truncf"(%arg22) : (f32) -> bf16
      "linalg.yield"(%29) : (bf16) -> ()
    }) : (tensor<1024xf32>, tensor<1024xbf16>) -> tensor<1024xbf16>
    %14 = "linalg.generic"(%9, %9) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg20: f32, %arg21: f32):
      %28 = "math.exp"(%arg20) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "linalg.yield"(%28) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %15 = "tensor.empty"() : () -> tensor<1024xf32>
    %16 = "linalg.generic"(%10, %15) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg18: i32, %arg19: f32):
      %27 = "arith.sitofp"(%arg18) : (i32) -> f32
      "linalg.yield"(%27) : (f32) -> ()
    }) : (tensor<1024xi32>, tensor<1024xf32>) -> tensor<1024xf32>
    %17 = "tensor.empty"() : () -> tensor<1024xf32>
    %18 = "linalg.generic"(%11, %17) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg16: f16, %arg17: f32):
      %26 = "arith.extf"(%arg16) : (f16) -> f32
      "linalg.yield"(%26) : (f32) -> ()
    }) : (tensor<1024xf16>, tensor<1024xf32>) -> tensor<1024xf32>
    %19 = "linalg.generic"(%9, %9) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg14: f32, %arg15: f32):
      %25 = "math.sqrt"(%arg14) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "linalg.yield"(%25) : (f32) -> ()
    }) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %20 = "tts.create_ptr"(%arg3, %2) : (tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<bf16>>
    "tt.store"(%20, %13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<bf16>>, tensor<1024xbf16>) -> ()
    %21 = "tts.create_ptr"(%arg4, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%21, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    %22 = "tts.create_ptr"(%arg5, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%22, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    %23 = "tts.create_ptr"(%arg6, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%23, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    %24 = "tts.create_ptr"(%arg7, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%24, %19) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<bf16>>'
    tt.store %save0, %5 : tensor<1024x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir:1 offset :32:5: note: see current operation: %23 = "memref.cast"(%arg3) : (tensor<1024x!tt.ptr<bf16>>) -> memref<?xbf16>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/masked_ldst_sitofp_other.mlir (50 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_mul_const_const.mlir (51 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_512_256_bf16_axis0.mlir (52 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_loopback.mlir (53 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/arith_not_ptr_arith.mlir (54 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_used_before_update.mlir (55 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_512_256_f32_axis0.mlir (56 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir (57 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %d, %10 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: note: see current operation: tt.store %arg3, %13 : tensor<1024x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %d, %10 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: note: see current operation: tt.store %arg3, %13 : tensor<1024x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<i1>) -> tensor<1024x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i1>>) -> tensor<1024xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
these are the uses:
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
tensor<1024x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<1024x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<i1>) -> tensor<1024x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i1>>) -> tensor<1024xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %11 = "tts.create_ptr"(%arg3, %0) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024xi32>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<1024x!tt.ptr<f32>>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<1024x!tt.ptr<f32>>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "tensor.empty"() : () -> tensor<1024xi32>
    %2 = "linalg.fill"(%0, %1) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg14: i32, %arg15: i32):
      "linalg.yield"(%arg14) : (i32) -> ()
    }) : (i32, tensor<1024xi32>) -> tensor<1024xi32>
    %3 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<i1>) -> memref<*xi1>
    %4 = "tts.make_tptr"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xi1>) -> tensor<1024x!tt.ptr<i1>>
    %5 = "builtin.unrealized_conversion_cast"(%arg1) : (!tt.ptr<f32>) -> memref<*xf32>
    %6 = "tts.make_tptr"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>) -> tensor<1024x!tt.ptr<f32>>
    %7 = "builtin.unrealized_conversion_cast"(%arg2) : (!tt.ptr<f32>) -> memref<*xf32>
    %8 = "tts.make_tptr"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xf32>) -> tensor<1024x!tt.ptr<f32>>
    %9 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<i1>>) -> tensor<1024xi1>
    %10 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %12 = "linalg.generic"(%9, %10, %11, %10) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 3, 1>}> ({
    ^bb0(%arg10: i1, %arg11: f32, %arg12: f32, %arg13: f32):
      %14 = "arith.select"(%arg10, %arg11, %arg12) : (i1, f32, f32) -> f32
      "linalg.yield"(%14) : (f32) -> ()
    }) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
    %13 = "tts.create_ptr"(%arg3, %2) : (tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<f32>>
    "tt.store"(%13, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: error: 'memref.cast' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
    tt.store %d, %10 : tensor<1024x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir:1 offset :24:5: note: see current operation: %16 = "memref.cast"(%arg3) : (tensor<1024x!tt.ptr<f32>>) -> memref<?xf32>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_expand_ptr.mlir (58 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/cumsum.mlir (59 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_more_init_args.mlir (60 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_loopback.mlir (61 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_loopback.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :11:11: remark: PtrAnalysis: scalar loadOp will not be rewritten
    %10 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: !tt.ptr<bf16>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :11:11: note: see current operation: %4 = tt.load %1 : !tt.ptr<bf16>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :11:11: remark: PtrAnalysis: Failed to rewrite LoadOp
    %10 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: !tt.ptr<bf16>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :11:11: note: see current operation: %4 = tt.load %1 : !tt.ptr<bf16>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :12:5: remark: PtrAnalysis: scalar storeOp will not be rewritten
    tt.store %1, %10 : !tt.ptr<bf16>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :12:5: note: see current operation: tt.store %3, %4 : !tt.ptr<bf16>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %1, %10 : !tt.ptr<bf16>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:1 offset :12:5: note: see current operation: tt.store %3, %4 : !tt.ptr<bf16>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
    %3 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
    %4 = "tt.load"(%2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<bf16>) -> bf16
    "tt.store"(%3, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<bf16>, bf16) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%2 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
actual processing
processing user
%2 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%4 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
actual processing
processing user
%4 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
~~~~
processing val
%3 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
these are the uses:
%6 = "tt.load"(%3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<bf16>) -> bf16
actual processing
processing user
%6 = "tt.load"(%3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<bf16>) -> bf16
!tt.ptr<bf16>
~~~~
processing val
%5 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
these are the uses:
"tt.store"(%5, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<bf16>, bf16) -> ()
actual processing
processing user
"tt.store"(%5, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<bf16>, bf16) -> ()
!tt.ptr<bf16>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.addi"(%1, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %3 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
    %4 = "arith.addi"(%0, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %5 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
    %6 = "tts.create_ptr"(%arg0, %2) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    %7 = "tt.load"(%3) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<bf16>) -> bf16
    %8 = "tts.create_ptr"(%arg1, %4) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    "tt.store"(%5, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<bf16>, bf16) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%3 = "tt.addptr"(%1, %arg2) : (i32, i32) -> !tt.ptr<bf16>
deleting
%5 = "tt.addptr"(%0, %arg2) : (i32, i32) -> !tt.ptr<bf16>
module {
  func.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
    %0 = "tts.create_ptr"(%arg0, %arg2) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    %1 = tt.load %0 : !tt.ptr<bf16>
    %2 = "tts.create_ptr"(%arg1, %arg2) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    tt.store %2, %1 : !tt.ptr<bf16>
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir:22:11: error: CHECK: expected string not found in input
// CHECK: [[LOAD_VAR_reinterpret_cast_MEM_:%.+]] = affine.load [[VAR_reinterpret_cast_]][0] : memref<1xbf16, strided<[1], offset: ?>>
          ^
<stdin>:6:155: note: scanning from here
 %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>>
                                                                                                                                                          ^
<stdin>:6:155: note: with "VAR_reinterpret_cast_" equal to "%reinterpret_cast"
 %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>>
                                                                                                                                                          ^
<stdin>:7:19: note: possible intended match here
 affine.store %1, %reinterpret_cast_0[0] : memref<1xbf16, strided<[1], offset: ?>>
                  ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_loopback.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
            1: module { 
            2:  func.func @kernel(%arg0: memref<*xbf16>, %arg1: memref<*xbf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) { 
            3:  %0 = arith.index_cast %arg2 : i32 to index 
            4:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>> 
            5:  %1 = affine.load %reinterpret_cast[0] : memref<1xbf16, strided<[1], offset: ?>> 
            6:  %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>> 
check:22'0                                                                                                                                                               X error: no match found
check:22'1                                                                                                                                                                 with "VAR_reinterpret_cast_" equal to "%reinterpret_cast"
            7:  affine.store %1, %reinterpret_cast_0[0] : memref<1xbf16, strided<[1], offset: ?>> 
check:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
check:22'2                       ?                                                                 possible intended match
            8:  return 
check:22'0     ~~~~~~~~
            9:  } 
check:22'0     ~~~
           10: } 
check:22'0     ~~
           11:  
check:22'0     ~
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_used_after_update.mlir (62 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_nested_loop.mlir (63 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_nested_loop.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir
module {
  func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
    %c2_i32 = arith.constant 2 : i32
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = tt.addptr %arg0, %arg4 : !tt.ptr<f32>, i32
    %1 = scf.for %arg13 = %c0_i32 to %c8_i32 step %c1_i32 iter_args(%arg14 = %0) -> (!tt.ptr<f32>)  : i32 {
      %2 = scf.for %arg15 = %c0_i32 to %c2_i32 step %c1_i32 iter_args(%arg16 = %arg14) -> (!tt.ptr<f32>)  : i32 {
        %3 = arith.muli %arg13, %arg15 : i32
        %4 = arith.sitofp %3 : i32 to f32
        tt.store %arg16, %4 : !tt.ptr<f32>
        %5 = tt.addptr %arg16, %c1_i32 : !tt.ptr<f32>, i32
        scf.yield %5 : !tt.ptr<f32>
      }
      scf.yield %2 : !tt.ptr<f32>
    }
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir:25:16: error: CHECK-SAME: expected string not found in input
// CHECK-SAME: ([[PARAM_0_:%.+]]: memref<*xf32> {tt.divisibility = 16 : i32}, [[PARAM_1_:%.+]]: i32, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32) {
               ^
<stdin>:2:32: note: scanning from here
 func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
                               ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_nested_loop.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
         1: module { 
         2:  func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) { 
same:25                                    X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found
         3:  %c2_i32 = arith.constant 2 : i32 
same:25     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         4:  %c8_i32 = arith.constant 8 : i32 
same:25     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         5:  %c0_i32 = arith.constant 0 : i32 
same:25     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         6:  %c1_i32 = arith.constant 1 : i32 
same:25     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         7:  %0 = tt.addptr %arg0, %arg4 : !tt.ptr<f32>, i32 
same:25     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         .
         .
         .
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_mul_value_const.mlir (64 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_512_256_bf16_axis1.mlir (65 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_scalar.mlir (66 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_scalar.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir:1 offset :14:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %res, %3 : !tt.ptr<bf16>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir:1 offset :14:5: note: see current operation: tt.store %arg1, %5 : !tt.ptr<bf16>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir:1 offset :14:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %res, %3 : !tt.ptr<bf16>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir:1 offset :14:5: note: see current operation: tt.store %arg1, %5 : !tt.ptr<bf16>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>
    %3 = "tts.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x!tt.ptr<bf16>>) -> tensor<128xbf16>
    %4 = "tt.reduce"(%3) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %5 = "arith.addf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%5) : (bf16) -> ()
    }) : (tensor<128xbf16>) -> bf16
    "tt.store"(%0, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, bf16) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, bf16) -> ()
actual processing
processing user
"tt.store"(%0, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, bf16) -> ()
!tt.ptr<bf16>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>
    %3 = "tts.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x!tt.ptr<bf16>>) -> tensor<128xbf16>
    %4 = "tt.reduce"(%3) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %6 = "arith.addf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%6) : (bf16) -> ()
    }) : (tensor<128xbf16>) -> bf16
    %5 = "tts.create_ptr"(%arg1, %0) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    "tt.store"(%0, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, bf16) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
"builtin.module"() ({
  "func.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "builtin.unrealized_conversion_cast"(%arg0) : (!tt.ptr<bf16>) -> memref<*xbf16>
    %2 = "tts.make_tptr"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (memref<*xbf16>) -> tensor<128x!tt.ptr<bf16>>
    %3 = "tts.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x!tt.ptr<bf16>>) -> tensor<128xbf16>
    %4 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %5 = "bufferization.alloc_tensor"() <{operandSegmentSizes = array<i32: 0, 0, 0>}> : () -> tensor<f32>
    %6 = "tensor.insert"(%4, %5) : (f32, tensor<f32>) -> tensor<f32>
    %7 = "linalg.reduce"(%3, %6) <{dimensions = array<i64: 0>}> ({
    ^bb0(%arg8: bf16, %arg9: f32):
      %11 = "arith.extf"(%arg8) : (bf16) -> f32
      %12 = "arith.addf"(%11, %arg9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%12) : (f32) -> ()
    }) : (tensor<128xbf16>, tensor<f32>) -> tensor<f32>
    %8 = "tensor.extract"(%7) : (tensor<f32>) -> f32
    %9 = "arith.truncf"(%8) : (f32) -> bf16
    %10 = "tts.create_ptr"(%arg1, %0) : (!tt.ptr<bf16>, i32) -> !tt.ptr<bf16>
    "tt.store"(%10, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<bf16>, bf16) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir:22:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: [0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>>
              ^
<stdin>:2:139: note: scanning from here
 func.func @kernel(%arg0: memref<*xbf16>, %arg1: memref<*xbf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
                                                                                                                                          ^
<stdin>:2:139: note: with "PARAM_1_" equal to "%arg1"
 func.func @kernel(%arg0: memref<*xbf16>, %arg1: memref<*xbf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
                                                                                                                                          ^
<stdin>:19:16: note: possible intended match here
 %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>>
               ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_scalar.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @kernel(%arg0: memref<*xbf16>, %arg1: memref<*xbf16>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) { 
dag:22'0                                                                                                                                               X error: no match found
dag:22'1                                                                                                                                                 with "PARAM_1_" equal to "%arg1"
          3:  %c0 = arith.constant 0 : index 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %cst = arith.constant 0.000000e+00 : f32 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          5:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          6:  %alloc = memref.alloc() : memref<128xbf16> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          7:  memref.copy %reinterpret_cast, %alloc : memref<128xbf16, strided<[1]>> to memref<128xbf16> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
         14:  %4 = arith.addf %3, %init : f32 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         15:  linalg.yield %4 : f32 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~
         16:  } 
dag:22'0     ~~~
         17:  %extracted = tensor.extract %reduced[] : tensor<f32> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         18:  %2 = arith.truncf %extracted : f32 to bf16 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         19:  %reinterpret_cast_0 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1], offset: ?>> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:22'2                    ?                                                                                                                                             possible intended match
         20:  affine.store %2, %reinterpret_cast_0[0] : memref<1xbf16, strided<[1], offset: ?>> 
dag:22'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         21:  return 
dag:22'0     ~~~~~~~~
         22:  } 
dag:22'0     ~~~
         23: } 
dag:22'0     ~~
         24:  
dag:22'0     ~
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir (67 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir
module {
  func.func @reduce_kernel_2d_0d1d2de3de(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) {
    %c1_i32 = arith.constant 1 : i32
    %c5_i32 = arith.constant 5 : i32
    %c0_i32 = arith.constant 0 : i32
    %0 = tt.addptr %arg1, %arg7 : !tt.ptr<f32>, i32
    %1 = arith.sitofp %arg7 : i32 to f32
    scf.for %arg16 = %c0_i32 to %c5_i32 step %c1_i32  : i32 {
      %2 = tt.addptr %0, %arg16 : !tt.ptr<f32>, i32
      tt.store %2, %1 : !tt.ptr<f32>
    }
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir:19:16: error: CHECK-SAME: expected string not found in input
// CHECK-SAME: ([[PARAM_0_:%.+]]: memref<*xf32> {tt.divisibility = 16 : i32}, [[PARAM_1_:%.+]]: memref<*xf32> {tt.divisibility = 16 : i32}, [[PARAM_2_:%.+]]: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, [[PARAM_3_:%.+]]: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: i32, [[PARAM_9_:%.+]]: i32) {
               ^
<stdin>:2:40: note: scanning from here
 func.func @reduce_kernel_2d_0d1d2de3de(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) {
                                       ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
         1: module { 
         2:  func.func @reduce_kernel_2d_0d1d2de3de(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 16 : i32}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32, %arg14: i32, %arg15: i32) { 
same:19                                            X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found
         3:  %c1_i32 = arith.constant 1 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         4:  %c5_i32 = arith.constant 5 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         5:  %c0_i32 = arith.constant 0 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         6:  %0 = tt.addptr %arg1, %arg7 : !tt.ptr<f32>, i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         7:  %1 = arith.sitofp %arg7 : i32 to f32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         .
         .
         .
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop.mlir (68 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --canonicalize --triton-arith-to-linalg --structured-to-memref /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir
module {
  func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %0 = scf.for %arg13 = %c0_i32 to %c8_i32 step %c1_i32 iter_args(%arg14 = %arg0) -> (!tt.ptr<f32>)  : i32 {
      %1 = arith.sitofp %arg13 : i32 to f32
      tt.store %arg14, %1 : !tt.ptr<f32>
      %2 = tt.addptr %arg14, %c1_i32 : !tt.ptr<f32>, i32
      scf.yield %2 : !tt.ptr<f32>
    }
    return
  }
}
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir:19:16: error: CHECK-SAME: expected string not found in input
// CHECK-SAME: ([[PARAM_0_:%.+]]: memref<*xf32> {tt.divisibility = 16 : i32}, [[PARAM_1_:%.+]]: i32, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32) {
               ^
<stdin>:2:32: note: scanning from here
 func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
                               ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/scalar_store_loop.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
         1: module { 
         2:  func.func @reduce_kernel_2d_0d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) { 
same:19                                    X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found
         3:  %c8_i32 = arith.constant 8 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         4:  %c0_i32 = arith.constant 0 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         5:  %c1_i32 = arith.constant 1 : i32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         6:  %0 = scf.for %arg13 = %c0_i32 to %c8_i32 step %c1_i32 iter_args(%arg14 = %arg0) -> (!tt.ptr<f32>) : i32 { 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         7:  %1 = arith.sitofp %arg13 : i32 to f32 
same:19     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         .
         .
         .
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_reshape_broadcast.mlir (69 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_addi_reduce.mlir (70 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_addi_reduce.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "addi", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.addi"(%arg1, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      "tt.reduce.return"(%3) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "addi", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.addi"(%arg1, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @addi(%arg0: !tt.ptr<i32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4096xi32>
    %1 = linalg.fill ins(%c0_i32_0 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32>
    %c0_i32_1 = arith.constant 0 : i32
    %2 = bufferization.alloc_tensor() : tensor<i32>
    %inserted = tensor.insert %c0_i32_1 into %2[] : tensor<i32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xi32>) outs(%inserted : tensor<i32>) dimensions = [0] 
      (%in: i32, %init: i32) {
        %4 = arith.addi %in, %init : i32
        linalg.yield %4 : i32
      }
    %extracted = tensor.extract %reduced[] : tensor<i32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    tt.store %3, %extracted : !tt.ptr<i32>
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir:19:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
              ^
<stdin>:2:113: note: scanning from here
 func.func @addi(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                ^
<stdin>:2:113: note: with "PARAM_0_" equal to "%arg0"
 func.func @addi(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                ^
<stdin>:15:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
             ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @addi(%arg0: memref<*xi32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:19'0                                                                                                                     X error: no match found
dag:19'1                                                                                                                       with "PARAM_0_" equal to "%arg0"
          3:  %c0 = arith.constant 0 : index 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %c0_i32 = arith.constant 0 : i32 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          5:  %0 = tensor.empty() : tensor<4096xi32> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          6:  %1 = linalg.fill ins(%c0_i32 : i32) outs(%0 : tensor<4096xi32>) -> tensor<4096xi32> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          7:  %2 = bufferization.alloc_tensor() : tensor<i32> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
         10:  (%in: i32, %init: i32) { 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
         11:  %3 = arith.addi %in, %init : i32 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         12:  linalg.yield %3 : i32 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~
         13:  } 
dag:19'0     ~~~
         14:  %extracted = tensor.extract %reduced[] : tensor<i32> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         15:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:19'2                  ?                                                                                                                                           possible intended match
         16:  affine.store %extracted, %reinterpret_cast[0] : memref<1xi32, strided<[1], offset: ?>> 
dag:19'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         17:  return 
dag:19'0     ~~~~~~~~
         18:  } 
dag:19'0     ~~~
         19: } 
dag:19'0     ~~
         20:  
dag:19'0     ~
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_add_value.mlir (71 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_for_used_after_update.mlir (72 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/triton_assert.mlir (73 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_splat.mlir (74 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_add_value.mlir (75 of 215)
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir (76 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>) -> (), sym_name = "maxnumf", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<4096xf32>}> : () -> tensor<4096xf32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: f32, %arg2: f32):
      %3 = "arith.maxnumf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%3) : (f32) -> ()
    }) : (tensor<4096xf32>) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>) -> (), sym_name = "maxnumf", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<4096xf32>}> : () -> tensor<4096xf32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: f32, %arg2: f32):
      %4 = "arith.maxnumf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%4) : (f32) -> ()
    }) : (tensor<4096xf32>) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @maxnumf(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<4096xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4096xf32>) -> tensor<4096xf32>
    %cst_0 = arith.constant 0xFF800000 : f32
    %2 = bufferization.alloc_tensor() : tensor<f32>
    %inserted = tensor.insert %cst_0 into %2[] : tensor<f32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xf32>) outs(%inserted : tensor<f32>) dimensions = [0] 
      (%in: f32, %init: f32) {
        %4 = arith.maxnumf %in, %init : f32
        linalg.yield %4 : f32
      }
    %extracted = tensor.extract %reduced[] : tensor<f32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %3, %extracted : !tt.ptr<f32>
    return
  }
}
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>) -> (), sym_name = "minnumf", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<4096xf32>}> : () -> tensor<4096xf32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: f32, %arg2: f32):
      %3 = "arith.minnumf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%3) : (f32) -> ()
    }) : (tensor<4096xf32>) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>) -> (), sym_name = "minnumf", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<4096xf32>}> : () -> tensor<4096xf32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: f32, %arg2: f32):
      %4 = "arith.minnumf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "tt.reduce.return"(%4) : (f32) -> ()
    }) : (tensor<4096xf32>) -> f32
    %3 = "tts.create_ptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
module {
  func.func @minnumf(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<4096xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4096xf32>) -> tensor<4096xf32>
    %cst_0 = arith.constant 0x7F800000 : f32
    %2 = bufferization.alloc_tensor() : tensor<f32>
    %inserted = tensor.insert %cst_0 into %2[] : tensor<f32>
    %reduced = linalg.reduce ins(%1 : tensor<4096xf32>) outs(%inserted : tensor<f32>) dimensions = [0] 
      (%in: f32, %init: f32) {
        %4 = arith.minnumf %in, %init : f32
        linalg.yield %4 : f32
      }
    %extracted = tensor.extract %reduced[] : tensor<f32>
    %3 = "tts.create_ptr"(%arg0, %c0_i32) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    tt.store %3, %extracted : !tt.ptr<f32>
    return
  }
}
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:20:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:2:116: note: scanning from here
 func.func @maxnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                   ^
<stdin>:2:116: note: with "PARAM_0_" equal to "%arg0"
 func.func @maxnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                   ^
<stdin>:16:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir:57:15: error: CHECK-DAG: expected string not found in input
// CHECK-DAG: [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
              ^
<stdin>:24:116: note: scanning from here
 func.func @minnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                   ^
<stdin>:24:116: note: with "PARAM_0_" equal to "%arg0"
 func.func @minnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) {
                                                                                                                   ^
<stdin>:38:14: note: possible intended match here
 %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>
             ^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: module { 
          2:  func.func @maxnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:20'0                                                                                                                        X error: no match found
dag:20'1                                                                                                                          with "PARAM_0_" equal to "%arg0"
          3:  %c0 = arith.constant 0 : index 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          4:  %cst = arith.constant 0xFF800000 : f32 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          5:  %cst_0 = arith.constant 0.000000e+00 : f32 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          6:  %0 = tensor.empty() : tensor<4096xf32> 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          7:  %1 = linalg.fill ins(%cst_0 : f32) outs(%0 : tensor<4096xf32>) -> tensor<4096xf32> 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
         11:  (%in: f32, %init: f32) { 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
         12:  %3 = arith.maxnumf %in, %init : f32 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         13:  linalg.yield %3 : f32 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~
         14:  } 
dag:20'0     ~~~
         15:  %extracted = tensor.extract %reduced[] : tensor<f32> 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         16:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:20'2                  ?                                                                                                                                           possible intended match
         17:  affine.store %extracted, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:20'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         18:  return 
dag:20'0     ~~~~~~~~
         19:  } 
dag:20'0     ~~~
         20: } 
dag:20'0     ~~
         21:  
dag:20'0     ~
         22: // ----- 
dag:20'0     ~~~~~~~~~
         23: module { 
dag:20'0     ~~~~~~~~~
         24:  func.func @minnumf(%arg0: memref<*xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32) { 
dag:20'0     ~~~~~~~~~~~~~~~~~~~
dag:57'0                                                                                                                        X error: no match found
dag:57'1                                                                                                                          with "PARAM_0_" equal to "%arg0"
         25:  %c0 = arith.constant 0 : index 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         26:  %cst = arith.constant 0x7F800000 : f32 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         27:  %cst_0 = arith.constant 0.000000e+00 : f32 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         28:  %0 = tensor.empty() : tensor<4096xf32> 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         29:  %1 = linalg.fill ins(%cst_0 : f32) outs(%0 : tensor<4096xf32>) -> tensor<4096xf32> 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          .
          .
          .
         33:  (%in: f32, %init: f32) { 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
         34:  %3 = arith.minnumf %in, %init : f32 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         35:  linalg.yield %3 : f32 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~
         36:  } 
dag:57'0     ~~~
         37:  %extracted = tensor.extract %reduced[] : tensor<f32> 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         38:  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [%c0], sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>> 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:57'2                  ?                                                                                                                                           possible intended match
         39:  affine.store %extracted, %reinterpret_cast[0] : memref<1xf32, strided<[1], offset: ?>> 
dag:57'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         40:  return 
dag:57'0     ~~~~~~~~
         41:  } 
dag:57'0     ~~~
         42: } 
dag:57'0     ~~
         43:  
dag:57'0     ~
>>>>>>

--

********************
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reduce_extend_fp32_precision.mlir (77 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/cumsum.mlir (78 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/kernel-05-layer-norm-dwdb.mlir (79 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_2d_elemwise_arith_ternary.mlir (80 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_loopback.mlir (81 of 215)
XFAIL: TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_unsupported_add_offset.mlir (82 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/make_tensor_ptr_ordering_error.mlir (83 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_2d_example.mlir (84 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/nested_loops.mlir (85 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_nested.mlir (86 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_for_more_init_args.mlir (87 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_minmax_reduce.mlir (88 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/masked_ldst_sitofp_other.mlir (89 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_for_2d.mlir (90 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/kernel-01-vector-add.mlir (91 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/use_dot_opc.mlir (92 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/triton-to-structured-prepass.mlir (93 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/block_ptr_advance.mlir (94 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_extern_elementwise.mlir (95 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_extern_elementwise.mlir (96 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_512_256_f32_axis0.mlir (97 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/get_num_programs.mlir (98 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/use_mid_chain.mlir (99 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_splat_2d.mlir (100 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/ridiculously_nested_loops.mlir (101 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/kernel-02-fused-softmax.mlir (102 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/kernel-03-matrix-multiplication.mlir (103 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/wraparound_stacked.mlir (104 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/wraparound_unsupported_add_offset.mlir (105 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/arith_not_ptr_arith.mlir (106 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_2d_elemwise_arith_unary.mlir (107 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_1d_elemwise_arith_unary.mlir (108 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_2d_elemwise_arith_unary.mlir (109 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_2d_elemwise_arith_binary.mlir (110 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_middle_dim.mlir (111 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/bitcast.mlir (112 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_512_256_bf16_axis0.mlir (113 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_minmax_reduce.mlir (114 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_nested.mlir (115 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_argmin_argmax.mlir (116 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_1d_elemwise_arith_ternary.mlir (117 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_middle_dim.mlir (118 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/use_dot_opc.mlir (119 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_argmin_argmax_2d.mlir (120 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_for_expand_ptr.mlir (121 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/unsupported_extern_elementwise.mlir (122 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/wraparound_side_by_side.mlir (123 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_nested.mlir (124 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/triton_assert.mlir (125 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_for_used_after_update.mlir (126 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_512_256_bf16_axis0.mlir (127 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_scalar.mlir (128 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/tensor_indices_loop_iterarg_with_masks.mlir (129 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/triton_assert.mlir (130 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducemax_32_256_bf16.mlir (131 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_loopback.mlir (132 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_512_256_bf16_axis1.mlir (133 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_2d_elemwise_arith_binary.mlir (134 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/dot.mlir (135 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/kernel-01-vector-add.mlir (136 of 215)
PASS: TRITON-SHARED :: Conversion/StructuredToMemref/convert_extern_elementwise.mlir (137 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_splat_2d.mlir (138 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_reshape_broadcast.mlir (139 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/kernel-05-layer-norm-fwd.mlir (140 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/masked_ldst_2d.mlir (141 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_dim1.mlir (142 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/arith_not_ptr_arith.mlir (143 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/kernel-03-matrix-multiplication.mlir (144 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_addi_reduce.mlir (145 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_loopback.mlir (146 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_minmax_fp_reduce.mlir (147 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_1d_elemwise_arith_ternary.mlir (148 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_1d_elemwise_arith_binary.mlir (149 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_for_expand_ptr.mlir (150 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_for_accumulation.mlir (151 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_nested.mlir (152 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/early_return.mlir (153 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/use_end_chain.mlir (154 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_for_accumulation.mlir (155 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/use_mid_chain.mlir (156 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/wraparound_side_by_side.mlir (157 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_512_256_f32_axis1.mlir (158 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_loopback.mlir (159 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_2d_example.mlir (160 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/kernel-02-fused-softmax.mlir (161 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_for.mlir (162 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/tensor_indices_loop_iterargs_not_used_ptranalysis_prepass.mlir (163 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_splat_float.mlir (164 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/kernel-05-layer-norm-fwd.mlir (165 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_minmax_fp_reduce.mlir (166 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/dot.mlir (167 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/sign_extend_i32_to_i64.mlir (168 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_512_256_f32_axis0.mlir (169 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_reshape_broadcast.mlir (170 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_tensor_reshape.mlir (171 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_for_used_before_update.mlir (172 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_argmin_argmax_2d.mlir (173 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/tensor_indices_loop_iterargs_nested.mlir (174 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_for_more_init_args.mlir (175 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_splat.mlir (176 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_mul_value_const.mlir (177 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/bitcast.mlir (178 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/wraparound_stacked.mlir (179 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_tensor_reshape.mlir (180 of 215)
XFAIL: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_dim1.mlir (181 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_addi_reduce.mlir (182 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_scalar_broadcast.mlir (183 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_for_2d.mlir (184 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_for_used_before_update.mlir (185 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/addptr_mul_const_const.mlir (186 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/masked_ldst_2d.mlir (187 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/block_ptr_advance.mlir (188 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_broadcast.mlir (189 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_add_value.mlir (190 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_mul_value_const.mlir (191 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_1d_elemwise_arith_binary.mlir (192 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_mul_const_const.mlir (193 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_argmin_argmax.mlir (194 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_512_256_bf16_axis1.mlir (195 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/tensor_indices_loop_iterargs_not_used_ptranalysis_e2e.mlir (196 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/cumsum.mlir (197 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducemax_32_256_bf16.mlir (198 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/block_ptr_advance.mlir (199 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_minmax.mlir (200 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/reducesum_512_256_f32_axis1.mlir (201 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/addptr_scalar_for.mlir (202 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/unsupported_extern_elementwise.mlir (203 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/use_end_chain.mlir (204 of 215)
PASS: TRITON-SHARED :: Conversion/TritonArithToLinalg/convert_1d_elemwise_arith_unary.mlir (205 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/masked_ldst_1d.mlir (206 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/dot.mlir (207 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_2d_elemwise_arith_ternary.mlir (208 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/masked_ldst_1d.mlir (209 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/kernel-05-layer-norm-dwdb.mlir (210 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/bitcast.mlir (211 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/reducesum_scalar.mlir (212 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToLinalg/convert_splat_float.mlir (213 of 215)
PASS: TRITON-SHARED :: Conversion/TritonToStructured/masked_ldst_sitofp_other.mlir (214 of 215)
XFAIL: TRITON-SHARED :: Conversion/TritonToLinalg/wraparound_unsupported_add_offset.mlir (215 of 215)
********************
Failed Tests (31):
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_chain.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_dim1.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_nested.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_loopback.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_advance.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/block_ptr_complex_offset.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_binary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_ternary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_1d_elemwise_arith_unary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_addi_reduce.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_fp_reduce.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_reduce.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/convert_splat_float.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/get_num_programs.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/kernel-05-layer-norm-fwd.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/nested_loops.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_middle_dim.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_scalar.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_loop_iterargs.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_nested_loop.mlir
  TRITON-SHARED :: Conversion/StructuredToMemref/scalar_store_no_iterargs.mlir
  TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir


Testing Time: 0.93s

Total Discovered Tests: 215
  Passed           : 181 (84.19%)
  Expectedly Failed:   3 (1.40%)
  Failed           :  31 (14.42%)
