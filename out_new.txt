-- Testing: 215 tests, 16 workers --
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_middle_dim.mlir (1 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/reducesum_middle_dim.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %out, %5 : tensor<32x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: note: see current operation: tt.store %arg2, %19 : tensor<32x16x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %out, %5 : tensor<32x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :37:5: note: see current operation: tt.store %arg2, %19 : tensor<32x16x!tt.ptr<bf16>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, tensor<32x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: tensor<32x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "tts.make_tptr"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %6 = "tt.reduce"(%5) <{axis = 1 : i32}> ({
    ^bb0(%arg3: bf16, %arg4: bf16):
      %7 = "arith.addf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%7) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<32x16xbf16>
    "tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
these are the uses:
"tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
actual processing
processing user
"tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
tensor<32x16x!tt.ptr<bf16>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, !tt.ptr<bf16>, tensor<32x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: tensor<32x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<32x16xi32>}> : () -> tensor<32x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 256 : index}> : () -> index
    %4 = "tts.make_tptr"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %6 = "tt.reduce"(%5) <{axis = 1 : i32}> ({
    ^bb0(%arg3: bf16, %arg4: bf16):
      %8 = "arith.addf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (bf16, bf16) -> bf16
      "tt.reduce.return"(%8) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<32x16xbf16>
    %7 = "tts.make_unstructured_tptr"(%arg2, %0) : (tensor<32x16x!tt.ptr<bf16>>, tensor<32x16xi32>) -> tensor<32x16x!tt.ptr<bf16>>
    "tt.store"(%0, %6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<32x16xi32>, tensor<32x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :31:12: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<32x256x16x!tt.ptr<bf16>>'
    %afm = tt.load %9 : tensor<32x256x16x!tt.ptr<bf16>>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir:1 offset :31:12: note: see current operation: "memref.copy"(%6, %7) : (tensor<32x256x16x!tt.ptr<bf16>>, memref<32x256x16xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducesum_middle_dim.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir (2 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %c, %res0 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: note: see current operation: tt.store %arg2, %15 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %c, %res0 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :27:9: note: see current operation: tt.store %arg2, %15 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %d, %res1 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: note: see current operation: tt.store %arg3, %16 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %d, %res1 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :28:9: note: see current operation: tt.store %arg3, %16 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<128x128x!tt.ptr<f32>>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %7 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %8 = "arith.addf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %9 = "arith.subf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: tensor<128x128x!tt.ptr<f32>>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %7 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %8 = "arith.addf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %9 = "arith.subf"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %10 = "tts.make_unstructured_tptr"(%arg2, %1) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%1, %8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %11 = "tts.make_unstructured_tptr"(%arg3, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :23:15: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<f32>>'
        %af = tt.load %9 : tensor<128x128x!tt.ptr<f32>>
              ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir:1 offset :23:15: note: see current operation: "memref.copy"(%6, %9) : (tensor<128x128x!tt.ptr<f32>>, memref<128x128xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_binary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir (3 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %res, %6 : tensor<256x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: note: see current operation: tt.store %arg1, %19 : tensor<256x16x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %res, %6 : tensor<256x16x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :37:5: note: see current operation: tt.store %arg1, %19 : tensor<256x16x!tt.ptr<bf16>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, tensor<256x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: tensor<256x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 256 : index}> : () -> index
    %3 = "tts.make_tptr"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %5 = "tt.reduce"(%4) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %6 = "arith.cmpf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (bf16, bf16) -> i1
      %7 = "arith.select"(%6, %arg2, %arg3) : (i1, bf16, bf16) -> bf16
      "tt.reduce.return"(%7) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<256x16xbf16>
    "tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
these are the uses:
"tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
actual processing
processing user
"tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
tensor<256x16x!tt.ptr<bf16>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<bf16>, tensor<256x16x!tt.ptr<bf16>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<bf16>, %arg1: tensor<256x16x!tt.ptr<bf16>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<256x16xi32>}> : () -> tensor<256x16xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 256 : index}> : () -> index
    %3 = "tts.make_tptr"(%arg0, %2) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, order = array<i32>, sizes = array<i64: 32, 256, 16>, static_offsets = array<i64: 0, 0, 0>, static_shape = array<i64: 0, 0, 0>, static_strides = array<i64: -9223372036854775808, 1, 1>}> : (!tt.ptr<bf16>, index) -> tensor<32x256x16x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<32x256x16x!tt.ptr<bf16>>) -> tensor<32x256x16xbf16>
    %5 = "tt.reduce"(%4) <{axis = 0 : i32}> ({
    ^bb0(%arg2: bf16, %arg3: bf16):
      %7 = "arith.cmpf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (bf16, bf16) -> i1
      %8 = "arith.select"(%7, %arg2, %arg3) : (i1, bf16, bf16) -> bf16
      "tt.reduce.return"(%8) : (bf16) -> ()
    }) : (tensor<32x256x16xbf16>) -> tensor<256x16xbf16>
    %6 = "tts.make_unstructured_tptr"(%arg1, %0) : (tensor<256x16x!tt.ptr<bf16>>, tensor<256x16xi32>) -> tensor<256x16x!tt.ptr<bf16>>
    "tt.store"(%0, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<256x16xi32>, tensor<256x16xbf16>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :30:12: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<32x256x16x!tt.ptr<bf16>>'
    %afm = tt.load %9 : tensor<32x256x16x!tt.ptr<bf16>>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir:1 offset :30:12: note: see current operation: "memref.copy"(%6, %7) : (tensor<32x256x16x!tt.ptr<bf16>>, memref<32x256x16xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/reducemax_32_256_bf16.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_side_by_side.mlir (4 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_side_by_side.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
module {
  tt.func public @wrap_side_by_side_masked_loop_01234567(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %cst = arith.constant -9.900000e+01 : f32
    %c0 = arith.constant 0 : index
    %c6 = arith.constant 6 : index
    %c2 = arith.constant 2 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg4 : i32 to index
    %1 = arith.muli %0, %c2 : index
    %2 = arith.index_cast %arg3 : i32 to index
    %3 = arith.index_cast %arg5 : i32 to index
    %4 = arith.muli %3, %c6 : index
    %5 = arith.muli %2, %3 : index
    %6 = arith.index_cast %arg6 : i32 to index
    %7 = arith.index_cast %arg7 : i32 to index
    %8 = arith.muli %arg4, %c4_i32 : i32
    %9 = arith.index_cast %8 : i32 to index
    %10 = arith.muli %arg5, %c4_i32 : i32
    %11 = arith.index_cast %10 : i32 to index
    %12:2 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %1, %arg10 = %c0) -> (index, index)  : i32 {
      %13 = tts.make_tptr %arg1 to sizes: [4, 4], strides: [%6, %7], offsets: [%arg10, %c0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %14 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%0, %3], offsets: [%arg9, %4], shape: [0, %5], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %15 = "tts.load"(%14, %cst) <{operandSegmentSizes = array<i32: 1, 0, 1>, static_mask_dims = array<i64: 2, 4>}> : (tensor<4x4x!tt.ptr<f32>>, f32) -> tensor<4x4xf32>
      "tts.store"(%13, %15) <{static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>, tensor<4x4xf32>) -> ()
      %16 = arith.addi %arg9, %9 : index
      %17 = arith.addi %arg10, %11 : index
      scf.yield %16, %17 : index, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @wrap_side_by_side_masked_loop_01234567(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %cst = arith.constant -9.900000e+01 : f32
    %c0 = arith.constant 0 : index
    %c6 = arith.constant 6 : index
    %c2 = arith.constant 2 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg4 : i32 to index
    %1 = arith.muli %0, %c2 : index
    %2 = arith.index_cast %arg3 : i32 to index
    %3 = arith.index_cast %arg5 : i32 to index
    %4 = arith.muli %3, %c6 : index
    %5 = arith.muli %2, %3 : index
    %6 = arith.index_cast %arg6 : i32 to index
    %7 = arith.index_cast %arg7 : i32 to index
    %8 = arith.muli %arg4, %c4_i32 : i32
    %9 = arith.index_cast %8 : i32 to index
    %10 = arith.muli %arg5, %c4_i32 : i32
    %11 = arith.index_cast %10 : i32 to index
    %12:2 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %1, %arg10 = %c0) -> (index, index)  : i32 {
      %13 = tts.make_tptr %arg1 to sizes: [4, 4], strides: [%6, %7], offsets: [%arg10, %c0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %14 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%0, %3], offsets: [%arg9, %4], shape: [0, %5], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %15 = "tts.load"(%14, %cst) <{operandSegmentSizes = array<i32: 1, 0, 1>, static_mask_dims = array<i64: 2, 4>}> : (tensor<4x4x!tt.ptr<f32>>, f32) -> tensor<4x4xf32>
      "tts.store"(%13, %15) <{static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>, tensor<4x4xf32>) -> ()
      %16 = arith.addi %arg9, %9 : index
      %17 = arith.addi %arg10, %11 : index
      scf.yield %16, %17 : index, index
    }
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir:1 offset :48:7: error: 'bufferization.materialize_in_destination' op failed to verify that all of {source, dest} have same element type
      tt.store %arg10, %34 : tensor<4x4x!tt.ptr<f32>>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir:1 offset :48:7: note: see current operation: %68 = "bufferization.materialize_in_destination"(%67, %25) <{writable}> : (tensor<4x4xf32>, tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4x!tt.ptr<f32>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir (5 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir
module {
  tt.func public @test_argmax(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1 = tt.expand_dims %0 {axis = 0 : i32} : tensor<4xi32> -> tensor<1x4xi32>
    %2 = arith.index_cast %arg2 : i32 to index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%2, %3], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4xf32>
    %6 = tt.broadcast %1 : tensor<1x4xi32> -> tensor<4x4xi32>
    %7:2 = "tt.reduce"(%5, %6) <{axis = 1 : i32}> ({
    ^bb0(%arg4: f32, %arg5: i32, %arg6: f32, %arg7: i32):
      %10 = arith.cmpf oeq, %arg4, %arg6 : f32
      %11 = arith.cmpi slt, %arg5, %arg7 : i32
      %12 = arith.andi %10, %11 : i1
      %13 = arith.cmpf ogt, %arg4, %arg6 : f32
      %14 = arith.ori %13, %12 : i1
      %15 = arith.select %14, %arg4, %arg6 : f32
      %16 = arith.select %14, %arg5, %arg7 : i32
      tt.reduce.return %15, %16 : f32, i32
    }) : (tensor<4x4xf32>, tensor<4x4xi32>) -> (tensor<4xf32>, tensor<4xi32>)
    %8 = tts.make_tptr %arg1 to sizes: [4], strides: [1], offsets: [0], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
    %9 = arith.sitofp %7#1 : tensor<4xi32> to tensor<4xf32>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @test_argmax(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1 = tt.expand_dims %0 {axis = 0 : i32} : tensor<4xi32> -> tensor<1x4xi32>
    %2 = arith.index_cast %arg2 : i32 to index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%2, %3], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4xf32>
    %6 = tt.broadcast %1 : tensor<1x4xi32> -> tensor<4x4xi32>
    %7:2 = "tt.reduce"(%5, %6) <{axis = 1 : i32}> ({
    ^bb0(%arg4: f32, %arg5: i32, %arg6: f32, %arg7: i32):
      %10 = arith.cmpf oeq, %arg4, %arg6 : f32
      %11 = arith.cmpi slt, %arg5, %arg7 : i32
      %12 = arith.andi %10, %11 : i1
      %13 = arith.cmpf ogt, %arg4, %arg6 : f32
      %14 = arith.ori %13, %12 : i1
      %15 = arith.select %14, %arg4, %arg6 : f32
      %16 = arith.select %14, %arg5, %arg7 : i32
      tt.reduce.return %15, %16 : f32, i32
    }) : (tensor<4x4xf32>, tensor<4x4xi32>) -> (tensor<4xf32>, tensor<4xi32>)
    %8 = tts.make_tptr %arg1 to sizes: [4], strides: [1], offsets: [0], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
    %9 = arith.sitofp %7#1 : tensor<4xi32> to tensor<4xf32>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir:1 offset :34:11: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4x4x!tt.ptr<f32>>'
    %12 = tt.load %11 : tensor<4x4x!tt.ptr<f32>>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir:1 offset :34:11: note: see current operation: "memref.copy"(%8, %9) : (tensor<4x4x!tt.ptr<f32>>, memref<4x4xf32>) -> ()
module {
  tt.func public @test_argmin(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1 = tt.expand_dims %0 {axis = 0 : i32} : tensor<4xi32> -> tensor<1x4xi32>
    %2 = arith.index_cast %arg2 : i32 to index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%2, %3], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4xf32>
    %6 = tt.broadcast %1 : tensor<1x4xi32> -> tensor<4x4xi32>
    %7:2 = "tt.reduce"(%5, %6) <{axis = 1 : i32}> ({
    ^bb0(%arg4: f32, %arg5: i32, %arg6: f32, %arg7: i32):
      %10 = arith.cmpf oeq, %arg4, %arg6 : f32
      %11 = arith.cmpi slt, %arg5, %arg7 : i32
      %12 = arith.andi %10, %11 : i1
      %13 = arith.cmpf olt, %arg4, %arg6 : f32
      %14 = arith.ori %13, %12 : i1
      %15 = arith.select %14, %arg4, %arg6 : f32
      %16 = arith.select %14, %arg5, %arg7 : i32
      tt.reduce.return %15, %16 : f32, i32
    }) : (tensor<4x4xf32>, tensor<4x4xi32>) -> (tensor<4xf32>, tensor<4xi32>)
    %8 = tts.make_tptr %arg1 to sizes: [4], strides: [1], offsets: [0], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
    %9 = arith.sitofp %7#1 : tensor<4xi32> to tensor<4xf32>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @test_argmin(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1 = tt.expand_dims %0 {axis = 0 : i32} : tensor<4xi32> -> tensor<1x4xi32>
    %2 = arith.index_cast %arg2 : i32 to index
    %3 = arith.index_cast %arg3 : i32 to index
    %4 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%2, %3], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
    %5 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4xf32>
    %6 = tt.broadcast %1 : tensor<1x4xi32> -> tensor<4x4xi32>
    %7:2 = "tt.reduce"(%5, %6) <{axis = 1 : i32}> ({
    ^bb0(%arg4: f32, %arg5: i32, %arg6: f32, %arg7: i32):
      %10 = arith.cmpf oeq, %arg4, %arg6 : f32
      %11 = arith.cmpi slt, %arg5, %arg7 : i32
      %12 = arith.andi %10, %11 : i1
      %13 = arith.cmpf olt, %arg4, %arg6 : f32
      %14 = arith.ori %13, %12 : i1
      %15 = arith.select %14, %arg4, %arg6 : f32
      %16 = arith.select %14, %arg5, %arg7 : i32
      tt.reduce.return %15, %16 : f32, i32
    }) : (tensor<4x4xf32>, tensor<4x4xi32>) -> (tensor<4xf32>, tensor<4xi32>)
    %8 = tts.make_tptr %arg1 to sizes: [4], strides: [1], offsets: [0], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
    %9 = arith.sitofp %7#1 : tensor<4xi32> to tensor<4xf32>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir:108 offset :34:11: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4x4x!tt.ptr<f32>>'
    %12 = tt.load %11 : tensor<4x4x!tt.ptr<f32>>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir:108 offset :34:11: note: see current operation: "memref.copy"(%8, %9) : (tensor<4x4x!tt.ptr<f32>>, memref<4x4xf32>) -> ()
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir:58:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: func.func @test_argmax
                ^
<stdin>:1:1: note: scanning from here
// -----
^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax_2d.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: // -----
label:58     X~~~~~~~~ error: no match found
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir (6 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
        tt.store %d, %100 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: remark: PtrAnalysis: Failed to rewrite StoreOp
        tt.store %d, %100 : tensor<128x128x!tt.ptr<f32>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :30:9: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i1>) -> tensor<128x128x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i1>>) -> tensor<128x128xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i1>, !tt.ptr<f32>, !tt.ptr<f32>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<i1>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %4 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i1>) -> tensor<128x128x!tt.ptr<i1>>
    %5 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %6 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %7 = "tts.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i1>>) -> tensor<128x128xi1>
    %8 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %9 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %10 = "arith.select"(%7, %8, %9) : (tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
    %11 = "tts.make_unstructured_tptr"(%arg3, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %10) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :26:15: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<i1>>'
        %am = tt.load %9 : tensor<128x128x!tt.ptr<i1>>
              ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir:1 offset :26:15: note: see current operation: "memref.copy"(%7, %12) : (tensor<128x128x!tt.ptr<i1>>, memref<128x128xi1>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_ternary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir (7 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save0, %5 : tensor<128x128x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save0, %5 : tensor<128x128x!tt.ptr<bf16>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :38:5: note: see current operation: tt.store %arg3, %19 : tensor<128x128x!tt.ptr<bf16>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save1, %6 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: note: see current operation: tt.store %arg4, %20 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save1, %6 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :39:5: note: see current operation: tt.store %arg4, %20 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save2, %7 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: note: see current operation: tt.store %arg5, %21 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save2, %7 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :40:5: note: see current operation: tt.store %arg5, %21 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save3, %10 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: note: see current operation: tt.store %arg6, %22 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save3, %10 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :41:5: note: see current operation: tt.store %arg6, %22 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %save4, %11 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: note: see current operation: tt.store %arg7, %23 : tensor<128x128x!tt.ptr<f32>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %save4, %11 : tensor<128x128x!tt.ptr<f32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :42:5: note: see current operation: tt.store %arg7, %23 : tensor<128x128x!tt.ptr<f32>>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<128x128x!tt.ptr<bf16>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<128x128x!tt.ptr<bf16>>, %arg4: tensor<128x128x!tt.ptr<f32>>, %arg5: tensor<128x128x!tt.ptr<f32>>, %arg6: tensor<128x128x!tt.ptr<f32>>, %arg7: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i32>>) -> tensor<128x128xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f16>>) -> tensor<128x128xf16>
    %14 = "arith.truncf"(%11) : (tensor<128x128xf32>) -> tensor<128x128xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %16 = "arith.sitofp"(%12) : (tensor<128x128xi32>) -> tensor<128x128xf32>
    %17 = "arith.extf"(%13) : (tensor<128x128xf16>) -> tensor<128x128xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
actual processing
processing user
"tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
tensor<128x128x!tt.ptr<bf16>>
~~~~
processing val
%3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
processing val
%0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
these are the uses:
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
actual processing
processing user
"tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
tensor<128x128x!tt.ptr<f32>>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f16>, tensor<128x128x!tt.ptr<bf16>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>, tensor<128x128x!tt.ptr<f32>>) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f16>, %arg3: tensor<128x128x!tt.ptr<bf16>>, %arg4: tensor<128x128x!tt.ptr<f32>>, %arg5: tensor<128x128x!tt.ptr<f32>>, %arg6: tensor<128x128x!tt.ptr<f32>>, %arg7: tensor<128x128x!tt.ptr<f32>>):
    %0 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %1 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %2 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %3 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %4 = "arith.constant"() <{value = dense<0> : tensor<128x128xi32>}> : () -> tensor<128x128xi32>
    %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %8 = "tts.make_tptr"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>
    %9 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>>
    %10 = "tts.make_tptr"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: 0, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>>
    %11 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
    %12 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<i32>>) -> tensor<128x128xi32>
    %13 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f16>>) -> tensor<128x128xf16>
    %14 = "arith.truncf"(%11) : (tensor<128x128xf32>) -> tensor<128x128xbf16>
    %15 = "math.exp"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %16 = "arith.sitofp"(%12) : (tensor<128x128xi32>) -> tensor<128x128xf32>
    %17 = "arith.extf"(%13) : (tensor<128x128xf16>) -> tensor<128x128xf32>
    %18 = "math.sqrt"(%11) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
    %19 = "tts.make_unstructured_tptr"(%arg3, %4) : (tensor<128x128x!tt.ptr<bf16>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<bf16>>
    "tt.store"(%4, %14) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xbf16>) -> ()
    %20 = "tts.make_unstructured_tptr"(%arg4, %3) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%3, %15) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %21 = "tts.make_unstructured_tptr"(%arg5, %2) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%2, %16) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %22 = "tts.make_unstructured_tptr"(%arg6, %1) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%1, %17) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    %23 = "tts.make_unstructured_tptr"(%arg7, %0) : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>) -> tensor<128x128x!tt.ptr<f32>>
    "tt.store"(%0, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<128x128xi32>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :30:12: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<f32>>'
    %afm = tt.load %9 : tensor<128x128x!tt.ptr<f32>>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir:1 offset :30:12: note: see current operation: "memref.copy"(%7, %12) : (tensor<128x128x!tt.ptr<f32>>, memref<128x128xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_2d_elemwise_arith_unary.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_accumulation.mlir (8 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_for_accumulation.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c1 = arith.constant 1 : index
    %c5 = arith.constant 5 : index
    %c3 = arith.constant 3 : index
    %c12 = arith.constant 12 : index
    %c0 = arith.constant 0 : index
    %0 = arith.index_cast %arg3 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3:2 = scf.for %arg5 = %c0 to %c12 step %c3 iter_args(%arg6 = %2, %arg7 = %0) -> (tensor<4x256xbf16>, index) {
      %5 = tts.make_tptr %arg1 to sizes: [4, 256], strides: [%c1, %c5], offsets: [%arg7, %c0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
      %6 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
      %7 = arith.addf %arg6, %6 : tensor<4x256xbf16>
      %8 = arith.addi %arg7, %c3 : index
      scf.yield %7, %8 : tensor<4x256xbf16>, index
    }
    %4 = tts.make_tptr %arg2 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%4, %3#0) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32_1 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c1 = arith.constant 1 : index
    %c5 = arith.constant 5 : index
    %c3 = arith.constant 3 : index
    %c12 = arith.constant 12 : index
    %c0 = arith.constant 0 : index
    %0 = arith.index_cast %arg3 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3:2 = scf.for %arg5 = %c0 to %c12 step %c3 iter_args(%arg6 = %2, %arg7 = %0) -> (tensor<4x256xbf16>, index) {
      %5 = tts.make_tptr %arg1 to sizes: [4, 256], strides: [%c1, %c5], offsets: [%arg7, %c0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
      %6 = "tts.load"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
      %7 = arith.addf %arg6, %6 : tensor<4x256xbf16>
      %8 = arith.addi %arg7, %c3 : index
      scf.yield %7, %8 : tensor<4x256xbf16>, index
    }
    %4 = tts.make_tptr %arg2 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%4, %3#0) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir:1 offset :36:11: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4x256x!tt.ptr<bf16>>'
    %19 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256x!tt.ptr<bf16>> // this will be replaced with a memref.copy
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir:1 offset :36:11: note: see current operation: "memref.copy"(%10, %11) : (tensor<4x256x!tt.ptr<bf16>>, memref<4x256xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_for_accumulation.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_stacked.mlir (9 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/wraparound_stacked.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir:1 offset :47:13: warning: PtrAnalysis: allowing adding pointer state with modulo in dim 0 to another pointer state with offset in dim 0.
Please verify the operand that contains a scalar is meant to increment pointers in dim1. If that is not the case it WILL LEAD TO WRONG COMPILATION RESULTS.

To avoid this warning, use expand_dims (instead of splat) to explicitly specify which dimension contains the scalar.
      %33 = tt.addptr %arg9, %30 : tensor<4x4x!tt.ptr<f32>>, tensor<4x4xi32>
            ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir:1 offset :47:13: note: see current operation: %48 = tt.addptr %arg9, %42 : tensor<4x4x!tt.ptr<f32>>, tensor<4x4xi32>
module {
  tt.func public @wrap_stacked_masked_loop_01234567(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %cst = arith.constant -9.900000e+01 : f32
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg4 : i32 to index
    %2 = arith.muli %1, %c2 : index
    %3 = arith.muli %0, %1 : index
    %4 = arith.index_cast %arg5 : i32 to index
    %5 = arith.muli %4, %c3 : index
    %6 = arith.index_cast %arg6 : i32 to index
    %7 = arith.index_cast %arg7 : i32 to index
    %8 = arith.muli %arg5, %c4_i32 : i32
    %9 = arith.index_cast %8 : i32 to index
    %10:2 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %2, %arg10 = %c0) -> (index, index)  : i32 {
      %11 = tts.make_tptr %arg1 to sizes: [4, 4], strides: [%6, %7], offsets: [%arg10, %c0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %12 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%1, %4], offsets: [%arg9, %5], shape: [%3, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %13 = "tts.load"(%12, %cst) <{operandSegmentSizes = array<i32: 1, 0, 1>, static_mask_dims = array<i64: 4, 3>}> : (tensor<4x4x!tt.ptr<f32>>, f32) -> tensor<4x4xf32>
      "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>, tensor<4x4xf32>) -> ()
      %14 = arith.addi %arg9, %9 : index
      %15 = arith.addi %arg10, %9 : index
      scf.yield %14, %15 : index, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @wrap_stacked_masked_loop_01234567(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %cst = arith.constant -9.900000e+01 : f32
    %c0 = arith.constant 0 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg4 : i32 to index
    %2 = arith.muli %1, %c2 : index
    %3 = arith.muli %0, %1 : index
    %4 = arith.index_cast %arg5 : i32 to index
    %5 = arith.muli %4, %c3 : index
    %6 = arith.index_cast %arg6 : i32 to index
    %7 = arith.index_cast %arg7 : i32 to index
    %8 = arith.muli %arg5, %c4_i32 : i32
    %9 = arith.index_cast %8 : i32 to index
    %10:2 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %2, %arg10 = %c0) -> (index, index)  : i32 {
      %11 = tts.make_tptr %arg1 to sizes: [4, 4], strides: [%6, %7], offsets: [%arg10, %c0], shape: [0, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %12 = tts.make_tptr %arg0 to sizes: [4, 4], strides: [%1, %4], offsets: [%arg9, %5], shape: [%3, 0], order: [] : <f32> to tensor<4x4x!tt.ptr<f32>>
      %13 = "tts.load"(%12, %cst) <{operandSegmentSizes = array<i32: 1, 0, 1>, static_mask_dims = array<i64: 4, 3>}> : (tensor<4x4x!tt.ptr<f32>>, f32) -> tensor<4x4xf32>
      "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<4x4x!tt.ptr<f32>>, tensor<4x4xf32>) -> ()
      %14 = arith.addi %arg9, %9 : index
      %15 = arith.addi %arg10, %9 : index
      scf.yield %14, %15 : index, index
    }
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir:1 offset :46:7: error: 'bufferization.materialize_in_destination' op failed to verify that all of {source, dest} have same element type
      tt.store %arg10, %32 : tensor<4x4x!tt.ptr<f32>>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir:1 offset :46:7: note: see current operation: %65 = "bufferization.materialize_in_destination"(%64, %23) <{writable}> : (tensor<4x4xf32>, tensor<4x4x!tt.ptr<f32>>) -> tensor<4x4x!tt.ptr<f32>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/wraparound_stacked.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir (10 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 128 : index}> : () -> index
    %3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
    %4 = "arith.constant"() <{value = 3 : index}> : () -> index
    %5 = "arith.constant"() <{value = 12 : index}> : () -> index
    %6 = "arith.constant"() <{value = 0 : index}> : () -> index
    %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %8 = "arith.muli"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "arith.index_cast"(%8) : (i32) -> index
    %10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
    %11:3 = "scf.for"(%6, %5, %4, %3, %10, %9) ({
    ^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
      %16 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %17 = "tts.make_tptr"(%arg1, %16) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
      %18 = "tts.load"(%17) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
      %19 = "math.exp"(%18) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
      %20 = "arith.addf"(%arg6, %19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %21 = "arith.index_cast"(%arg5) : (index) -> i32
      %22 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %23 = "tt.addptr"(%arg7, %21) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "scf.yield"(%20, %23, %22) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
    }) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
    %12 = "arith.muli"(%7, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %13 = "arith.index_cast"(%12) : (i32) -> index
    %14 = "arith.addi"(%13, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %15 = "tts.make_tptr"(%arg0, %14) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
    "tts.store"(%15, %11#0) <{static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%10 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "tt.addptr"(%arg7, %22) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %24, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
actual processing
processing user
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: !tt.ptr<f32>, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "tt.addptr"(%arg7, %22) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %24, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, !tt.ptr<f32>, index)
arg number: 4
init arg size
3
num region iter-args
3
dump from that index
iter arg
init arg
%3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
%9 = "arith.index_cast"(%8) : (i32) -> index
~~~~
processing val
<block argument> of type 'i32' at index: 2
these are the uses:
%24 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%24 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: i32, %arg8: index):
  %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
  %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
  %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
  %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
  %22 = "arith.index_cast"(%arg5) : (index) -> i32
  %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %24 = "arith.addi"(%arg7, %22) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
}) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, i32, index)
these are the uses:
actual processing
~~~~
processing val
%25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
actual processing
processing user
"scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
~~~~
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 128 : index}> : () -> index
    %3 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<128x128xf32>}> : () -> tensor<128x128xf32>
    %4 = "arith.constant"() <{value = 3 : index}> : () -> index
    %5 = "arith.constant"() <{value = 12 : index}> : () -> index
    %6 = "arith.constant"() <{value = 0 : index}> : () -> index
    %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %8 = "arith.muli"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "arith.index_cast"(%8) : (i32) -> index
    %10 = "arith.addi"(%0, %8) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
    %12:3 = "scf.for"(%6, %5, %4, %3, %11, %9) ({
    ^bb0(%arg5: index, %arg6: tensor<128x128xf32>, %arg7: i32, %arg8: index):
      %17 = "arith.addi"(%arg8, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %18 = "tts.make_tptr"(%arg1, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
      %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>) -> tensor<128x128xf32>
      %20 = "math.exp"(%19) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>) -> tensor<128x128xf32>
      %21 = "arith.addf"(%arg6, %20) <{fastmath = #arith.fastmath<none>}> : (tensor<128x128xf32>, tensor<128x128xf32>) -> tensor<128x128xf32>
      %22 = "arith.index_cast"(%arg5) : (index) -> i32
      %23 = "arith.addi"(%arg8, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %24 = "arith.addi"(%arg7, %22) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
      "scf.yield"(%21, %25, %23) : (tensor<128x128xf32>, !tt.ptr<f32>, index) -> ()
    }) : (index, index, index, tensor<128x128xf32>, !tt.ptr<f32>, index) -> (tensor<128x128xf32>, i32, index)
    %13 = "arith.muli"(%7, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %14 = "arith.index_cast"(%13) : (i32) -> index
    %15 = "arith.addi"(%14, %2) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %16 = "tts.make_tptr"(%arg0, %15) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 128, 128>, static_offsets = array<i64: -9223372036854775808, 0>, static_shape = array<i64: 0, 0>, static_strides = array<i64: 1, 1>}> : (!tt.ptr<f32>, index) -> tensor<128x128x!tt.ptr<f32>>
    "tts.store"(%16, %12#0) <{static_mask_dims = array<i64>}> : (tensor<128x128x!tt.ptr<f32>>, tensor<128x128xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%11 = "tt.addptr"(%0, %8) : (i32, i32) -> !tt.ptr<f32>
deleting
%25 = "tt.addptr"(%arg7, %22) : (i32, i32) -> !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir:1 offset :27:13: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<128x128x!tt.ptr<f32>>'
      %12 = tt.load %11 : tensor<128x128x!tt.ptr<f32>>
            ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir:1 offset :27:13: note: see current operation: "memref.copy"(%20, %21) : (tensor<128x128x!tt.ptr<f32>>, memref<128x128xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for_2d.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/use_end_chain.mlir (11 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/use_end_chain.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c6 = arith.constant 6 : index
    %cst = arith.constant dense<6> : tensor<256x128xi32>
    %0 = tt.make_range {end = 768 : i32, start = 512 : i32} : tensor<256xi32>
    %1 = tt.expand_dims %0 {axis = 1 : i32} : tensor<256xi32> -> tensor<256x1xi32>
    %2 = tt.broadcast %1 : tensor<256x1xi32> -> tensor<256x128xi32>
    %3 = tt.make_range {end = 1152 : i32, start = 1024 : i32} : tensor<128xi32>
    %4 = tt.expand_dims %3 {axis = 0 : i32} : tensor<128xi32> -> tensor<1x128xi32>
    %5 = tt.broadcast %4 : tensor<1x128xi32> -> tensor<256x128xi32>
    %6 = arith.muli %5, %cst : tensor<256x128xi32>
    %7 = arith.addi %2, %6 : tensor<256x128xi32>
    %8 = tts.make_tptr %arg1 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <bf16> to tensor<256x128x!tt.ptr<bf16>>
    %9 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>) -> tensor<256x128xbf16>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    %10 = arith.sitofp %7 : tensor<256x128xi32> to tensor<256x128xbf16>
    "tts.store"(%8, %10) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c6 = arith.constant 6 : index
    %cst = arith.constant dense<6> : tensor<256x128xi32>
    %0 = tt.make_range {end = 768 : i32, start = 512 : i32} : tensor<256xi32>
    %1 = tt.expand_dims %0 {axis = 1 : i32} : tensor<256xi32> -> tensor<256x1xi32>
    %2 = tt.broadcast %1 : tensor<256x1xi32> -> tensor<256x128xi32>
    %3 = tt.make_range {end = 1152 : i32, start = 1024 : i32} : tensor<128xi32>
    %4 = tt.expand_dims %3 {axis = 0 : i32} : tensor<128xi32> -> tensor<1x128xi32>
    %5 = tt.broadcast %4 : tensor<1x128xi32> -> tensor<256x128xi32>
    %6 = arith.muli %5, %cst : tensor<256x128xi32>
    %7 = arith.addi %2, %6 : tensor<256x128xi32>
    %8 = tts.make_tptr %arg1 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <bf16> to tensor<256x128x!tt.ptr<bf16>>
    %9 = "tts.load"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>) -> tensor<256x128xbf16>
    "tts.store"(%8, %9) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    %10 = arith.sitofp %7 : tensor<256x128xi32> to tensor<256x128xbf16>
    "tts.store"(%8, %10) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir:1 offset :29:9: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<256x128x!tt.ptr<bf16>>'
  %19 = tt.load %18 : tensor<256x128x!tt.ptr<bf16>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir:1 offset :29:9: note: see current operation: "memref.copy"(%18, %19) : (tensor<256x128x!tt.ptr<bf16>>, memref<256x128xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_end_chain.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir (12 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir
module {
  tt.func public @tensor_indices_nested(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c3_i32 = arith.constant 3 : i32
    %cst = arith.constant dense<4> : tensor<4xi32>
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1:4 = scf.for %arg2 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg3 = %0, %arg4 = %c0, %arg5 = %0, %arg6 = %c0) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
      %2 = arith.muli %arg2, %c2_i32 : i32
      %3 = arith.index_cast %2 : i32 to index
      %4 = tt.splat %2 : i32 -> tensor<4xi32>
      %5 = arith.addi %arg3, %4 : tensor<4xi32>
      %6 = arith.addi %arg4, %3 : index
      %7 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      %8 = "tts.load"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
      %9 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      "tts.store"(%9, %8) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
      %10 = arith.addi %5, %cst : tensor<4xi32>
      %11 = arith.addi %arg5, %cst : tensor<4xi32>
      %12 = arith.addi %6, %c4 : index
      %13 = arith.addi %arg6, %c4 : index
      %14:4 = scf.for %arg7 = %c0_i32_1 to %c3_i32 step %c1_i32 iter_args(%arg8 = %10, %arg9 = %12, %arg10 = %11, %arg11 = %13) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
        %15 = arith.muli %arg7, %c3_i32 : i32
        %16 = arith.index_cast %15 : i32 to index
        %17 = tt.splat %15 : i32 -> tensor<4xi32>
        %18 = arith.addi %arg8, %17 : tensor<4xi32>
        %19 = arith.addi %arg9, %16 : index
        %20 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%19], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        %21 = "tts.load"(%20) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
        %22 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg11], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        "tts.store"(%22, %21) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
        %23 = arith.addi %18, %cst : tensor<4xi32>
        %24 = arith.addi %arg10, %cst : tensor<4xi32>
        %25 = arith.addi %19, %c4 : index
        %26 = arith.addi %arg11, %c4 : index
        scf.yield %23, %25, %24, %26 : tensor<4xi32>, index, tensor<4xi32>, index
      }
      scf.yield %14#0, %14#1, %14#2, %14#3 : tensor<4xi32>, index, tensor<4xi32>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @tensor_indices_nested(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c3_i32 = arith.constant 3 : i32
    %cst = arith.constant dense<4> : tensor<4xi32>
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
    %1:4 = scf.for %arg2 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg3 = %0, %arg4 = %c0, %arg5 = %0, %arg6 = %c0) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
      %2 = arith.muli %arg2, %c2_i32 : i32
      %3 = arith.index_cast %2 : i32 to index
      %4 = tt.splat %2 : i32 -> tensor<4xi32>
      %5 = arith.addi %arg3, %4 : tensor<4xi32>
      %6 = arith.addi %arg4, %3 : index
      %7 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      %8 = "tts.load"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
      %9 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg6], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
      "tts.store"(%9, %8) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
      %10 = arith.addi %5, %cst : tensor<4xi32>
      %11 = arith.addi %arg5, %cst : tensor<4xi32>
      %12 = arith.addi %6, %c4 : index
      %13 = arith.addi %arg6, %c4 : index
      %14:4 = scf.for %arg7 = %c0_i32_1 to %c3_i32 step %c1_i32 iter_args(%arg8 = %10, %arg9 = %12, %arg10 = %11, %arg11 = %13) -> (tensor<4xi32>, index, tensor<4xi32>, index)  : i32 {
        %15 = arith.muli %arg7, %c3_i32 : i32
        %16 = arith.index_cast %15 : i32 to index
        %17 = tt.splat %15 : i32 -> tensor<4xi32>
        %18 = arith.addi %arg8, %17 : tensor<4xi32>
        %19 = arith.addi %arg9, %16 : index
        %20 = tts.make_tptr %arg0 to sizes: [4], strides: [%c1], offsets: [%19], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        %21 = "tts.load"(%20) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>) -> tensor<4xf32>
        %22 = tts.make_tptr %arg1 to sizes: [4], strides: [%c1], offsets: [%arg11], shape: [0], order: [] : <f32> to tensor<4x!tt.ptr<f32>>
        "tts.store"(%22, %21) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
        %23 = arith.addi %18, %cst : tensor<4xi32>
        %24 = arith.addi %arg10, %cst : tensor<4xi32>
        %25 = arith.addi %19, %c4 : index
        %26 = arith.addi %arg11, %c4 : index
        scf.yield %23, %25, %24, %26 : tensor<4xi32>, index, tensor<4xi32>, index
      }
      scf.yield %14#0, %14#1, %14#2, %14#3 : tensor<4xi32>, index, tensor<4xi32>, index
    }
    tt.return
  }
}
total addptr count: 0
/home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir:19:12: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4x!tt.ptr<f32>>'
      %8 = tt.load %7 : tensor<4x!tt.ptr<f32>>
           ^
/home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir:19:12: note: see current operation: "memref.copy"(%22, %23) : (tensor<4x!tt.ptr<f32>>, memref<4xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/TritonToLinalg/tensor_indices_loop_iterargs_nested.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for.mlir (13 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_for.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
    %3 = "arith.constant"() <{value = 3 : index}> : () -> index
    %4 = "arith.constant"() <{value = 12 : index}> : () -> index
    %5 = "arith.constant"() <{value = 0 : index}> : () -> index
    %6 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %7 = "arith.muli"(%6, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
    %10:3 = "scf.for"(%5, %4, %3, %9, %8, %2) ({
    ^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
      %14 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
      %15 = "tts.load"(%14) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
      %16 = "math.exp"(%15) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
      %17 = "arith.addf"(%arg8, %16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %18 = "arith.index_cast"(%arg5) : (index) -> i32
      %19 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %20 = "tt.addptr"(%arg6, %18) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "scf.yield"(%20, %19, %17) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
    }) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
    %11 = "arith.muli"(%6, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %12 = "arith.index_cast"(%11) : (i32) -> index
    %13 = "tts.make_tptr"(%arg0, %12) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
    "tts.store"(%13, %10#2) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%9 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "tt.addptr"(%arg6, %19) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
actual processing
processing user
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: !tt.ptr<f32>, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "tt.addptr"(%arg6, %19) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
  "scf.yield"(%21, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (!tt.ptr<f32>, index, tensor<1024xf32>)
arg number: 3
init arg size
3
num region iter-args
3
dump from that index
iter arg
init arg
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
%8 = "arith.index_cast"(%7) : (i32) -> index
%2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
~~~~
processing val
<block argument> of type 'i32' at index: 1
these are the uses:
%21 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%21 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
^bb0(%arg5: index, %arg6: i32, %arg7: index, %arg8: tensor<1024xf32>):
  %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
  %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
  %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
  %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
  %19 = "arith.index_cast"(%arg5) : (index) -> i32
  %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
  %21 = "arith.addi"(%arg6, %19) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
  "scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
}) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (i32, index, tensor<1024xf32>)
these are the uses:
actual processing
~~~~
processing val
%22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
"scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
actual processing
processing user
"scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
~~~~
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {}, {}, {}], function_type = (!tt.ptr<f32>, !tt.ptr<f32>, i32, i32, i32) -> (), sym_name = "kernel"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<1024xf32>}> : () -> tensor<1024xf32>
    %3 = "arith.constant"() <{value = 3 : index}> : () -> index
    %4 = "arith.constant"() <{value = 12 : index}> : () -> index
    %5 = "arith.constant"() <{value = 0 : index}> : () -> index
    %6 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %7 = "arith.muli"(%6, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %8 = "arith.index_cast"(%7) : (i32) -> index
    %9 = "arith.addi"(%0, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
    %11:3 = "scf.for"(%5, %4, %3, %10, %8, %2) ({
    ^bb0(%arg5: index, %arg6: i32, %arg7: index, %arg8: tensor<1024xf32>):
      %15 = "tts.make_tptr"(%arg1, %arg7) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
      %16 = "tts.load"(%15) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
      %17 = "math.exp"(%16) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>) -> tensor<1024xf32>
      %18 = "arith.addf"(%arg8, %17) <{fastmath = #arith.fastmath<none>}> : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>
      %19 = "arith.index_cast"(%arg5) : (index) -> i32
      %20 = "arith.addi"(%arg7, %arg5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
      %21 = "arith.addi"(%arg6, %19) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
      %22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
      "scf.yield"(%22, %20, %18) : (!tt.ptr<f32>, index, tensor<1024xf32>) -> ()
    }) : (index, index, index, !tt.ptr<f32>, index, tensor<1024xf32>) -> (i32, index, tensor<1024xf32>)
    %12 = "arith.muli"(%6, %arg3) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %13 = "arith.index_cast"(%12) : (i32) -> index
    %14 = "tts.make_tptr"(%arg0, %13) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 1024>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<1024x!tt.ptr<f32>>
    "tts.store"(%14, %11#2) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 2
deleting
%22 = "tt.addptr"(%arg6, %19) : (i32, i32) -> !tt.ptr<f32>
deleting
%10 = "tt.addptr"(%0, %7) : (i32, i32) -> !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir:1 offset :20:12: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
      %8 = tt.load %5 : tensor<1024x!tt.ptr<f32>>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir:1 offset :20:12: note: see current operation: "memref.copy"(%17, %18) : (tensor<1024x!tt.ptr<f32>>, memref<1024xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_for.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax.mlir (14 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_argmin_argmax.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :25:5: remark: PtrAnalysis: scalar storeOp will not be rewritten
    tt.store %9, %8#1 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :25:5: note: see current operation: tt.store %12, %11#1 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :25:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %9, %8#1 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :25:5: note: see current operation: tt.store %12, %11#1 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, i32) -> (), sym_name = "argmax_012", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %3 = "arith.muli"(%2, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %4 = "arith.index_cast"(%3) : (i32) -> index
    %5 = "tt.make_range"() <{end = 4096 : i32, start = 0 : i32}> : () -> tensor<4096xi32>
    %6 = "tts.make_tptr"(%arg0, %4) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 4096>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<4096x!tt.ptr<f32>>
    %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4096x!tt.ptr<f32>>) -> tensor<4096xf32>
    %8:2 = "tt.reduce"(%7, %5) <{axis = 0 : i32}> ({
    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):
      %10 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
      %11 = "arith.cmpi"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %12 = "arith.andi"(%10, %11) : (i1, i1) -> i1
      %13 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
      %14 = "arith.ori"(%13, %12) : (i1, i1) -> i1
      %15 = "arith.select"(%14, %arg3, %arg5) : (i1, f32, f32) -> f32
      %16 = "arith.select"(%14, %arg4, %arg6) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%15, %16) : (f32, i32) -> ()
    }) : (tensor<4096xf32>, tensor<4096xi32>) -> (f32, i32)
    %9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
    "tt.store"(%9, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
actual processing
processing user
%9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
~~~~
processing val
%10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
these are the uses:
"tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
actual processing
processing user
"tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, i32) -> (), sym_name = "argmax_012", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %3 = "arith.muli"(%2, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %4 = "arith.index_cast"(%3) : (i32) -> index
    %5 = "tt.make_range"() <{end = 4096 : i32, start = 0 : i32}> : () -> tensor<4096xi32>
    %6 = "tts.make_tptr"(%arg0, %4) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 4096>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<4096x!tt.ptr<f32>>
    %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4096x!tt.ptr<f32>>) -> tensor<4096xf32>
    %8:2 = "tt.reduce"(%7, %5) <{axis = 0 : i32}> ({
    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):
      %12 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
      %13 = "arith.cmpi"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %14 = "arith.andi"(%12, %13) : (i1, i1) -> i1
      %15 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
      %16 = "arith.ori"(%15, %14) : (i1, i1) -> i1
      %17 = "arith.select"(%16, %arg3, %arg5) : (i1, f32, f32) -> f32
      %18 = "arith.select"(%16, %arg4, %arg6) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%17, %18) : (f32, i32) -> ()
    }) : (tensor<4096xf32>, tensor<4096xi32>) -> (f32, i32)
    %9 = "arith.addi"(%0, %2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
    %11 = "tts.make_unstructured_tptr"(%arg1, %9) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 1
deleting
%10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :12:10: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4096x!tt.ptr<f32>>'
    %7 = tt.load %6 : tensor<4096x!tt.ptr<f32>>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:1 offset :12:10: note: see current operation: "memref.copy"(%6, %7) : (tensor<4096x!tt.ptr<f32>>, memref<4096xf32>) -> ()
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :25:5: remark: PtrAnalysis: scalar storeOp will not be rewritten
    tt.store %9, %8#1 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :25:5: note: see current operation: tt.store %12, %11#1 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :25:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %9, %8#1 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :25:5: note: see current operation: tt.store %12, %11#1 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, i32) -> (), sym_name = "argmin_012", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %3 = "arith.muli"(%2, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %4 = "arith.index_cast"(%3) : (i32) -> index
    %5 = "tt.make_range"() <{end = 4096 : i32, start = 0 : i32}> : () -> tensor<4096xi32>
    %6 = "tts.make_tptr"(%arg0, %4) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 4096>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<4096x!tt.ptr<f32>>
    %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4096x!tt.ptr<f32>>) -> tensor<4096xf32>
    %8:2 = "tt.reduce"(%7, %5) <{axis = 0 : i32}> ({
    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):
      %10 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
      %11 = "arith.cmpi"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %12 = "arith.andi"(%10, %11) : (i1, i1) -> i1
      %13 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
      %14 = "arith.ori"(%13, %12) : (i1, i1) -> i1
      %15 = "arith.select"(%14, %arg3, %arg5) : (i1, f32, f32) -> f32
      %16 = "arith.select"(%14, %arg4, %arg6) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%15, %16) : (f32, i32) -> ()
    }) : (tensor<4096xf32>, tensor<4096xi32>) -> (f32, i32)
    %9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
    "tt.store"(%9, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
actual processing
processing user
%9 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
~~~~
processing val
%10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
these are the uses:
"tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
actual processing
processing user
"tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<i32>, i32) -> (), sym_name = "argmin_012", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: i32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %3 = "arith.muli"(%2, %arg2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %4 = "arith.index_cast"(%3) : (i32) -> index
    %5 = "tt.make_range"() <{end = 4096 : i32, start = 0 : i32}> : () -> tensor<4096xi32>
    %6 = "tts.make_tptr"(%arg0, %4) <{operandSegmentSizes = array<i32: 1, 0, 1, 0>, order = array<i32>, sizes = array<i64: 4096>, static_offsets = array<i64: -9223372036854775808>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> tensor<4096x!tt.ptr<f32>>
    %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4096x!tt.ptr<f32>>) -> tensor<4096xf32>
    %8:2 = "tt.reduce"(%7, %5) <{axis = 0 : i32}> ({
    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):
      %12 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
      %13 = "arith.cmpi"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %14 = "arith.andi"(%12, %13) : (i1, i1) -> i1
      %15 = "arith.cmpf"(%arg3, %arg5) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
      %16 = "arith.ori"(%15, %14) : (i1, i1) -> i1
      %17 = "arith.select"(%16, %arg3, %arg5) : (i1, f32, f32) -> f32
      %18 = "arith.select"(%16, %arg4, %arg6) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%17, %18) : (f32, i32) -> ()
    }) : (tensor<4096xf32>, tensor<4096xi32>) -> (f32, i32)
    %9 = "arith.addi"(%0, %2) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
    %11 = "tts.make_unstructured_tptr"(%arg1, %9) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%10, %8#1) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<i32>, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 1
deleting
%10 = "tt.addptr"(%0, %2) : (i32, i32) -> !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :12:10: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4096x!tt.ptr<f32>>'
    %7 = tt.load %6 : tensor<4096x!tt.ptr<f32>>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:73 offset :12:10: note: see current operation: "memref.copy"(%6, %7) : (tensor<4096x!tt.ptr<f32>>, memref<4096xf32>) -> ()
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir:31:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: func.func @argmax_012
                ^
<stdin>:1:1: note: scanning from here
// -----
^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_argmin_argmax.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: // -----
label:31     X~~~~~~~~ error: no match found
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_reduce.mlir (15 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax_reduce.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_sgt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 4 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_sgt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 4 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<i32>'
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:1 offset :11:5: note: see current operation: %11 = "memref.reinterpret_cast"(%arg0, %10) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>, index) -> memref<1xi32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ugt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 8 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ugt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 8 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<i32>'
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:37 offset :12:5: note: see current operation: %11 = "memref.reinterpret_cast"(%arg0, %10) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>, index) -> memref<1xi32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_slt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_slt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 2 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<i32>'
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:74 offset :12:5: note: see current operation: %11 = "memref.reinterpret_cast"(%arg0, %10) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>, index) -> memref<1xi32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: note: see current operation: tt.store %arg0, %0 : !tt.ptr<i32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ult", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %3 = "arith.cmpi"(%arg1, %arg2) <{predicate = 6 : i64}> : (i32, i32) -> i1
      %4 = "arith.select"(%3, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%4) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
!tt.ptr<i32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<i32>) -> (), sym_name = "minmax_ult", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<i32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = dense<0> : tensor<4096xi32>}> : () -> tensor<4096xi32>
    %2 = "tt.reduce"(%1) <{axis = 0 : i32}> ({
    ^bb0(%arg1: i32, %arg2: i32):
      %4 = "arith.cmpi"(%arg1, %arg2) <{predicate = 6 : i64}> : (i32, i32) -> i1
      %5 = "arith.select"(%4, %arg1, %arg2) : (i1, i32, i32) -> i32
      "tt.reduce.return"(%5) : (i32) -> ()
    }) : (tensor<4096xi32>) -> i32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<i32>, i32) -> !tt.ptr<i32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, i32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<i32>'
    tt.store %arg0, %63 : !tt.ptr<i32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:112 offset :12:5: note: see current operation: %11 = "memref.reinterpret_cast"(%arg0, %10) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<i32>, index) -> memref<1xi32, strided<[1], offset: ?>>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir:16:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: func.func @minmax_sgt
                ^
<stdin>:1:1: note: scanning from here
// -----
^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax_reduce.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: // -----
label:16     X~~~~~~~~ error: no match found
          2: // -----
label:16     ~~~~~~~~~
          3: // -----
label:16     ~~~~~~~~~
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/nested_loops.mlir (16 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/nested_loops.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir
module {
  tt.func public @nested2_complex_body(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c3 = arith.constant 3 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg2, %c2_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = arith.addi %arg5, %c1 : index
      %6 = arith.addi %arg6, %c1 : index
      %7:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %5, %arg9 = %6) -> (index, index)  : i32 {
        %12 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%12, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg8, %c3 : index
        %16 = arith.addi %arg9, %c3 : index
        scf.yield %15, %16 : index, index
      }
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %8, %c1 : index
      %10 = arith.addi %arg6, %3 : index
      %11 = arith.addi %10, %c1 : index
      scf.yield %9, %11 : index, index
    }
    tt.return
  }
  tt.func public @nested2_use_loop_results(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg3, %c4_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg6, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      "tts.store"(%5, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %arg6, %3 : index
      %10:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %8, %arg9 = %9) -> (index, index)  : i32 {
        %11 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %14 = arith.addi %arg8, %3 : index
        %15 = arith.addi %arg9, %3 : index
        scf.yield %14, %15 : index, index
      }
      scf.yield %10#0, %10#1 : index, index
    }
    tt.return
  }
  tt.func public @nested3(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %10 = arith.addi %arg9, %4 : index
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%10, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %10, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %14 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%14, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %18 = arith.addi %arg15, %4 : index
          %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %18, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          scf.yield %15, %23, %22 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        scf.yield %13#0, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %9 = arith.addi %8#0, %4 : index
      scf.yield %9, %8#1, %8#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
  tt.func public @nested_use_same_level_loop_result(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5) -> (index)  : i32 {
        %9 = arith.addi %arg9, %4 : index
        scf.yield %9 : index
      }
      %7:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %6, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %9 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %10 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %11 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %12 = arith.addi %arg9, %4 : index
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%12, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%9, %11) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg11, %4 : index
        %16 = arith.addi %15, %4 : index
        %17 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        "tts.store"(%17, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %18 = arith.addi %16, %4 : index
        %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %20 = arith.addi %12, %4 : index
        scf.yield %20, %19, %18 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %8 = arith.addi %7#0, %4 : index
      scf.yield %8, %7#1, %7#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @nested2_complex_body(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c3 = arith.constant 3 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg2, %c2_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = arith.addi %arg5, %c1 : index
      %6 = arith.addi %arg6, %c1 : index
      %7:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %5, %arg9 = %6) -> (index, index)  : i32 {
        %12 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%12, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg8, %c3 : index
        %16 = arith.addi %arg9, %c3 : index
        scf.yield %15, %16 : index, index
      }
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %8, %c1 : index
      %10 = arith.addi %arg6, %3 : index
      %11 = arith.addi %10, %c1 : index
      scf.yield %9, %11 : index, index
    }
    tt.return
  }
  tt.func public @nested2_use_loop_results(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c4_i32 = arith.constant 4 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = arith.muli %arg3, %c4_i32 : i32
    %3 = arith.index_cast %2 : i32 to index
    %4:2 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %c0) -> (index, index)  : i32 {
      %5 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg6, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      "tts.store"(%5, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
      %8 = arith.addi %arg5, %3 : index
      %9 = arith.addi %arg6, %3 : index
      %10:2 = scf.for %arg7 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg8 = %8, %arg9 = %9) -> (index, index)  : i32 {
        %11 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg8, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%11, %13) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %14 = arith.addi %arg8, %3 : index
        %15 = arith.addi %arg9, %3 : index
        scf.yield %14, %15 : index, index
      }
      scf.yield %10#0, %10#1 : index, index
    }
    tt.return
  }
  tt.func public @nested3(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %10 = arith.addi %arg9, %4 : index
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%10, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %10, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %14 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%14, %7) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %18 = arith.addi %arg15, %4 : index
          %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %18, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          scf.yield %15, %23, %22 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        scf.yield %13#0, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %9 = arith.addi %8#0, %4 : index
      scf.yield %9, %8#1, %8#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
  tt.func public @nested_use_same_level_loop_result(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5) -> (index)  : i32 {
        %9 = arith.addi %arg9, %4 : index
        scf.yield %9 : index
      }
      %7:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %6, %arg10 = %arg6, %arg11 = %arg7) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %9 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %10 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %11 = "tts.load"(%10) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %12 = arith.addi %arg9, %4 : index
        %13 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%12, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %14 = "tts.load"(%13) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        "tts.store"(%9, %11) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %15 = arith.addi %arg11, %4 : index
        %16 = arith.addi %15, %4 : index
        %17 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        "tts.store"(%17, %14) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
        %18 = arith.addi %16, %4 : index
        %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%18, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %20 = arith.addi %12, %4 : index
        scf.yield %20, %19, %18 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %8 = arith.addi %7#0, %4 : index
      scf.yield %8, %7#1, %7#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir:1 offset :32:15: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<2x2x!tt.ptr<f32>>'
        %26 = tt.load %arg8 : tensor<2x2x!tt.ptr<f32>>
              ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir:1 offset :32:15: note: see current operation: "memref.copy"(%25, %26) : (tensor<2x2x!tt.ptr<f32>>, memref<2x2xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/nested_loops.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/use_mid_chain.mlir (17 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/use_mid_chain.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c6 = arith.constant 6 : index
    %0 = tt.make_range {end = 768 : i32, start = 512 : i32} : tensor<256xi32>
    %1 = tt.expand_dims %0 {axis = 1 : i32} : tensor<256xi32> -> tensor<256x1xi32>
    %2 = tt.broadcast %1 : tensor<256x1xi32> -> tensor<256x128xi32>
    %3 = tts.make_tptr %arg1 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <bf16> to tensor<256x128x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>) -> tensor<256x128xbf16>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    %5 = tts.make_tptr %arg2 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <i32> to tensor<256x128x!tt.ptr<i32>>
    "tts.store"(%5, %2) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<i32>>, tensor<256x128xi32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_1 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c6 = arith.constant 6 : index
    %0 = tt.make_range {end = 768 : i32, start = 512 : i32} : tensor<256xi32>
    %1 = tt.expand_dims %0 {axis = 1 : i32} : tensor<256xi32> -> tensor<256x1xi32>
    %2 = tt.broadcast %1 : tensor<256x1xi32> -> tensor<256x128xi32>
    %3 = tts.make_tptr %arg1 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <bf16> to tensor<256x128x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>) -> tensor<256x128xbf16>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xbf16>) -> ()
    %5 = tts.make_tptr %arg2 to sizes: [256, 128], strides: [1, %c6], offsets: [512, 6144], shape: [0, 0], order: [] : <i32> to tensor<256x128x!tt.ptr<i32>>
    "tts.store"(%5, %2) <{static_mask_dims = array<i64>}> : (tensor<256x128x!tt.ptr<i32>>, tensor<256x128xi32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir:1 offset :30:9: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<256x128x!tt.ptr<bf16>>'
  %19 = tt.load %18 : tensor<256x128x!tt.ptr<bf16>>
        ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir:1 offset :30:9: note: see current operation: "memref.copy"(%9, %10) : (tensor<256x128x!tt.ptr<bf16>>, memref<256x128xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/use_mid_chain.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_chain.mlir (18 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_chain.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: remark: PtrAnalysis: scalar loadOp will not be rewritten
      %6 = tt.load %2 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: note: see current operation: %16 = tt.load %8 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: remark: PtrAnalysis: Failed to rewrite LoadOp
      %6 = tt.load %2 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: note: see current operation: %16 = tt.load %8 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: remark: PtrAnalysis: scalar loadOp will not be rewritten
      %7 = tt.load %3 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: note: see current operation: %17 = tt.load %10 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: remark: PtrAnalysis: Failed to rewrite LoadOp
      %7 = tt.load %3 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :17:12: note: see current operation: %17 = tt.load %10 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: remark: PtrAnalysis: scalar storeOp will not be rewritten
      tt.store %4, %6 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: note: see current operation: tt.store %13, %16 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: remark: PtrAnalysis: Failed to rewrite StoreOp
      tt.store %4, %6 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :18:7: note: see current operation: tt.store %13, %16 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: remark: PtrAnalysis: scalar storeOp will not be rewritten
      tt.store %5, %7 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: note: see current operation: tt.store %15, %17 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: remark: PtrAnalysis: Failed to rewrite StoreOp
      tt.store %5, %7 : !tt.ptr<f32>
      ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :19:7: note: see current operation: tt.store %15, %17 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "addptr", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 10 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
    %7 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
    "scf.for"(%4, %3, %2) ({
    ^bb0(%arg2: i32):
      %8 = "tt.addptr"(%6, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %9 = "tt.addptr"(%8, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %11 = "tt.addptr"(%10, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %12 = "tt.load"(%8) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %13 = "tt.load"(%9) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      "tt.store"(%10, %12) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "tt.store"(%11, %13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%6 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%8 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%8 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%10 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%13 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%13 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
%16 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
%12 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
%16 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
!tt.ptr<f32>
processing user
%12 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%15, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
%16 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
actual processing
processing user
"tt.store"(%15, %18) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
processing user
%16 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
~~~~
processing val
%13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
%20 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
actual processing
processing user
%20 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
!tt.ptr<f32>
~~~~
processing val
%17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
these are the uses:
"tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
actual processing
processing user
"tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "addptr", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = 2 : i32}> : () -> i32
    %3 = "arith.constant"() <{value = 10 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 1 : i32}> : () -> i32
    %6 = "arith.addi"(%1, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
    %8 = "arith.addi"(%0, %5) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
    "scf.for"(%4, %3, %2) ({
    ^bb0(%arg2: i32):
      %10 = "arith.addi"(%7, %arg2) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %12 = "arith.addi"(%11, %5) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %14 = "arith.addi"(%9, %arg2) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %16 = "arith.addi"(%15, %5) <{overflowFlags = #arith.overflow<none>}> : (!tt.ptr<f32>, i32) -> i32
      %17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %18 = "tts.make_unstructured_tptr"(%arg0, %10) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %19 = "tt.load"(%11) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %20 = "tts.make_unstructured_tptr"(%arg0, %12) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      %21 = "tt.load"(%13) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
      %22 = "tts.make_unstructured_tptr"(%arg1, %14) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "tt.store"(%15, %19) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      %23 = "tts.make_unstructured_tptr"(%arg1, %16) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
      "tt.store"(%17, %21) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<f32>, f32) -> ()
      "scf.yield"() : () -> ()
    }) : (i32, i32, i32) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
total addptr count: 6
deleting
%9 = "tt.addptr"(%0, %5) : (i32, i32) -> !tt.ptr<f32>
deleting
%15 = "tt.addptr"(%9, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%17 = "tt.addptr"(%15, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%7 = "tt.addptr"(%1, %5) : (i32, i32) -> !tt.ptr<f32>
deleting
%11 = "tt.addptr"(%7, %arg2) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
deleting
%13 = "tt.addptr"(%11, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
      %6 = tt.load %2 : !tt.ptr<f32>
           ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir:1 offset :16:12: note: see current operation: %10 = "memref.reinterpret_cast"(%arg0, %9) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<1xf32, strided<[1], offset: ?>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_chain.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir (19 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir
module {
  tt.func public @rand(%arg0: !tt.ptr<i32>, %arg1: !tt.ptr<i32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32>
    %1 = tts.make_tptr %arg0 to sizes: [8], strides: [1], offsets: [0], shape: [0], order: [] : <i32> to tensor<8x!tt.ptr<i32>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<8x!tt.ptr<i32>>) -> tensor<8xi32>
    %3 = tt.extern_elementwise %2, %0 {libname = "", libpath = "", pure = true, symbol = "some_symbol"} : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>
    %4 = tts.make_tptr %arg1 to sizes: [8], strides: [1], offsets: [0], shape: [0], order: [] : <i32> to tensor<8x!tt.ptr<i32>>
    "tts.store"(%4, %3) <{static_mask_dims = array<i64>}> : (tensor<8x!tt.ptr<i32>>, tensor<8xi32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @rand(%arg0: !tt.ptr<i32>, %arg1: !tt.ptr<i32>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32>
    %1 = tts.make_tptr %arg0 to sizes: [8], strides: [1], offsets: [0], shape: [0], order: [] : <i32> to tensor<8x!tt.ptr<i32>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<8x!tt.ptr<i32>>) -> tensor<8xi32>
    %3 = tt.extern_elementwise %2, %0 {libname = "", libpath = "", pure = true, symbol = "some_symbol"} : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>
    %4 = tts.make_tptr %arg1 to sizes: [8], strides: [1], offsets: [0], shape: [0], order: [] : <i32> to tensor<8x!tt.ptr<i32>>
    "tts.store"(%4, %3) <{static_mask_dims = array<i64>}> : (tensor<8x!tt.ptr<i32>>, tensor<8xi32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir:1 offset :8:10: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<8x!tt.ptr<i32>>'
    %3 = tt.load %2 : tensor<8x!tt.ptr<i32>>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir:1 offset :8:10: note: see current operation: "memref.copy"(%5, %6) : (tensor<8x!tt.ptr<i32>>, memref<8xi32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/unsupported_extern_elementwise.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax.mlir (20 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/convert_minmax.mlir' FAILED ********************
Exit Code: 1

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental  /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_olt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_olt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 4 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:1 offset :6:5: note: see current operation: %4 = "memref.reinterpret_cast"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<1xf32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ole", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 5 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ole", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 5 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:11 offset :7:5: note: see current operation: %4 = "memref.reinterpret_cast"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<1xf32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ogt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_ogt", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:22 offset :7:5: note: see current operation: %4 = "memref.reinterpret_cast"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<1xf32, strided<[1], offset: ?>>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: remark: PtrAnalysis: pointer is not replace with tts.make_tptr so storeOp cannot be rewritten
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: remark: PtrAnalysis: Failed to rewrite StoreOp
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: note: see current operation: tt.store %arg0, %1 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_oge", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 3 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
actual processing
processing user
"tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, f32, f32) -> (), sym_name = "minmax_oge", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: f32, %arg2: f32):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.cmpf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>, predicate = 3 : i64}> : (f32, f32) -> i1
    %2 = "arith.select"(%1, %arg1, %arg2) : (i1, f32, f32) -> f32
    %3 = "tts.make_unstructured_tptr"(%arg0, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    "tt.store"(%0, %2) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (i32, f32) -> ()
    "tt.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: error: 'memref.reinterpret_cast' op operand #0 must be ranked or unranked memref of any type values, but got '!tt.ptr<f32>'
    tt.store %arg0, %1 : !tt.ptr<f32>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:33 offset :7:5: note: see current operation: %4 = "memref.reinterpret_cast"(%arg0, %3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>, index) -> memref<1xf32, strided<[1], offset: ?>>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir:44:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: func.func @minmax_olt
                ^
<stdin>:1:1: note: scanning from here
// -----
^

Input file: <stdin>
Check file: /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/convert_minmax.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
          1: // -----
label:44     X~~~~~~~~ error: no match found
          2: // -----
label:44     ~~~~~~~~~
          3: // -----
label:44     ~~~~~~~~~
>>>>>>

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_2d_example.mlir (21 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_2d_example.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c5 = arith.constant 5 : index
    %0 = arith.index_cast %arg3 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3 = tts.make_tptr %arg1 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %5 = arith.addf %2, %4 : tensor<4x256xbf16>
    %6 = tts.make_tptr %arg2 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%6, %5) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
processing val
%c0_i32_1 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c5 = arith.constant 5 : index
    %0 = arith.index_cast %arg3 : i32 to index
    %1 = tts.make_tptr %arg0 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %2 = "tts.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %3 = tts.make_tptr %arg1 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>) -> tensor<4x256xbf16>
    %5 = arith.addf %2, %4 : tensor<4x256xbf16>
    %6 = tts.make_tptr %arg2 to sizes: [4, 256], strides: [1, %c5], offsets: [%0, 0], shape: [0, 0], order: [] : <bf16> to tensor<4x256x!tt.ptr<bf16>>
    "tts.store"(%6, %5) <{static_mask_dims = array<i64>}> : (tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xbf16>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir:1 offset :34:11: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<4x256x!tt.ptr<bf16>>'
    %10 = tt.load %9 : tensor<4x256x!tt.ptr<bf16>>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir:1 offset :34:11: note: see current operation: "memref.copy"(%6, %7) : (tensor<4x256x!tt.ptr<bf16>>, memref<4x256xbf16>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_2d_example.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_splat.mlir (22 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_splat.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %arg2 : i32
    %2 = arith.index_cast %1 : i32 to index
    %3 = tts.make_tptr %arg1 to sizes: [1024], strides: [1], offsets: [%2], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %5 = math.exp %4 : tensor<1024xf32>
    %6 = arith.muli %0, %arg3 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = tts.make_tptr %arg0 to sizes: [1024], strides: [1], offsets: [%7], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    "tts.store"(%8, %5) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %arg2 : i32
    %2 = arith.index_cast %1 : i32 to index
    %3 = tts.make_tptr %arg1 to sizes: [1024], strides: [1], offsets: [%2], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    %4 = "tts.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %5 = math.exp %4 : tensor<1024xf32>
    %6 = arith.muli %0, %arg3 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = tts.make_tptr %arg0 to sizes: [1024], strides: [1], offsets: [%7], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    "tts.store"(%8, %5) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir:1 offset :14:10: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
    %8 = tt.load %5 : tensor<1024x!tt.ptr<f32>>
         ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir:1 offset :14:10: note: see current operation: "memref.copy"(%5, %6) : (tensor<1024x!tt.ptr<f32>>, memref<1024xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_splat.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/get_num_programs.mlir (23 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/get_num_programs.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir
module {
  tt.func public @num_programs(%arg0: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = tt.get_num_programs x : i32
    %1 = tt.get_num_programs y : i32
    %2 = tt.get_num_programs z : i32
    %3 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c0], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %4 = tt.splat %0 : i32 -> tensor<1xi32>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %5 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c1], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %6 = tt.splat %1 : i32 -> tensor<1xi32>
    "tts.store"(%5, %6) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %7 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c2], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %8 = tt.splat %2 : i32 -> tensor<1xi32>
    "tts.store"(%7, %8) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    tt.return
  }
}
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @num_programs(%arg0: !tt.ptr<i32>) {
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = tt.get_num_programs x : i32
    %1 = tt.get_num_programs y : i32
    %2 = tt.get_num_programs z : i32
    %3 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c0], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %4 = tt.splat %0 : i32 -> tensor<1xi32>
    "tts.store"(%3, %4) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %5 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c1], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %6 = tt.splat %1 : i32 -> tensor<1xi32>
    "tts.store"(%5, %6) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    %7 = tts.make_tptr %arg0 to sizes: [1], strides: [0], offsets: [%c2], shape: [0], order: [] : <i32> to tensor<1x!tt.ptr<i32>>
    %8 = tt.splat %2 : i32 -> tensor<1xi32>
    "tts.store"(%7, %8) <{static_mask_dims = array<i64>}> : (tensor<1x!tt.ptr<i32>>, tensor<1xi32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir:1 offset :14:5: error: 'bufferization.materialize_in_destination' op failed to verify that all of {source, dest} have same element type
    tt.store %7, %8 : tensor<1x!tt.ptr<i32>>
    ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir:1 offset :14:5: note: see current operation: %8 = "bufferization.materialize_in_destination"(%7, %5) <{writable}> : (tensor<1xi32>, tensor<1x!tt.ptr<i32>>) -> tensor<1x!tt.ptr<i32>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/get_num_programs.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_nested.mlir (24 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/addptr_scalar_nested.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --split-input-file --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir
module {
  tt.func @kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %arg2 : i32
    %2 = arith.index_cast %1 : i32 to index
    %3 = arith.muli %0, %arg3 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5 = arith.addi %2, %4 : index
    %6 = arith.muli %0, %arg4 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = arith.addi %5, %7 : index
    %9 = tts.make_tptr %arg1 to sizes: [1024], strides: [1], offsets: [%8], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    %10 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %11 = math.exp %10 : tensor<1024xf32>
    %12 = tts.make_tptr %arg0 to sizes: [1024], strides: [1], offsets: [%4], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    "tts.store"(%12, %11) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func @kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %arg2 : i32
    %2 = arith.index_cast %1 : i32 to index
    %3 = arith.muli %0, %arg3 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5 = arith.addi %2, %4 : index
    %6 = arith.muli %0, %arg4 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = arith.addi %5, %7 : index
    %9 = tts.make_tptr %arg1 to sizes: [1024], strides: [1], offsets: [%8], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    %10 = "tts.load"(%9) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024xf32>
    %11 = math.exp %10 : tensor<1024xf32>
    %12 = tts.make_tptr %arg0 to sizes: [1024], strides: [1], offsets: [%4], shape: [0], order: [] : <f32> to tensor<1024x!tt.ptr<f32>>
    "tts.store"(%12, %11) <{static_mask_dims = array<i64>}> : (tensor<1024x!tt.ptr<f32>>, tensor<1024xf32>) -> ()
    tt.return
  }
}
total addptr count: 0
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir:1 offset :20:11: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<1024x!tt.ptr<f32>>'
    %10 = tt.load %9 : tensor<1024x!tt.ptr<f32>>
          ^
within split at /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir:1 offset :20:11: note: see current operation: "memref.copy"(%11, %12) : (tensor<1024x!tt.ptr<f32>>, memref<1024xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/addptr_scalar_nested.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/ridiculously_nested_loops.mlir (25 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/ridiculously_nested_loops.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir
module {
  tt.func public @nested_who_knows_how_many_levels(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:4 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7, %arg12 = %7) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
        %11 = arith.addi %arg9, %4 : index
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %14:5 = scf.for %arg13 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg14 = %11, %arg15 = %arg10, %arg16 = %arg11, %arg17 = %arg12, %arg18 = %13) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
          %16 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = arith.addi %arg14, %4 : index
          %18 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%17, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%16, %arg17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %arg16, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %arg18) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%23, %19) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %24 = arith.addi %22, %4 : index
          %25 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%24, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %26:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %arg17, %arg21 = %18, %arg22 = %17, %arg23 = %arg18, %arg24 = %19, %arg25 = %25, %arg26 = %24) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:7 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26, %arg34 = %29) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:7 = scf.for %arg35 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg36 = %32, %arg37 = %31, %arg38 = %arg31, %arg39 = %arg32, %arg40 = %arg33, %arg41 = %arg34, %arg42 = %33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg40, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg37, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %arg41) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg40, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %arg42) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %45:7 = scf.for %arg43 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg44 = %arg41, %arg45 = %37, %arg46 = %36, %arg47 = %arg42, %arg48 = %38, %arg49 = %44, %arg50 = %43) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                  %46 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg46, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                  %47 = "tts.load"(%46) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                  %48:7 = scf.for %arg51 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg52 = %arg45, %arg53 = %arg46, %arg54 = %arg47, %arg55 = %arg48, %arg56 = %arg49, %arg57 = %arg50, %arg58 = %47) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
                    %49 = arith.addi %arg53, %4 : index
                    %50 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%49, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                    %51 = "tts.load"(%50) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                    %52:7 = scf.for %arg59 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg60 = %50, %arg61 = %49, %arg62 = %arg55, %arg63 = %arg56, %arg64 = %arg57, %arg65 = %arg58, %arg66 = %51) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                      %53 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg64, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %54 = arith.addi %arg61, %4 : index
                      %55 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%54, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %56 = "tts.load"(%55) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                      "tts.store"(%53, %arg65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %57 = arith.addi %arg64, %4 : index
                      %58 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%57, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%58, %arg66) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %59 = arith.addi %57, %4 : index
                      %60 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%59, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%60, %56) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %61 = arith.addi %59, %4 : index
                      %62 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%61, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %63:7 = scf.for %arg67 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg68 = %arg65, %arg69 = %55, %arg70 = %54, %arg71 = %arg66, %arg72 = %56, %arg73 = %62, %arg74 = %61) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                        %64 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg70, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                        %65 = "tts.load"(%64) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                        %66:6 = scf.for %arg75 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg76 = %arg69, %arg77 = %arg70, %arg78 = %arg71, %arg79 = %arg72, %arg80 = %arg73, %arg81 = %arg74) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                          %67 = arith.addi %arg77, %4 : index
                          %68 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%67, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                          %69 = "tts.load"(%68) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                          %70:5 = scf.for %arg82 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg83 = %68, %arg84 = %67, %arg85 = %arg79, %arg86 = %arg80, %arg87 = %arg81) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                            %71 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg87, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %72 = arith.addi %arg84, %4 : index
                            %73 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%72, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %74 = "tts.load"(%73) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                            "tts.store"(%71, %65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %75 = arith.addi %arg87, %4 : index
                            %76 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%75, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%76, %69) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %77 = arith.addi %75, %4 : index
                            %78 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%77, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%78, %74) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %79 = arith.addi %77, %4 : index
                            %80 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%79, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            scf.yield %73, %72, %74, %80, %79 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                          }
                          scf.yield %70#0, %70#1, %69, %70#2, %70#3, %70#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                        }
                        scf.yield %65, %66#0, %66#1, %66#2, %66#3, %66#4, %66#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                      }
                      scf.yield %63#1, %63#2, %63#4, %63#5, %63#6, %63#0, %63#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
                    }
                    scf.yield %52#0, %52#1, %52#6, %52#2, %52#3, %52#4, %52#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
                  }
                  scf.yield %48#6, %48#0, %48#1, %48#2, %48#3, %48#4, %48#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                }
                scf.yield %45#1, %45#2, %45#4, %45#5, %45#6, %45#0, %45#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
              }
              scf.yield %34#0, %34#1, %34#6, %34#2, %34#3, %34#4, %34#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
            }
            scf.yield %30#6, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          %27:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %26#0, %arg21 = %26#1, %arg22 = %26#2, %arg23 = %26#3, %arg24 = %26#4, %arg25 = %26#5, %arg26 = %26#6) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:6 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:5 = scf.for %arg34 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg35 = %32, %arg36 = %31, %arg37 = %arg31, %arg38 = %arg32, %arg39 = %arg33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg36, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %29) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg39, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %33) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                scf.yield %37, %36, %38, %44, %43 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
              }
              scf.yield %34#0, %34#1, %33, %34#2, %34#3, %34#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
            }
            scf.yield %29, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %27#2, %27#5, %27#6, %27#0, %27#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
        }
        %15 = arith.addi %14#0, %4 : index
        scf.yield %15, %14#1, %14#2, %14#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
      }
      %9:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %8#0, %arg10 = %8#1, %arg11 = %8#2) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %arg9, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          %18:3 = scf.for %arg16 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg17 = %15, %arg18 = %arg14, %arg19 = %arg15) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg19, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %20 = arith.addi %arg17, %4 : index
            %21 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %22 = "tts.load"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %23 = arith.addi %arg19, %4 : index
            %24 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%23, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%24, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %25 = arith.addi %23, %4 : index
            %26 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%25, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%26, %22) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %27 = arith.addi %25, %4 : index
            %28 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%27, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            scf.yield %20, %28, %27 : index, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %18#0, %18#1, %18#2 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        %14 = arith.addi %13#0, %4 : index
        scf.yield %14, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %10 = arith.addi %9#0, %4 : index
      scf.yield %10, %9#1, %9#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
processing val
%c0_i32_0 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
processing val
%c0_i32 = arith.constant 0 : i32
these are the uses:
actual processing
~~~~
module {
  tt.func public @nested_who_knows_how_many_levels(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_0 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = arith.index_cast %arg3 : i32 to index
    %2 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [0, 0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
    %3 = arith.muli %arg3, %c2_i32 : i32
    %4 = arith.index_cast %3 : i32 to index
    %5:3 = scf.for %arg4 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg5 = %c0, %arg6 = %2, %arg7 = %c0) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
      %6 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg5, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
      %7 = "tts.load"(%6) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
      %8:4 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %arg5, %arg10 = %arg6, %arg11 = %arg7, %arg12 = %7) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
        %11 = arith.addi %arg9, %4 : index
        %12 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%11, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %13 = "tts.load"(%12) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %14:5 = scf.for %arg13 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg14 = %11, %arg15 = %arg10, %arg16 = %arg11, %arg17 = %arg12, %arg18 = %13) -> (index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
          %16 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg16, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = arith.addi %arg14, %4 : index
          %18 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%17, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %19 = "tts.load"(%18) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          "tts.store"(%16, %arg17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %20 = arith.addi %arg16, %4 : index
          %21 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%21, %arg18) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %22 = arith.addi %20, %4 : index
          %23 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          "tts.store"(%23, %19) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
          %24 = arith.addi %22, %4 : index
          %25 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%24, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %26:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %arg17, %arg21 = %18, %arg22 = %17, %arg23 = %arg18, %arg24 = %19, %arg25 = %25, %arg26 = %24) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:7 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26, %arg34 = %29) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:7 = scf.for %arg35 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg36 = %32, %arg37 = %31, %arg38 = %arg31, %arg39 = %arg32, %arg40 = %arg33, %arg41 = %arg34, %arg42 = %33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg40, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg37, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %arg41) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg40, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %arg42) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %45:7 = scf.for %arg43 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg44 = %arg41, %arg45 = %37, %arg46 = %36, %arg47 = %arg42, %arg48 = %38, %arg49 = %44, %arg50 = %43) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                  %46 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg46, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                  %47 = "tts.load"(%46) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                  %48:7 = scf.for %arg51 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg52 = %arg45, %arg53 = %arg46, %arg54 = %arg47, %arg55 = %arg48, %arg56 = %arg49, %arg57 = %arg50, %arg58 = %47) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>)  : i32 {
                    %49 = arith.addi %arg53, %4 : index
                    %50 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%49, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                    %51 = "tts.load"(%50) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                    %52:7 = scf.for %arg59 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg60 = %50, %arg61 = %49, %arg62 = %arg55, %arg63 = %arg56, %arg64 = %arg57, %arg65 = %arg58, %arg66 = %51) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>)  : i32 {
                      %53 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg64, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %54 = arith.addi %arg61, %4 : index
                      %55 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%54, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %56 = "tts.load"(%55) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                      "tts.store"(%53, %arg65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %57 = arith.addi %arg64, %4 : index
                      %58 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%57, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%58, %arg66) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %59 = arith.addi %57, %4 : index
                      %60 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%59, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      "tts.store"(%60, %56) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                      %61 = arith.addi %59, %4 : index
                      %62 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%61, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                      %63:7 = scf.for %arg67 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg68 = %arg65, %arg69 = %55, %arg70 = %54, %arg71 = %arg66, %arg72 = %56, %arg73 = %62, %arg74 = %61) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                        %64 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg70, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                        %65 = "tts.load"(%64) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                        %66:6 = scf.for %arg75 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg76 = %arg69, %arg77 = %arg70, %arg78 = %arg71, %arg79 = %arg72, %arg80 = %arg73, %arg81 = %arg74) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                          %67 = arith.addi %arg77, %4 : index
                          %68 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%67, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                          %69 = "tts.load"(%68) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                          %70:5 = scf.for %arg82 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg83 = %68, %arg84 = %67, %arg85 = %arg79, %arg86 = %arg80, %arg87 = %arg81) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                            %71 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg87, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %72 = arith.addi %arg84, %4 : index
                            %73 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%72, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            %74 = "tts.load"(%73) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                            "tts.store"(%71, %65) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %75 = arith.addi %arg87, %4 : index
                            %76 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%75, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%76, %69) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %77 = arith.addi %75, %4 : index
                            %78 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%77, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            "tts.store"(%78, %74) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                            %79 = arith.addi %77, %4 : index
                            %80 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%79, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                            scf.yield %73, %72, %74, %80, %79 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                          }
                          scf.yield %70#0, %70#1, %69, %70#2, %70#3, %70#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                        }
                        scf.yield %65, %66#0, %66#1, %66#2, %66#3, %66#4, %66#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                      }
                      scf.yield %63#1, %63#2, %63#4, %63#5, %63#6, %63#0, %63#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
                    }
                    scf.yield %52#0, %52#1, %52#6, %52#2, %52#3, %52#4, %52#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
                  }
                  scf.yield %48#6, %48#0, %48#1, %48#2, %48#3, %48#4, %48#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
                }
                scf.yield %45#1, %45#2, %45#4, %45#5, %45#6, %45#0, %45#3 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
              }
              scf.yield %34#0, %34#1, %34#6, %34#2, %34#3, %34#4, %34#5 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
            }
            scf.yield %30#6, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          %27:7 = scf.for %arg19 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg20 = %26#0, %arg21 = %26#1, %arg22 = %26#2, %arg23 = %26#3, %arg24 = %26#4, %arg25 = %26#5, %arg26 = %26#6) -> (tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %28 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg22, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %29 = "tts.load"(%28) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            %30:6 = scf.for %arg27 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg28 = %arg21, %arg29 = %arg22, %arg30 = %arg23, %arg31 = %arg24, %arg32 = %arg25, %arg33 = %arg26) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
              %31 = arith.addi %arg29, %4 : index
              %32 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%31, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
              %33 = "tts.load"(%32) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
              %34:5 = scf.for %arg34 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg35 = %32, %arg36 = %31, %arg37 = %arg31, %arg38 = %arg32, %arg39 = %arg33) -> (tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
                %35 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %36 = arith.addi %arg36, %4 : index
                %37 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%36, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                %38 = "tts.load"(%37) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
                "tts.store"(%35, %29) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %39 = arith.addi %arg39, %4 : index
                %40 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%39, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%40, %33) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %41 = arith.addi %39, %4 : index
                %42 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%41, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                "tts.store"(%42, %38) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
                %43 = arith.addi %41, %4 : index
                %44 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%43, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
                scf.yield %37, %36, %38, %44, %43 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
              }
              scf.yield %34#0, %34#1, %33, %34#2, %34#3, %34#4 : tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
            }
            scf.yield %29, %30#0, %30#1, %30#2, %30#3, %30#4, %30#5 : tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %27#2, %27#5, %27#6, %27#0, %27#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>, tensor<2x2xf32>
        }
        %15 = arith.addi %14#0, %4 : index
        scf.yield %15, %14#1, %14#2, %14#3 : index, tensor<2x2x!tt.ptr<f32>>, index, tensor<2x2xf32>
      }
      %9:3 = scf.for %arg8 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg9 = %8#0, %arg10 = %8#1, %arg11 = %8#2) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
        %11 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg9, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
        %12 = "tts.load"(%11) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
        %13:3 = scf.for %arg12 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg13 = %arg9, %arg14 = %arg10, %arg15 = %arg11) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
          %15 = arith.addi %arg13, %4 : index
          %16 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%15, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
          %17 = "tts.load"(%16) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
          %18:3 = scf.for %arg16 = %c0_i32_1 to %c2_i32 step %c1_i32 iter_args(%arg17 = %15, %arg18 = %arg14, %arg19 = %arg15) -> (index, tensor<2x2x!tt.ptr<f32>>, index)  : i32 {
            %19 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%arg19, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %20 = arith.addi %arg17, %4 : index
            %21 = tts.make_tptr %arg0 to sizes: [2, 2], strides: [%0, %1], offsets: [%20, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            %22 = "tts.load"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0>, static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>) -> tensor<2x2xf32>
            "tts.store"(%19, %12) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %23 = arith.addi %arg19, %4 : index
            %24 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%23, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%24, %17) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %25 = arith.addi %23, %4 : index
            %26 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%25, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            "tts.store"(%26, %22) <{static_mask_dims = array<i64>}> : (tensor<2x2x!tt.ptr<f32>>, tensor<2x2xf32>) -> ()
            %27 = arith.addi %25, %4 : index
            %28 = tts.make_tptr %arg1 to sizes: [2, 2], strides: [%0, %1], offsets: [%27, %c0], shape: [0, 0], order: [] : <f32> to tensor<2x2x!tt.ptr<f32>>
            scf.yield %20, %28, %27 : index, tensor<2x2x!tt.ptr<f32>>, index
          }
          scf.yield %18#0, %18#1, %18#2 : index, tensor<2x2x!tt.ptr<f32>>, index
        }
        %14 = arith.addi %13#0, %4 : index
        scf.yield %14, %13#1, %13#2 : index, tensor<2x2x!tt.ptr<f32>>, index
      }
      %10 = arith.addi %9#0, %4 : index
      scf.yield %10, %9#1, %9#2 : index, tensor<2x2x!tt.ptr<f32>>, index
    }
    tt.return
  }
}
total addptr count: 0
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir:33:13: error: 'memref.copy' op operand #0 must be ranked or unranked memref of any type values, but got 'tensor<2x2x!tt.ptr<f32>>'
      %25 = tt.load %arg5 : tensor<2x2x!tt.ptr<f32>>
            ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir:33:13: note: see current operation: "memref.copy"(%15, %16) : (tensor<2x2x!tt.ptr<f32>>, memref<2x2xf32>) -> ()
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/ridiculously_nested_loops.mlir

--

********************
FAIL: TRITON-SHARED :: Conversion/StructuredToMemref/early_return.mlir (26 of 215)
******************** TEST 'TRITON-SHARED :: Conversion/StructuredToMemref/early_return.mlir' FAILED ********************
Exit Code: 2

Command Output (stderr):
--
RUN: at line 1: /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir | FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir
+ FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir
+ /home/nhat/github/triton_shared/triton/python/build/cmake.linux-x86_64-cpython-3.8/third_party/triton_shared/tools/triton-shared-opt/triton-shared-opt --triton-to-linalg-experimental /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:19:10: remark: PtrAnalysis: scalar loadOp will not be rewritten
    %2 = tt.load %1 : !tt.ptr<f32>
         ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:19:10: note: see current operation: %3 = tt.load %2 : !tt.ptr<f32>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:19:10: remark: PtrAnalysis: Failed to rewrite LoadOp
    %2 = tt.load %1 : !tt.ptr<f32>
         ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:19:10: note: see current operation: %3 = tt.load %2 : !tt.ptr<f32>
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "test_1", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = -1.000000e+00 : f32}> : () -> f32
    %3 = "arith.constant"() <{value = dense<1> : tensor<4xi32>}> : () -> tensor<4xi32>
    %4 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %5 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
    %6 = "tt.load"(%5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
    %7 = "arith.cmpf"(%6, %2) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
    "cf.cond_br"(%7)[^bb1, ^bb2] <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (i1) -> ()
  ^bb1:  // pred: ^bb0
    "tt.return"() : () -> ()
  ^bb2:  // pred: ^bb0
    %8 = "tt.make_range"() <{end = 4 : i32, start = 0 : i32}> : () -> tensor<4xi32>
    %9 = "arith.addi"(%8, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
    %10 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<4x!tt.ptr<f32>>
    %11 = "arith.sitofp"(%9) : (tensor<4xi32>) -> tensor<4xf32>
    "tts.store"(%10, %11) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
processing val
%1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
%5 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
actual processing
processing user
%5 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
~~~~
processing val
%0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
these are the uses:
actual processing
~~~~
processing val
%6 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
these are the uses:
%7 = "tt.load"(%6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
actual processing
processing user
%7 = "tt.load"(%6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
!tt.ptr<f32>
~~~~
"builtin.module"() ({
  "tt.func"() <{function_type = (!tt.ptr<f32>, !tt.ptr<f32>) -> (), sym_name = "test_1", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
    %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %2 = "arith.constant"() <{value = -1.000000e+00 : f32}> : () -> f32
    %3 = "arith.constant"() <{value = dense<1> : tensor<4xi32>}> : () -> tensor<4xi32>
    %4 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %5 = "arith.addi"(%1, %4) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %6 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
    %7 = "tts.make_unstructured_tptr"(%arg0, %5) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
    %8 = "tt.load"(%6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<f32>) -> f32
    %9 = "arith.cmpf"(%8, %2) <{fastmath = #arith.fastmath<none>, predicate = 1 : i64}> : (f32, f32) -> i1
    "cf.cond_br"(%9)[^bb1, ^bb2] <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (i1) -> ()
  ^bb1:  // pred: ^bb0
    "tt.return"() : () -> ()
  ^bb2:  // pred: ^bb0
    %10 = "tt.make_range"() <{end = 4 : i32, start = 0 : i32}> : () -> tensor<4xi32>
    %11 = "arith.addi"(%10, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
    %12 = "tts.make_tptr"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, order = array<i32>, sizes = array<i64: 4>, static_offsets = array<i64: 0>, static_shape = array<i64: 0>, static_strides = array<i64: 1>}> : (!tt.ptr<f32>) -> tensor<4x!tt.ptr<f32>>
    %13 = "arith.sitofp"(%11) : (tensor<4xi32>) -> tensor<4xf32>
    "tts.store"(%12, %13) <{static_mask_dims = array<i64>}> : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()
total addptr count: 1
deleting
%6 = "tt.addptr"(%1, %4) : (i32, i32) -> !tt.ptr<f32>
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:30:5: error: 'bufferization.materialize_in_destination' op failed to verify that all of {source, dest} have same element type
    tt.store %7, %8 : tensor<4x!tt.ptr<f32>>
    ^
/home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir:30:5: note: see current operation: %15 = "bufferization.materialize_in_destination"(%14, %12) <{writable}> : (tensor<4xf32>, tensor<4x!tt.ptr<f32>>) -> tensor<4x!tt.ptr<f32>>
FileCheck error: '<stdin>' is empty.
FileCheck command line:  FileCheck /home/nhat/github/triton_shared/test/Conversion/StructuredToMemref/early_return.mlir

--

********************
  interrupted by user, skipping remaining tests

Testing Time: 1.15s

Total Discovered Tests: 215
  Skipped: 215 (100.00%)
